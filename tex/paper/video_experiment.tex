\documentclass[12pt]{article}
\linespread{1.5}
\pdfminorversion=6

\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{bookmark}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{float,color}
\usepackage{graphicx,scalerel}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=black,urlcolor=blue,citecolor=black}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{amsxtra}
\usepackage{setspace}
\newcommand{\Rho}{\mathrm{P}}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage[flushleft]{threeparttable}
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{multirow}
\usepackage{array}
\usepackage{tikz}
\usepackage{csquotes}

\usepackage{epigraph}
\setlength \epigraphwidth {\linewidth}
\setlength \epigraphrule {0pt}
\AtBeginDocument{\renewcommand {\epigraphflush}{center}}
\renewcommand {\sourceflush} {center}

\usepackage[
hyperref=true,
giveninits=true, % render first and middle names as initials
uniquename=init,
maxcitenames=3,
maxbibnames=99,
style=authoryear,
dashed=false, % re-print recurring author names in bibliography
natbib=true,
useprefix=true, % for inclusion of 'de' 'da' in surname
urldate=long,
backend=biber
]{biblatex}
\addbibresource{bibliography.bib}

% Scale hats and bars (e.g. \wh{x})
\newcommand\wh[1]{\hstretch{2}{\hat{\hstretch{.5}{#1}}}}
\newcommand\wb[1]{\hstretch{2}{\bar{\hstretch{.5}{#1}}}}

% custom table functions
% indents second level entries
\newcommand{\indentrow}[1]{\quad #1}

% adds space above row
\newcommand{\customlinespace}{\addlinespace[.1cm]}

% command for all table/figure notes
\newcommand{\Fignote}[1]{
	\begin{tablenotes}[para,flushleft]\footnotesize
		\textit{Note}: #1
	\end{tablenotes}
}

% note addendem for regression tables
\newcommand{\Regnote}{Standard errors in parentheses are robust to heteroskedasticity. ***, **, and * indicate significance at the 1, 5, and 10 percent critical levels, respectively.}

\title{The effect of supplementary video lectures on learning in intermediate microeconomics}
\author{Melissa Famulari}
\author{Zachary A. Goodman\thanks{mfamulari@ucsd.edu and zgoodman@ucsd.edu. The authors thank the students who took intermediate microeconomics in the fall of 2018 and 2019 who consented to the use of their data for this study. We also thank UC San Diego's Teaching and Learning Commons for providing campus data on the students in this study as well as anonymizing the data for analysis. Finally, we thank the applied microeconomics group at UC San Diego for their help with the experimental design. This research was approved under UC San Diego's Human Research Protections Program (IRB approval 170886 in fall 2018 and 2019). The paper investigates the use of Intermediate Microeconomics Video Handbook (IMVH) video lectures by UC San Diego students, some of which were developed by one of the authors, in collaboration with UC San Diego and the UC Office of the President. UC San Diego currently owns the rights to distribute the IMVH. The videos lectures were provided to the subjects at no charge and neither author has a direct financial interest in the distribution of the IMVH at UC San Diego. As of fall 2020, one of the authors has a financial interest in the distribution of the IMVH outside of UC San Diego.}}
\affil{University of California, San Diego}
\date{This version: November 2020} % TODO: add link to most recent version


% 8.5 x 11 with 0.5 inch margins
%\geometry{reset, letterpaper, height=10in, width=7.5in, hmarginratio=1:1, vmarginratio=1:1, marginparsep=0pt, marginparwidth=0pt} %, headheight=15pt}

\newcommand{\red}[1]{\textcolor{red}{#1}}

% Estout related things
\newcommand{\sym}[1]{\rlap{#1}}% Thanks to David Carlisle

\let\estinput=\input% define a new input command so that we can still flatten the document

\newcommand{\estwide}[3]{
	\vspace{.75ex}{
		\begin{tabular*}
			{\textwidth}{@{\hskip\tabcolsep\extracolsep\fill}l*{#2}{#3}}
			\toprule
			\estinput{#1}
			\bottomrule
			\addlinespace[.75ex]
		\end{tabular*}
	}
}

\newcommand{\estauto}[3]{
	\vspace{.75ex}{
		\begin{tabular}{l*{#2}{#3}}
			\toprule
			\estinput{#1}
			\bottomrule
			\addlinespace[.75ex]
		\end{tabular}
	}
}

% Allow line breaks with \\ in specialcells
\newcommand{\specialcell}[2][c]{%
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% siunitx
\usepackage{siunitx} % centering in tables
\sisetup{
	detect-mode,
	tight-spacing		= true,
	group-digits		= false ,
	input-signs		= ,
	input-symbols		= ( ) [ ] - + *,
	input-open-uncertainty	= ,
	input-close-uncertainty	= ,
	table-align-text-post	= false
}


% *****************************************************************
% Begin Paper
% *****************************************************************

\begin{document}

\maketitle
\begin{abstract}
	The abstract goes here eventually.
\end{abstract}

\newpage

% *****************************************************************
% Introduction
% *****************************************************************

\section{Introduction}

\epigraph{\textit{``You expect me to read the textbook? Ha!''}}{--- Anonymous student}\bigskip

University students spend tens of thousands of dollars annually on tuition and hundreds of hours in lecture and completing assignments, in large part, to learn. Instructors can improve how well students learn by employing pedagogical tools that have the greatest returns per unit time and financial cost. Despite the importance of comparing the effectiveness of different teaching technologies, little empirical work exists that estimates . In this paper we examine the impact of low marginal cost, video-based learning materials on exam scores in a large, intermediate microeconomic theory course. % at a four-year highly selective public university.

The Intermediate Microeconomic Video Handbook (IMVH) at UC San Diego was designed to \textit{supplement} lecture, not replace it, as an audiovisual version of a conventional course textbook. Part of the impetus for creating the IMVH was a discussion with a student who described her inability to read the course text, not because of poor reading skills, but because she did not find the text engaging enough to command her attention. We hypothesized that current students, who have had unprecedented exposure to electronic media, would find video materials more engaging and ultimately study more effectively or for more time than they would have if provided only conventional studying materials. Though students may use the IMVH more than the textbook, it is an empirical question whether the videos ultimately improve learning outcomes.

We answer this question using a field experiment involving over 400 undergraduates enrolled in the same microeconomics course over two years. Only students who scored below the median on the first midterm were eligible for the experiment, since previous work and institutional knowledge suggests that students in the top half of the distribution would not benefit (and may even be harmed) from being induced to watch the videos. While the optimal experimental design for identifying average treatment effects would involve restricting access to the IMVH to only treated students, ethical considerations required that all students have access to the IMVH. Hence, we opted for an encouragement design in which treated students are induced to watch more videos than their control group peers through a grade-based incentive, which more than doubled the number of videos watched by treated students. This experimental design permits identification of treatment effects local to those students induced by the encouragement to watch more videos.

We find that being assigned treatment (ITT) increased midterm and final exam scores by 0.18 and 0.17 standard deviations, respectively, and that the marginal hour of video watched increased exam scores (LATE) by 0.08 standard deviations. Although the confidence intervals are, admittedly, wide, the point estimates are statistically and economically significant: a student could increase their course letter grade by one step (e.g. from a B+ to A-) by watching XX hours of videos. Our estimates suggest that XX percent of students in the control group who failed the course would have earned passing grades had they watched as many videos as their treated counterparts.

Although treated students performed better on course assessments, for determining welfare it is important to identify where the time watching videos came from: leisure time, working, student organizations, studying for other classes, studying for current class using other methods, etc. On one hand, if watching videos is more productive than the next best studying method, then the utility of requiring videos is unambiguously positive as students can substitute studying time towards the more productive option. On the other hand, if students must reduce time allocated towards leisure or studying for other classes so they can watch more videos, then the welfare implications are less clear and could be negative depending on the students' preferences.

We attempt to disentangle whether treated students spent more time studying or used their time more effectively by examining proxies for time use including class attendance, visits to a tutoring center (specific to this course), downloading materials from the course website, posting on the class discussion board, and reported time use from an in-class survey. Although our estimates are noisy, we find no statistically significant differences between treatment and control, and we can reject large decreases in take-up of other study methods by treated students. Surprisingly, in nearly all cases, the point estimates suggest that the treatment group used study methods beyond the videos at \textit{greater} rates than did their control peers. Though estimates are noisy, we find no significant differences in reported leisure time across treatment and control. Finally, we investigate spillovers to other courses taken during the same academic term as the experiment and similarly find that treated students perform \textit{better} than their control peers, which suggests that watching the videos likely did not dramatically reduce time spent studying for other classes.

Finally, we attempt to distinguish between two models of student learning which could explain why the video watching inducement improved student exam performance: an incomplete information model where students do not know how to study effectively versus a two-selves model where students like good grades but dislike studying. One important (and testable) difference between the incomplete information model and the two-selves model is what happens after exogenous incentives to watch videos are removed. While the former predicts that students exposed to treatment will continue watching videos given their new knowledge of a relatively productive studying technology, the latter predicts that the students will return to their lower baseline levels of video watching as their doer selves no longer have a commitment device reducing the temptation of immediately gratifying leisure. We examine video watching behavior during the term following the experiment in the subsequent microeconomics course and find that treated students watch significantly more videos than their control classmates, consistent with the incomplete information model.

Collectively, we interpret our findings as strong evidence that requiring studying tools known by the instructor to be effective is utility enhancing for students who manifest their limited knowledge of how best to study, perhaps through poor performance on an early stage assessment. Finally, we provide suggestive evidence that students found the IMVH to be a relatively effective study method by examining video watching across the treatment and controls in the next class in the sequence. The rest of the paper is organized as follows. Section \ref{models} presents competing models of studying behavior that may explain the observed phenomena. Section \ref{background} provides background on existing related literature. Section \ref{studydesign} describes the study design. Section \ref{results} presents the results of the experiment, and Section \ref{discussion} discusses those results. Section \ref{conclusion} concludes.


% *****************************************************************
% Jotting down some thoughts about models below
% *****************************************************************

\section{Models of Studying Behavior} \label{models}

In this section we consider three models of student studying behavior: a neoclassical model, an imperfect information model, and a behavioral/procrastination model. For all three models, we consider the effects of an instructor's inducement to encourage student use an effective study method. We do not address the issue that the IMVH is a relatively unique study tool in that, to our knowledge, it is the first instructional book to be created entirely of videos. However, given the availability of close substitutes to the IMVH (lecture capture for example) we only briefly explore the added issues of inducing students to use a study tool whose usefulness is not known to the instructor.

Neoclassical models of studying behavior assume that rational agents know their returns to studying using the methods available to them and allocate the optimal study time to each method given their utility function, which is increasing in leisure and grades and decreasing in time spent studying. In this model there is no room for an instructor to increase student well-being by intervening in their study decisions. \textcite{oettinger2002} provides some empirical support for the neoclassical model by demonstrating that student effort responds rationally to nonlinear grade incentives. Across 1200 students in a principles of economics class with absolute grading standards, he finds evidence of bunching just above the letter grade cutoffs and student performance on the final exam is higher if the student is just below a grade threshold. %blurb about instructors unable to observe students' utility functions, perhaps students have high returns outside of studying (e.g. networking, dating, etc.)?

In addition to teaching specific skills, many would agree that the ``raison d'etre" of higher education is to teach students how to learn. There is evidence from psychology that college students do not know the return to various study methods.\footnote{See, for example, mccabe2011, prcc2007, drmnw2013} Universities often fund ``Teaching and Learning Centers" or ``Academic Skills Centers," part of whose mission is to help undergraduates learn to study more productively.\footnote{All nine University of California campuses have one. Some others in the US include Dartmouth's Academic Skills Center, Michigan's Center for Research on Teaching and Learning, UNC's Learning Center, and Yale's Teaching and Learning Center} We posit that for many students, a key assumption of the neoclassical model does not hold: that students possess complete information about the returns across studying methods. Instead, we offer the alternative hypothesis that students supply a quantity of study time that is optimal given their information constraints. In this `imperfect information' model, students choose study methods and quantities that are suboptimal relative to those they would have picked in a full information setting. Hence, an intervention by an entity that has more information about returns to studying across various methods (i.e. an instructor) can enhance student utility.

A third model is a behavioral one in which students plan to study more than they end up studying when the time comes. This phenomenon is consistent with two-self models in which a person's ``planner" self, the one who desires high grades at the expense of leisure, is at odds with her ``doer" self who must choose between immediately gratifying leisure and delayed gratification from higher grades.  Indeed, survey and experimental data suggest that many students study less than they report they ``should" and finish the term with grades lower than what they had anticipated they would earn at the start of the term,
\footnote{see, for example, \textcite{ferrari1992},  \textcite{ccog2017} and \textcite{llo2016}.}  \textcite{cgpr2020} provide empirical evidence that setting tasked-based goals helps improve college student performance.

We consider the testable implications of the three models applied to a setting where students are incentivized to use a time-consuming educational input, say, a set of instructional videos (or attending class, reading the textbook, answering homework problems, etc.). The incentive is structured such that students who consume the educational input receive a higher grade in the course by consuming a set level of the input. In this simple setting, students gain utility only from leisure and grades. We assume grades, a function of time spent studying, and utility are both continuous, smooth, and increasing and concave in their inputs. Students can choose to study using the incentivized educational input or some outside option that is not directly incentivized (or a combination thereof).

Across all three models, before the first educational input is incentivized, students allocate time to the two studying methods until the marginal benefit of each (through higher grades) is equal to the marginal cost of forgone leisure. Consider the population of students initially consuming below the requisite level to earn the grade incentive. These students must decide if earning the grade incentive is worth forgone leisure and less time allocated to their outside studying option. Next we explore the differences in predictions across the three models.

% For the `compliers' - those for whom the incentive induces greater take-up of the incentivized input -

% Depending on the relative returns to studying and the students' preferences, it is theoretically ambiguous how adding the incentive will affect grades. Next we highlight the primary differences between the models.

% Do we want to write formal models? I think we should...it may be easier for some to see the implications if presented as equations.

\begin{figure}
\begin{center}
  \begin{tikzpicture}[scale=0.6]
		% axes
		\draw [thick,<->] (0,8) node[left]{$v^*(L)$}--(0,0)--(11,0) node[right]{$L$};
		\node [below left] at (0,0) {$0$};

		% v*
		\draw (0,5) -- (2,4);
		\draw (2,4) -- (6,4);
		\draw (6,4) -- (6,2);
		\draw (6,2) -- (10,0);
		\draw [dashed] (2,4) -- (6,2);

		% video incentive threshold
		\node [left] at (0,4) {$T$};
		\draw (-0.1,4) -- (0.1, 4);
		\node [below] at (6,0) {$1-T$};
		\draw (6,-0.1) -- (6,0.1);
		%\draw [dashed] (0,4) -- (10,4);
		%\draw [dashed] (6,0) -- (6,10);

		% points for U1 and U2
		\draw [fill] (4.4,2.8) circle [radius=.05];
		\node [below] at (4.4,2.8) {$v_1$};
		\draw [fill] (3.95,4) circle [radius=.05];
		\node [above] at (3.95, 4) {$v_2$};

  \end{tikzpicture}

  \vspace{1\baselineskip}

	% Utility curves
	\begin{tikzpicture}[scale=0.6]
	% axis
	\draw[thick,<->] (0,8) node[left]{$G(v^*,o^*)$}--(0,0)--(11,0) node[right]{$L$};
	\node [below left] at (0,0) {$0$};
	% video incentive threshold
	\node [below] at (6,0) {$1-T$};
	\draw (6,-0.1) -- (6,0.1);

	% production frontier 1 - not incentivized
	\draw (10,0)--(10,2);
	\draw plot [smooth] coordinates {(10,2) (8,4) (6,5) (4,5.6) (2,5.85) (0,6)};

	% production frontier 2 - incentivized
	\draw plot [smooth] coordinates {(10,0) (8,2) (6,3)};
	\draw (6,3)--(6,3.75);
	\draw plot [smooth] coordinates {(6,3.75) (5,4.75) (4,5.3) (2,5.85)};

	% indifference curves
	% original
	\draw[blue] (4,5.65) arc[start angle=252, delta angle=15, radius=9cm];
	\draw[blue] (4,5.65) arc[start angle=252, delta angle=-19, radius=9cm] coordinate (u1);
	\node[left] at (u1) {\scriptsize $U_1$};
	% tangency
	\draw [fill] (4.4,5.52) circle [radius=.05];
	\node [above] at (4.4,5.52) {$v_1$};

	% new
	\draw[blue] (4,5.32) arc[start angle=252, delta angle=9, radius=14cm];
	\draw[blue] (4,5.32) arc[start angle=252, delta angle=-12, radius=14cm] coordinate (u2);
	\node[left] at (u2) {\scriptsize $U_2$};
	% tangency
	\draw [fill] (3.95,5.33) circle [radius=.05];
	\node [below] at (3.95, 5.33) {$v_2$};

	% grade incentive
	\node [left] at (0,3.75) {$G(T,0)$};
	\draw (-0.1,3.75) -- (0.1,3.75);
	\node [left] at (0,2) {$I(v \geq T)$};
	\draw (-0.1,2) -- (0.1,2);


	\end{tikzpicture}

	\caption{\textit{Above}: Demand curve for video watching as a function of leisure $L$. At $L=1-T$, the student maximizes grades $G(v,0)$ by spending all studying time watching videos, i.e. $v^*=T$. \textit{Below}: Student's utility maximization problem for the neoclassical model. The student maximizes her utility over leisure $L$ and grades $G$, which is a function of time allocated to video watching $v$ and her next best studying option $o$. The grade incentive $I$ is given to the student conditional on watching $T$ hours of videos (inner-time budget constraint) or, in the unincentivized case, given regardless of video watching (outer time-budget constraint).}
\end{center}
\end{figure}



\begin{figure}
\begin{center}
% Utility curves
\begin{tikzpicture}[scale=0.6]
% axis
\draw[thick,<->] (0,10) node[left]{$G(v^*,o^*)$}--(0,0)--(10,0) node[right]{$L$};
\node [below left] at (0,0) {$0$};
% production frontier 1
\draw (9,0)--(9,1);
\draw[domain=0:9, smooth] plot ({\x},{2*sqrt(9-\x) + 1});
% production frontier 2
\draw[domain=5:9, smooth] plot ({\x},{2*sqrt(9-\x) + 0});
\draw (5,4)--(5,4.7);
\draw[domain=0:5, smooth] plot ({\x},{2.2*sqrt(9-\x) + 0.3});
% indifference curves
% kink
\draw[blue] (5,4.7) arc[start angle=232, delta angle=10, radius=14cm];
\draw[blue] (5,4.7) arc[start angle=232, delta angle=-10, radius=14cm] coordinate (u1);
\node[above] at (u1) {$U_1$};
% tangency
\draw[blue] (6.5,4.2) arc[start angle=235, delta angle=15, radius=9cm];
\draw[blue] (6.5,4.2) arc[start angle=235, delta angle=-19, radius=9cm] coordinate (u2);
\node[above] at (u2) {$U_2$};
\end{tikzpicture}
% Isograde curves
\begin{tikzpicture}[scale=0.6]
	% axis
	\draw[thick,<->] (0,10) node[above]{$G(v^*,o^*)$}--(0,0)--(10,0) node[right]{$L$};
	\node [below left] at (0,0) {$0$};
	% Isograde curves
	\draw (4.45,5.3) to [out=-90, in=140] (5.75,3.2) to [out=-40, in=160] (7.7,2.2);

	\draw (4.15,4.3) to [out=-90, in=120] (4.55,3) to [out=-60, in=160] (7.4,1.1);

	\draw (0,6.45) to [out=0, in=115] (4.4,3.2) to [out=-65, in=90] (5.05,0);
\end{tikzpicture}

\caption{Student's utility maximization problem for the neoclassical model. The student maximizes her utility over leisure $L$ and grades $G$, which is a function of time allocated to video watching $v$ and her next best studying option $o$.}
\end{center}
\end{figure}

In the neoclassical model, the marginal return to grades of the incentivized input is less than that of the outside option for the `compliers', or those induced by the incentive to consume at least a fixed level of the incentivized input. This model predicts bunching at the incentivized level cutoff since compliers would prefer to spend their marginal hours on leisure or studying with their other method. This model predicts a strict increase in video watching and weak decrease in other studying and leisure consumption. It is ambiguous whether cumulative study time increases or decreases as this depends on relative utility benefits of leisure and grades and the returns to studying by each method. However, if cumulative study time remains constant or decreases, then exam performance should strictly decrease since students are now suboptimally allocating study time versus their first-best allocation when considering only marginal returns to studying. On the other hand, if cumulative study time increases, students may earn greater exam grades but achieve lower utility compared to baseline. Importantly, this model predicts that in subsequent quarters students return to their pre-incentive levels of studying.

In the imperfect information model, students' \textit{ex ante} allocations to each studying method are not necessarily first-best. Compliers update their priors about the returns to watching videos as they work towards hitting the minimum required level. At this cutoff, they make a decision whether to continue watching videos depending on their updated perceptions of the marginal benefit. Hence, bunching at the cutoff is predicted only if the updated marginal benefit at the cutoff is lower than the marginal benefit of the next best studying option or the marginal utility of leisure.

 that the marginal benefit at the cutoff is greater (lower) than the marginal benefit of their next best option

the incentive to watch videos will increase exam performance as long as total study time does not fall. A sharp prediction is that video watching will continue at the incentivized level in the absence of the grade incentive as students have learned an effective study tool. We also expect the treatment effect to be greater for students with more information problems and one plausible group is transfer students who are taking their first class at UC San Diego, their first upper division class and, typically, their first class under the quarter system (vs semester).

Finally, in the behavioral model, the instructor's inducement helps students stick to study plans. As long as total study time does not fall, the inducement will increase exam performance. In the absence of the inducement, a sharp prediction is that video watching will revert to pre-inducement levels.

In the empirical section, we test for the effects of being induced to watch the IMVH on both exam scores and several other study methods students could use to learn microeconomics (lecture attendance, visits to a class-specific tutoring lab, use of a class discussion board, downloads from the class web page). We also compare grades in other classes taken in the same quarter across the treatment and control group. For a subset of our sample we have survey responses on total study time in the quarter and leisure time. Since the experiment was conducted in the first of a required three-class sequence, we examine video watching in the second class. We test whether the effect of the inducement to watch videos is greater for transfer students (assumed to have more information problems) and non-native English speakers (assumed to benefit the most from closed captioning, a key feature of the IMVH).

% *****************************************************************
% Background
% *****************************************************************

\section{Related Literature and Contributions} \label{background}

Students have many time-consuming activities to help them learn including attending class, watching recorded lectures, reading the textbook, doing homework, attending office hours, etc. There are several empirical challenges to estimating the causal effects of a learning activity. First, unobserved student characteristics, such as ability and motivation, are likely positively correlated both with the use of a learning activity and class performance. To estimate causal effects, empirical studies must address the selection into using a study method. Second, most instructors have experience working with motivated students who want to improve their study strategies after a negative exam shock and \textcite{oettinger2002} and \textcite{ss2008} provide empircal evidence of dynamic selection.\footnote{\textcite{oettinger2002} finds that students close to a grade threshold before the final exam perform better on the final}  Dynamic selection means that including student fixed effects in class performance regressions, a common empirical approach in this literature, will not uncover causal effects. Third, learning activities are substitutable and inducements to use one study strategy may affect student use of another. In these cases, even randomized experiments will not identify the causal effect of a study method but will rather identify the causal effects of a study policy and all of the changes in behavior caused by the policy. The causal effects of an educational policy, such as requiring homework, are likely useful for educators considering how to design their classes but are less useful for students wanting to know the relative effectiveness of study strategies. Fourth, the existence of the resource may change how the instructor teaches.  For example, several papers discuss how lecture capture changes the way instructors lecture.  Again, experiments randomly assigning student to classes taught one way versus another will not identify the causal effect of a study method if other aspects of learning transmission are changed.  A fifth empirical issue is that study strategies all take time and so most empirical studies jointly test the effectiveness of a particular learning method and devoting more time to the course. It is possible that the primary benefit to students is simply devoting more time to learning the course material, regardless of the study method.

Our study uses a randomized control trial as well as a RD approach which solves issues (1) and (2).  Since we use within-class randomization, we solve issue (4).  Our study does not solve issues (3) and (5), however, we have empirical evidence on a large number of alternative study methods and test if the inducement to use the video book affected these other study methods to shed light on issue (3) and we have survey data on study time for the course for a selected sample of students which we use to test whether total study time for the course differs across treated and control students to shed light on issue (5).

Attending class: \textcite{cl2008} estimate the average effect of attending lecture for those who attend lecture (average treatment effect on the treated). The same instructor taught two required public finance classes of size 67 students and 47 students.  The instructor provided identical PowerPoints to both classes after lecture, but did not lecture on randomly chosen topics to a randomly chosen class.  The outcome is whether the student answers a multiple choice exam question correctly.  Physically attending lecture involves transportation costs and this costly aspect of lecture is not captured by the authors since the exam score comparison is across students who were all in class.  However, this study does identify the effect of attending a remote lecture versus having a written explanation.

\textcite{dgm2010} analyze a policy where lecture attendance was voluntary before the midterm, but after the midterm, students scoring below the median were required to attend class. The policy affected 352 students taking three classes, two intermediate micro and one econometrics class. The policy led to a 36 percentage point increase in post-midterm attendance at the threshold. Using a regression discontinuity design, they find that a 10 percentage point increase in overall attendance results in a 0.17 standard deviation increase in the final exam score. They find no effect of the attendance policy on grades in other classes taken the same quarter, attending TA sections, homework scores, and the use of university tutors.
% Zack--They also found positive but not statistically significant spillover effects on other classes. They hypothesize could be due to fixed costs of coming to campus.
\textcite{ans2012} study section (as opposed to lecture) attendance across intermediate microeconomics, intermediate macroeconomics and econometrics for 444 students. The authors find that absenteeism depends on day of week and time of day and, since students are randomly assigned to sections, use these variables as instruments for absenteeism. They also include student fixed-effects. Surprisingly, they find significant attendance effects only for students in the top quantiles: missing 10 percent of sections results in a 1 percentage point performance loss. The authors have no information about other uses of the student's time use, including attending the main lecture.

Effect of Homework: \textcite{ts2012} randomly require two-thirds of students taking Principles of Economics classes to complete a one of three homework assignment for a grade. The other third may complete the homework, but it does not contribute to their grade. The outcome measure is exam performance on questions related to the three homework assignments. The authors use the score on the remaining exam questions as a control variable. They find significant effects pf homework on the first midterm but not the final exam. \textcite{gr2013} use within-class randomization to estimate the effects of required homework for 423 microeconomics principles students. A coin flip determined whether a student was in the treated group, where course points are based on both homework and exams, or in the control group, where all course points are based on exams. Treatment led to a 58 percentage point increase in completing all homework assignments and a 84 percentage point increase in completing the majority of homework assignments. They find that treated students are less likely to drop the class and score higher on the first two but not the last two exams. The average across the four exams is increased 5-6 percent by treatment and the control group GPA would increase from 2.44 to 2.68 if they had been required to do homework. They find three times larger treatment effects for students who initially fail the first exam (10 to 15 percent vs 4 to 6 percent increase in average test scores). The authors do not examine whether other uses of student time are affected by the homework policy.

Effect of Study time: In a convincing empirical paper on the effects of study time, \textcite{ss2008} examine 210 Berea College students who were randomly assigned a roommate. Students whose roommates brought a video game to college, earn lower grades and spend less time studying. They authors instrument for study time using presence of a roommate with a video game and find that a one hour increase in study time per day (a .67 standard deviation increase in their sample) has the same effect of first semester GPA as a 5.21 increase in the ACT (an increase of 1.4 standard deviations in their sample).

Other researchers have investigated using technology to improve learning. \textcite{abbm2020} conduct an experiment in Botswana during the COVID-19 pandemic and find that text messages and phone calls deployed as low cost, scalable learning technologies improved test scores by 0.16 to 0.29 standard deviations.

Effect of Recorded Lectures: An educational resource closely related to the IMVH is when instructors record their lectures and make them available to students. Recorded lectures have course administration information which the IMVH does not have. Compared to an IMVH video, recorded lectures are typically much longer, less organized, and may include components that do not work well when recorded, such as group work or class discussion.  \textcite{savage2009} taught two intermediate micro classes: one 42-student class had "talk and chalk" lectures and the other 45-student class used technology that allowed lecture capture which was then made available to the students. The author finds no significant differences in observed student characteristics across the two sections but exam performance was significantly higher across the classes.

Effect of Setting Goals: \textcite{cgpr2020} explore the effect of having students set goals on class performance. They find that setting \textit{task-based} goals of completing a specific number of online practice exams improves student performance on exams. Those randomly assigned to complete task-based goals completed 0.10 standard deviations more practice exams and increased total course points by .07 standard deviations. The authors found no effect of setting \textit{performance-based} goals of achieving specific grades in the course or on exams on total course points. In the present study, we set for the students a \textit{task-based} goal of watching a specific number of videos. Unlike the

caution that it is important that the tasks are based on a productive study method. The authors also explored performance-based goals, getting specific grades in the course or on exams, did not have an effect on course points.  In the intervention studied in this class, the instructor gave the student a task-based goal, as opposed to the student setting theiwas set by the instructor as opposed to the student and there was a direct grade penalty to not achieving the goal.

This study adds to this body of research by studying the effectiveness of an educational innovation: a video textbook. We randomly assign half the students scoring below the mdican on the first midterm to a grading scheme which placed 4 percent, or 40 points, of the student's grade on watching 40 videos and down-weighted the first midterm by 4 percent.  This experiment allows two empirical strategies to test for causal effects: within class randomization for students scoring below the median on the first exam and a regression discontinuity approach at the median first exam score. We examine a large set of student study behaviors (lecture attendance, homework downloads from course web page, contributions to a discussion board, use of a class-specific tutoring lab) to determine if any of these study methods are substitutes or complements with video watching. We test for spillovers to other classes taken in the same quarter. We test for heterogeneous treatment effects using techniques that are robust to p-hacking. Finally, our research setting allows us to examine video views in the absence of the grade incentive in the next, required intermediate microeconomics class.

% *****************************************************************
% Study design
% *****************************************************************

\section{Study Design} \label{studydesign}

\subsection{Description of the Sample}

We conducted the field experiment in an undergraduates intermediate microeconomics course taught during fall 2018 and fall 2019 by one of the authors. The university is a large, diverse and selective public research university in the United States.\footnote{The Carnegie Classification of Institutions of Higher Education classifies the university as an R1 (very high research activity) university. For the 2017-2018 academic year, the undergraduate student body shared the following demographics: 49.1\% female, 50.6\% male; 75.0\% in-state, 5.5\% out-of-state, and 19.5\% international; 59\% students of color; 28.6\% majoring in the social sciences, 26\% of which major in Economics. Among newly admitted students, about one-third were transfer students, and average SAT scores were 652 and 605 for math and critical reading, respectively. About 34\% of students are the first in their family to attend a four-year university.} At this institution, intermediate microeconomics is a three-quarter sequence required for students majoring in Economics. The experiment was conducted in the first course of the sequence, \textit{Micro A}. We also observe grades and video watching in the second course of the sequence, \textit{Micro B}, which was taught by the same instructor during both the winter 2018 and winter 2019 quarters. Both Micro A and B instructors created half of the videos relevant to their course in the IMVH.

The structure is similar across the three courses in the Micro sequence. Students have the option to attend one of two lectures offered back to back twice per week, each lasting about 90 minutes. Two midterm and final exams are held at a common time outside of lecture. In addition to lecture, students have access to weekly one-hour discussion sections run by graduate teaching assistants (TAs) who are all Economics PhD candidates, including one of the authors. In lieu of office hours, the TAs and Undergraduate Instructional Assistants (UIAs) staff a tutoring lab open between three and four hours a day, six days per week. Students may also attend weekly Supplemental Instruction (SI) sessions offered by undergraduates majoring in Economics and trained by the university in SI. Besides the IMVH, students have access to a variety of learning resources online including a discussion board (moderated by the instructor, TAs, and UIAs), four years of previous exam questions, weekly (ungraded) problem sets, and semi-weekly (graded) online quizzes.

% blurb about the curriculum? Are lectures recorded?

% TODO: rename PSET "tutoring lab", Piazza "discussion board"

% why might the results be externally valid, or at least more interesting beyond UCSD?

Students were told about the experiment during the first lecture and provided an informed consent form in the syllabus. At any time during the quarter, students could opt out of having their data included in the analysis.\footnote{Students could opt out via an online form visible to a third party university organization so that neither the instructor nor research team could observe which students elected to opt out.} Students below the age of 18 at the start of the course as well as students enrolled via the university's extension program were removed from the analysis dataset.\footnote{Students under the age of 18 were excluded per IRB protocol. We exclude extension students because of their potentially very different preparation for the course and our inability to observe pretreatment covariates and outcomes outside of Micro A.} Ultimately, four students under 18, five extension students, and seven students who opted-out were removed from the analysis dataset, leaving a sample of 850 students.

There are two unique features of the class worth noting. First, many non-econ majors take the class to either satisfy general education requirements or to explore majoring in economics. As there are many students in the experiment on the margin of majoring in economics, an important outcome is the likelihood the student takes Micro B. Second, about 37\% of the class is transfer students, for whom the class is not only their first experience with upper division coursework at a four-year research university, but also typically their first time taking classes under the faster-paced quarter system.\footnote{Community colleges, the most common previous institution for transfer students, are on the semester system in the state of the university.} We examine treatment effect heterogeneity to understand how transfer students might differentially benefit from the IMVH.

\subsection{Description of the IMVH}

The Intermediate Microeconomics Video Handbook (IMVH) is a collection of 220 short videos that cover the material in a year-long intermediate microeconomics course sequence.\footnote{A preview of the IMVH can be found at \url{https://iti.ucsd.edu/IMVH_Misc/Promo/IMVHPromo.html}.} The videos, designed to complement or replace a course textbook, include graphical and verbal intuition as well as formal algebraic and calculus-based definitions and proofs.

The videos were created by six UC San Diego faculty members with professional videographer and production support. Many videos utilize the "learning glass," an innovative presentation technology where instructors write with neon markers on a large sheet of glass that has lights embedded along the glass edge to make the colors pop. The remaining videos feature faculty superimposed in front of slides. Videos are closed captioned and were checked by graduate students for accuracy.

Given the complexity of the material, a key objective was to keep the web interface clean and simple so as not to distract from the content. The videos are organized by content area (e.g., consumer theory, producer theory, etc.) that help students understand where various topics "live" in intermediate microeconomics. When considering the design of the platform, one goal was to help students find material quickly. Besides a table of contents and an index, each video contains time stamps of the concepts therein. Another helpful feature is searchable captions, which allow the student to jump to the part of a video containing the searched-for word.

While we do not know of another textbook completely comprised of videos, the IMVH is similar to Khan Academy, lecture podcasts, and textbook websites that incorporate instructional videos. Besides the engaging viewable nature, the IMVH differs from a traditional textbook in that the instructors explain, graph, and derive mathematical results in much the same way one would in a conventional lecture. However, the IMVH differs from lectures in that students control the pace: they can rewatch, speed up, or slow down the videos. Another difference is the ability to read captions and the reduced demand on the instructor's time. We summarize some options to present course material to students in Table \ref{tech_comparison}.\footnote{This table is a modification of a classification Martin Osborne proposed to one of the authors in an e-mail correspondence.}

% alternative checkmark option:
% \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\begin{table}
	\caption{Comparison of information transmission formats} \label{tech_comparison}
	\centering
	\begin{tabular}{|c*{4}{|>{\centering\arraybackslash}m{0.1\linewidth}}|}
		\hline
		Feature & Lecture & eText-book & Lecture Capture & IMVH\\
		\hline
		Instructor's time used & \checkmark & & &\\
		Instructor-learner interaction & \checkmark & & &\\
		Learner-learner interaction & \checkmark & & & \\
		Readable & & \checkmark & ? & \checkmark \\
		Scalable & ? & \checkmark & \checkmark & \checkmark \\
		Searchable & & \checkmark & & \checkmark \\
		Skimmable & & \checkmark & & \checkmark \\
		Stoppable & ? & \checkmark & \checkmark & \checkmark \\
		Watchable & \checkmark & & \checkmark & \checkmark \\

		\hline

	\end{tabular}
	\label{infotransmission}
\end{table}

\subsection{Experiment Design} \label{expdesign}

The experiment began four weeks into the term following the first midterm exam. All students who scored above the median on the first midterm, the \textit{Above median} arm, and half of students who scored below the median, the \textit{Control} arm, were assigned a conventional grading scheme that places weight only on exams and quizzes. The remaining half of students below the median we assigned to the \textit{Incentive} arm, whose grading scheme allots four percentage points conditional on watching at least 40 of 48 eligible videos in the IMVH.\footnote{Watched in standard speed, 40 videos would require students to spend between 5.5 and 7.1 hours, depending on the length of videos chosen (on average 9.7 minutes in length each). Watching all 48 incentivized videos in standard speed would require just shy of eight hours.} The 48 eligible videos comprise a subset of the 74 course-relevant videos that cover new class material since the first midterm. The two different grading schemes are outlined in Table \ref{gradescheme}. Notably, the four percentage points come at the expense of reduced weight placed on the first midterm score, which had already occurred at the time of treatment assignment. Hence, at the time of treatment assignment, the video incentive is the sole forward-looking difference between treatment arms.

\begin{table}
	\caption{Grade scheme by treatment arm. \textit{Control} represents same grade scheme as \textit{Above median}. Differences between the two grade schemes in bold.}
	\centering
	\begin{tabular}{ c|c|c }
		Assessment & Incentive & Control \\
		\hline
		$>$40 videos & \textbf{4\%} & \textbf{0\%} \\
		Midterm 1 & \textbf{18\%} & \textbf{22\%} \\
		Midterm 2 & 22\% & 22\% \\
		Final Exam & 50\% & 50\% \\
		Math Quiz & 1\% & 1\% \\
		Best 5 of 6 Quizzes & 5\% & 5\% \\
		\hline
		Total & 100\% & 100\% \\
	\end{tabular}
	\label{gradescheme}
\end{table}

To improve balance between \textit{Incentive} and \textit{Control} arms and increase statistical power, we assigned students to treatment arms using paired randomization \parencite{ai2017}, matching students by their midterm scores before randomly assigning one member of each pair to \textit{Incentive} and the other to \textit{Control} (Further details on treatment assignment can be found in Appendix \ref{a_randomization}). We emailed each student letting them know their assignment and grading scheme. Students could also find their assignment listed in the online gradebook. To confirm that students correctly knew their assignment, we surveyed students using an in-class attendance quiz, and 94\% of students correctly identified their grading scheme. We emailed the students who responded incorrectly to clarify their assignments.\footnote{11 of 164 \textit{Incentive}, 23 of 167 \textit{Control}, and 10 of 373 \textit{Above median} students did not identify their grading schemes correctly. 146 students did not answer the quiz, many of whom had dropped the course following the first midterm.}

We informed \textit{Incentive} students that they must watch the entire video and only one video at a time to get credit towards their 40 required videos. However, it is impossible to truly verify ``watching" as students could, for example, minimize their browser, walk away from their computer, or otherwise play a video without actively engaging with it. As a proxy for watching, we use the data recorded by the IMVH software, which includes timestamps for when a student opens and closes a video. We define ``duration" as the time between opening and closing a video, and since students could watch a video at up to 2X speed, we required that the duration be at least half of the runtime of the video for the video to count towards the 40 required.

We helped students keep track of their progress towards 40 videos by periodically updating the online gradebook with counts determined from the IMVH data. Although most students followed the proscribed protocol, a few students ignored our instructions, opening 40 video links within a matter of a few minutes. These students, after seeing ``0 videos watched" in the gradebook, often emailed the instructional staff claiming they had watched 40 videos, to which we replied with a reminder that videos must be watched completely from start to end to count.

Although it is clear that a student cannot gain much from whatever ``watching" may have occurred in the span of the few minutes during which students opened and closed 40 links, to aid in interpretability, we calculate four measures of video watching:
\begin{enumerate}
	\item \textit{Videos}: Number of links opened, including duplicates
	\item \textit{Unique videos}: Number of unique links opened
	\item \textit{Hours of videos}: Total hours that the video link is open
	\item \textit{Hours of unique videos}: Total hours counting each video once, keeping the longest duration recorded among duplicates
\end{enumerate}

For the duration measures, we top code duration at the runtime of the video to remove bias from, for example, students leaving a video link open after the video concludes. About 27\% of video durations are top coded.\footnote{17\%, 13\%, and 7\% of durations are longer than 1.5X, 2X, and 3X the runtime of the video, respectively.} Additionally, 43\% of timestamps in the IMVH failed to register an end timestamp. For these videos, we impute duration by multiplying each student's mean percent of runtime watched by the runtime of the video being imputed. 15 students, watching an average of 2.1 videos, have no runtime recorded for any of their videos. For these students, we use the mean percent of runtime watched by students with 3 or fewer videos watched.

To ensure fairness, we informed students that final letter grades would \textit{not} be affected by being in the experiment. We accomplished parity between \textit{Control} and \textit{Incentive} arms through curving final grades. First, we applied a curve to the \textit{Control} and \textit{Above median} arms as one group to achieve a grade distribution in line with that of previous cohorts. Second, we curved the \textit{Incentive} arm's scores so that the mean score was equal to that of the \textit{Control} arm after curving in the pervious step. Since course grades are by construction \textit{ex post} equal between the \textit{Control} and \textit{Incentive} arms, we use the second midterm and final exam scores as our primary outcomes of interest. As secondary outcomes of interest, we examine term GPA and number of courses passed, restricting to both econ and non-econ courses. We also examine spillover effects to other forms of studying in Micro A and to video watching and exam scores in Micro B.

% *****************************************************************
% Empirical Strategy
% *****************************************************************

\subsection{Empirical strategies} \label{empiricalstrat}

In this paper, we estimate the effect of being assigned to the \textit{Incentive} arm on our outcome variables of interest, Intent To Treat (ITT) effects, as well as the effect of watching videos for those induced by the incentive to watch more videos, a Local Average Treatment Effect (LATE), also referred to as a Complier Average Treatment Effect (CATE) \parencite{ir2015}.

Below we outline the empirical strategies for estimating both the ITTs and LATEs.

\subsubsection{Intention To Treat (ITT)}

In this section, we examine the empirical strategy for estimating the causal effect of being assigned to the \textit{Incentive} arm on outcomes of interest, such as exam scores. These are ITT estimates and not average treatment effect estimates because of two-sided non-compliance: some students in the treatment arm do not watch videos and some students in the control arm do watch videos. Since the incentive itself in our setting is representative of how future instructors may induce their students to watch videos, the ITT estimates are policy-relevant for instructors considering adopting the IMVH or other video-based learning methods in their courses.

Our baseline ITT specification is the partially linear model:

\begin{equation} \label{itt_spec}
	Y_i = \beta Z_i + f(X_i) + \epsilon_i
\end{equation}

where $Y_i$ is an outcome of interest (e.g. videos watched or test scores) for student $i$, $Z_i \in \{0,1\}$ is a treatment indicator with those in the \textit{Control} arm having $Z_i=0$ and those in the \textit{Incentive} arm having $Z_i=1$, $f()$ is a generic function through which $X_i$, a vector of controls, affects $Y_i$, and $\epsilon_i$ is an unobserved residual. $\beta$, our parameter of interest, is the causal effect of being assigned to the \textit{Incentive} arm on the outcome of interest $Y$, assumed to be constant across the population.\footnote{Our experiment takes place over two years, and we pool the sample across both years. Out of the 850 student-years, one student repeated the course in both years, and hence there are 849 unique students. For simplicity, we drop the subscript $t$ from our specifications, treating the one repeating student independently across years. Dropping this student from the sample leaves the results virtually unchanged.} Under the assumption of unconfoundedness, $\wh{\beta}$ is an unbiased estimate of the ITT effect \parencite{ir2015}.\footnote{Though we cannot test whether $Z_i$ is confounded by unobservable covariates, we have confidence in this assumption given the random assignment of $Z_i$ and the balance across observable covariates as demonstrated in Table \ref{balance_table_mid2} and \ref{balance_table_final}.}

In our baseline estimation of Equation \ref{itt_spec}, we include in $X_i$ year indicators and first midterm score, following the advice of \textcite{bm2009} to control for all covariates used in seeking balance. In a second model, we include additional controls chosen using the Post-Double-Selection (PDS) procedure of \textcite{bch2014a}, explained in detail in Appendix \ref{a_selection}. In a third model, to check that our results are robust to potentially nonrandom attrition by treatment arm, we fit Equation \ref{itt_spec} including pair fixed effects. These fixed effects subsume the year indicator (since pairs were assigned separately across years), so we drop the year indicator but keep midterm 1 score to control for small differences within pairs along that dimension. As entity fixed effects require at least two observations within the entity to be estimable, we drop any students whose matched pair attrited.

In our results, we present an additional nonparametric estimate using Neyman's (\citeyear{neyman1923}) repeated sampling approach, considering each pair (block) an independent, completely randomized experiment and averaging the results. We estimate the point estimate of the ITT as the mean difference in outcomes across pairs:

\begin{equation} \label{neyman_spec}
	\wh{\tau} = \frac{1}{J}\sum_{j=1}^{J} \wh{\tau_j} = \frac{1}{J}\sum_{j=1}^{J} y^\text{obs}_{j,I} - y^\text{obs}_{j,C}
\end{equation}

where $\wh{\tau}$ is the point estimate of the ITT, $J$ is the number of pairs in the sample, and $\wh{\tau_j} = y^{obs}_{j,I} - y^{obs}_{j,C}$ is the observed difference in outcome for pair $j$. The estimated standard error of $\wh{\tau}$ \citep{imai2008, ir2015, ai2017} is:

\begin{equation} \label{se_neyman}
	\wh{SE}(\wh{\tau}) = \Big(\frac{1}{J}\sum_{j=1}^{J} \wh{V}(\wh{\tau_j}) \Big)^\frac{1}{2}
\end{equation}

where $\wh{V}(\wh{\tau_j})$ is the estimated variance within block (pair) $j\in \{1,...,J\}$. This within-block variance given one control and one treated unit per block is (\cite{ir2015}, \cite{ai2017}):

\begin{equation} \label{v_block}
	\wh{V}(\wh{\tau_j}) = s_{j, I}^2 + s_{j, C}^2
\end{equation}

where $s_{j, I}$ and $s_{j, C}$ are the \textit{Incentive} and \textit{Control} sample variances within block $j$, respectively. Unfortunately, these sample variances are not estimable in a matched-pair setting as there is only one unit in each arm per block. As such, we use the following estimator, which is conservative (confidence intervals wider) if there is heterogeneity in the treatment effect (\cite{imai2008}, \cite{ir2015}, \cite{ai2017}):

\begin{equation} \label{se_imai}
	\wh{SE}(\wh{\tau_j}) = \Big(\frac{1}{J(J-1)}\sum_{j=1}^{J} (\wh{\tau_j} - \wh{\tau})^2 \Big)^\frac{1}{2}
\end{equation}

Similar to the fixed effect model, the Neyman repeated sampling approach is only estimable if we drop all students whose matched pair attrited. This drop in observations increases the width of our confidence intervals, albeit modestly since including only matched pairs reduces unexplained variance in the outcome variables of interest. We present estimates from all four models to demonstrate that the results are generally similar and not sensitive to model specification or choice of control variables.

\subsubsection{Local Average Treatment Effect (LATE)}

Here we examine the empirical strategies for estimating the causal effect of watching videos on outcomes of interest, exam scores. We discuss Two-Stage Least Squares (2SLS) estimation of LATEs for those students below the median score on the first midterm induced by the incentive to watch videos.

The average causal effect of watching videos can be modeled using the potential outcome framework or Rubin Causal Model \parencite{ir2015}:

\begin{equation}
	\gamma = \operatorname{E}[Y_i(v_i|Z_i=1) - Y_i(v_i|Z_i=0)]
\end{equation}

where $\gamma$ is the causal effect of watching videos and $Y_i()$ and $Y_i(0)$ are the potential outcomes (exam scores) for a student $i$ who does and does not watch videos, respectively. We can observe each student's exogenously assigned treatment status $Z_i \in \{0,1\}$, maintaining the notation from Equation \ref{itt_spec}. This instrument affects $Y_i$ by influencing $v_i$, an observed, endogenous quantity of videos watched. We also observe a vector of pretreatment covariates $X_i$ and outcomes $Y_i$.

Because a student's decision to watch videos is endogenous, regressing $Y_i$ on $v_i$ will provide biased estimates of $\gamma$. To calculate unbiased estimates, we rely on variation in $v_i$ induced by exogenous assignment of the instrument $Z_i$. We estimate $\gamma$ using 2SLS:

\begin{equation} \label{firststage_spec}
	v_i = \alpha Z_i + f(X_i) + e_i
\end{equation}
\begin{equation} \label{secondstage_spec}
	y_i = \gamma \wh{v}_i + g(X_i) + u_i
\end{equation}

where $\wh{v}_i$ is instrumented videos estimated by Equation \ref{firststage_spec}, $f()$ and $g()$ are generic functions through which $X_i$ affects $v_i$ and $y_i$, respectively, and $e_i$ and $u_i$ are unobserved model residuals assumed to be mean zero conditional on observables. $\gamma$ is the LATE of watching videos, local to those students induced by the incentive to watch videos.

Under the assumptions of excludability, monotonicity, and non-interference, $\wh{\gamma}$ is an unbiased estimate of the LATE \parencite{ai1995}. Excludability assumes that outcomes (grades) are only affected by the instrument (incentive) through watching videos. This assumption could be violated if, for example, telling a student she is treated were to give her more confidence on subsequent exams during the quarter. Monotonicity, sometimes referred to as the ``no defiers" assumption, is necessary because of two-sided noncompliance and requires that students assigned treatment watch weakly more videos than they would if they were assigned control. A violation of this assumption could occur if students get utility from rebelling against their assigned grade scheme. Non-interference, also known as the Stable Unit Treatment Value Assumption (SUTVA), assumes that each student's outcome depends only on their own treatment status and not the treatment status of their peers. Violations of SUTVA may include control students benefiting from having treated students in the same class and, perhaps, studying together.

Although we believe excludability\footnote{While this assumption is not testable, we took care in the experimental design to make the treatment and control arms as similar as possible except for the grading schemes. Of course, watching videos inherently requires time that takes away from some other activity. Hence, the results should be interpreted as the causal effects of more videos and less of whatever else they would have been doing. This subtle point could matter for external validity as a different population of students with zero leisure time may respond differently to the incentive.} and monotonicity\footnote{Though not testable directly, one testable implication of monotonicity is that the cumulative distribution function of videos watched for each treatment arm should not cross. Indeed, Figure \ref{combo_cdf} shows that the two CDFs do not cross.} are reasonable assumptions, we have more concern about non-interference because of the potential for spillovers between students in the same class. If we had unlimited resources, a robust experimental design would assign treatment at the class (or coarser) level, reducing the chance for interactions between treated and control students. However, given our resource constraints, assigning treatment at coarser levels would have resulted in insufficient statistical power to detect reasonable effect sizes. Hence, we proceed acknowledging the potential for spillovers between students. We hypothesize that spillovers likely bias our estimates of the treatment effect \textit{downwards} as we believe control students are more likely to benefit from having well-studied peers than they are to lose from, for example, having peers too busy watching videos to join a study group.\footnote{Although spillovers are possible, we believe the magnitude of the spillovers are likely small given that students have for the most part not yet formed strong social networks. 47\% of students in the \textit{Incentive} or \textit{Control} arms are transfer students in their first term at the university. The remaining students are predominantly sophomores taking their first upper division course. Social dynamics at the university facilitate networks within ``colleges" more than majors for the very reason of encouraging academic diversity among peer groups. One example of a possible positive spillover is the online discussion board where students could ask questions about content covered in the IMVH.}

Similar to our estimates of Equation \ref{itt_spec}, we estimate Equation \ref{secondstage_spec} with three sets of controls: only year and first midterm score, controls chosen using PDS, and a fixed-effect model with controls chosen using PDS. We additionally estimate the LATE using Neyman's repeated sampling approach whose estimators we derive in Appendix \ref{a_neyman_late}.

\subsubsection{Treatment Effects at the Cutoff}

Here we describe estimation of treatment effects at the first midterm score cutoff. Because the probability of being assigned to the \textit{Incentive} arm changes discontinuously from 0.5 to 0 at the midterm score cutoff, our setting is appropriate for estimating local treatment effects using a regression discontinuity (RD) design \citep{tc1960, ap2008, il2008}. With this method, we compare students in the \textit{Incentive} arm who scored just below the cutoff to those in the \textit{Above median} and \textit{Control} arms who scored just above or below the cutoff, respectively. These two groups are similar across pretreatment characteristics but different in treatment status, thereby providing an estimate of the treatment effect local to those who scored near the cutoff.

Since RD designs require that agents near the cutoff be similar across covariates except treatment status, a threat to validity is manipulation of the forcing variable (in our study, midterm score), which biases treatment effect estimates by nonrandom selection into treatment. This manipulation can occur if agents behave strategically to target a particular side of the cutoff, for example, scoring slightly higher than a published minimum SAT score for college admission. Since students in our experiment do not know the cutoff \textit{ex ante}, it is unlikely that students would attempt to target a particular side of the midterm score cutoff\footnote{It would be surprising for students who value high grades to target the expected median score since any student capable of doing so would likely earn a higher grade in the course by scoring as high as possible on the midterm exam rather than strategically scoring just below the expected median cutoff.}. Ultimately, we must \textit{assume} continuity of the conditional means of the potential outcomes along the midterm score; however, we do not observe a discontinuity in any observable pretreatment covariate at the cutoff, which gives us further confidence that this assumption holds.

To estimate local ITT effects using a sharp RD, we return to the potential outcomes framework modeling the treatment effect $\tau(c)$ as the difference in expected outcomes at the cutoff $c$ along the forcing variable $x$ (midterm score):

\begin{equation} \label{rd_spec}
\begin{split}
	\tau(c) & = \lim_{x \uparrow c} \operatorname{E}[Y_i | X_i = x] - \lim_{x \downarrow c} \operatorname{E}[Y_i | X_i = x] \\
	& = \operatorname{E}[Y_i(1) | X_i = c] - \operatorname{E}[Y_i(0) | X_i = c]
\end{split}
\end{equation}

We estimate $\tau(c)$ using local low-order polynomials, per the advice of \textcite{gi2019}.

Sharp RD designs used in the literature frequently do not observe $Y(1)$ and $Y(0)$ for the same values of $x$. In our setting, however, we observe $Y(0)$ both above and below the cutoff. Hence, we need to assume continuity only for $Y(1)$ as we do not observe any outcomes for treated students scoring above the cutoff but \textit{do} observe outcomes for control students both above and below the cutoff.



% quantile regression?


% *****************************************************************
% Results
% *****************************************************************

\section{Results} \label{results}

In this section, we first establish that \textit{Incentive} and \textit{Control} arms are balanced on observable characteristics both when students were assigned treatment and when they took midterm and final exams. Second, we show that the grade-based encouragement worked, i.e. that students in the \textit{Incentive} arm watched significantly more videos than did their \textit{Control} peers. Third, we present results from the LATE and ITT specifications as described in the previous section. Finally, we estimate spillover effects in other courses during the experiment term as well as we the term following.

\subsection{Balance between treatment arms}

\subsection{Relevancy of the encouragement instrument}

As described in the previous section, we use a Two-Stage Least Squares approach to estimate the LATE of watching videos on exam performance. We must check that our instrument is both valid and relevant to ensure this method will produce an unbiased estimate of the LATE \parencite{ir2015}. The validity condition is met by assigning the treatment arm at random conditional on midterm exam score and year of instruction. Balance across pretreatment obserables, as demonstrated in Appendix Tables \ref{balance_table_mid2} and \ref{balance_table_final}, give us further confidence that treatment status is uncorrelated with demographics.

Next we check relevancy, that is, whether treatment status generates significantly more video watching. In Table \ref{firststage_table} we present estimates from Equation \ref{itt_spec} and find that being assigned to the \textit{Incentive} arm induces students to watch between 38.4 - 39.2 more videos and 20.5 - 21.6 unique videos by the final exam than does being assigned to the \textit{Control} arm. By the second midterm, being assigned treatment causes students to watch 9.1 - 10.5 videos and 6.0 - 6.8 unique videos more than being assigned control. Graphically, we depict the distribution of videos watched as a function of treatment in Figure \ref{combo_cdf}. Notably, at every quantity of videos watched in the domain, a greater portion of \textit{Incentive} students watched

Collectively, given the highly significant first stage regression results and monotonic increase in video watching across the sample, we conclude that treatment status meets the relevancy criterion.

Although we code ``watching" as binary, video watching varies significantly in intensity. Some students took notes, pausing and rewatching portions of the video as needed. Other students, we suspect, played videos in the background without absorbing any material. Throughout the remainder of the paper, we

\subsection{Estimation of causal effects}

First, we estimate the causal effect of being assigned to the \textit{Incentive} arm on exam scores (ITT). This estimate is relevant for educators interested in predicting how requiring videos will change exam scores in their classes using the same grade-based incentive implemented in our experiment. Second, we estimate the causal effect of watching videos on exam scores, which is of interest to educators deciding which teaching technologies to provide for their classes as well as to students choosing among different studying tools.

For both the ITTs and LATEs, we examine effects on the second midterm and final exams using both parametric methods (i.e. Equations \ref{itt_spec} and \ref{secondstage_spec}) and nonparametric methods a la the repeated sampling framework of \textcite{neyman1923}% and Generalized Random Forests proposed by \textcite{wa2018}
. We check that our parametric results are robust to model specification by estimating Equations \ref{itt_spec} and \ref{secondstage_spec} with and without $f(X_i)$ as a vector of linear control variables chosen via PDS \parencite{bch2014a}. To rule out nonrandom attrition across treatment arms as a confounder, we fit a fixed effect model that drops any student whose matched pair attrited.

\subsubsection{Effect of grade incentive on video watching}

Table \ref{firststage_table}

\subsubsection{Effect of treatment on exam scores}

Table \ref{secondstage_table}

\subsection{Spillover effects}

Here we estimate spillover effects to other other courses taken concurrently during the term of the experiment. We also estimate spillover effects to Micro B, the subsequent course in the intermediate microeconomics sequence. It is important to examine spillover effects

\subsubsection{Concurrent courses}

Table \ref{spillover_grades}

\subsubsection{Substituion from other forms of studying}

Table \ref{spillover_studying}

\subsubsection{Subsequent intermediate microeconomics course}

Table \ref{spillover_100b}



% *****************************************************************
% Discussion
% *****************************************************************

\section{Discussion} \label{discussion}


Students overwhelmingly report that they find the IMVH useful. But considering psych learning theories, why might videos be effective?   The IMVH does not have students use retrieval practice (and example is when students test themselves).  However, it is possible the IMVH facilitates interleaving (mixing topics up while studying), spaced practice (attend lecture and watch videos either before or after), learning information from different formats (IMVH presents information both verbally, graphically and algebraically), and using worked examples (IMVH includes worked examples.  Again, it is also possible that the inducement to watch the IMVH simply led students to spend more times on the class.  Why do the videos not appear effective for students at the median?  Not sure unless it is simply consistent with a learning model where students above median already know how to study and the inducement to use the IMVH


\subsection{Limitations}

The present study has several limitations that should be considered before, for example, creating one's own video handbook and requiring students to use it. First, the population studied is students who score below the median on the first midterm of an intermediate microeconomics course at a large, highly-selective public research univerisity. The extent to which treatment effects vary by course, instructor, university, or along the midterm score distribution is beyond the scope of this paper. Additionally, the causal effects of watching videos that we estimate are local to compliers, i.e. students induced by the grade incentive to watch additional videos. We cannot recover the \textit{population} average treatment effect, though annecdotal evidence and economic theory both suggest that the population average treatment effect is likely greater than the LATE.%because control students watched many videos too even without incentive

Some researchers may wonder why we included only the bottom half of midterm scorers in the experiment instead of the entire class. Though we cannot estimate heterogeneity in treatment effects along the entire midterm distribution with our design, we believe this loss is justified by reduced risk of welfare losses by high performing students. The first midterm provides a signal of which students likely know for themselves how much and what kind of studying they should be doing. Coercing these high-type students to spend time with an alternative studying method is unlikely to be helpful and runs a higher risk of harming utility. On the other hand, students who have made manifest a need for alternative or more time studying stand to benefit the most from instructor-provided guidance.

Another consideration is the time frame during which the experiment took place, 2018 to 2019. About three months after the conclusion of our experiment, most students in the United States and all students at the studied university began remote learning as the coronavirus pandemic prompted stay-at-home orders. With increased experience learning via electronic media, it is possible that treatment effects will be higher in the future than we estimate in our paper. On the other hand, if students find online learning materials increasingly \textit{less} engaging, we may find the opposite.

Generalizability of our results. The experiment was conducted in intermediate microeconomics, a required course in all economics programs which typically has high failure rates.\footnote{At UC San Diego it is and at LSE it ranges from 12 percent in their least mathematical version to 24 percent in the more mathematical class, see https://www.lse.ac.uk/study-at-lse/The-General-Course/pdf/Choosing-economics-Course-guide-2020-21-HR-1.pdf} It is the first upper division class for many students and, for transfer students, also their first class in the university and under a XX percent quarter system. As such there may be large information problems about how to successfully study for the class.

Future research: most importantly, to see of our results hold up in other educational settings (e.g., different students, types of classes, instructors, and universities). We would like to examine the role of weekly deadlines instead of one final deadline at the end of the term, which may reduce the deleterious effects of binge-watching.


% *****************************************************************
% Conclusion
% *****************************************************************

\section{Conclusion} \label{conclusion}

We examine the effectiveness of an educational innovation, a video handbook composed of 220 brief instructional videos on intermediate microeconomic theory. We used random assignment of a grade-based incentive to experimentally vary takeup of the video handbook, and we found that greater takeup caused student to score significantly higher on exams. Specifically, we estimate that for students on the margin of watching videos, an additional hour of video watching causes students to score XX to XX standard deviations higher on exams.

Instructors may have concerns about making a resource such as the IMVH available if they believe students may substitute away from lectures or other more productive studying methods \textcite{kay2012}. Another concern is that forcing students to spend more time studying in one's class may cause worse performance in other classes. Our analysis provides some confidence that neither of these fears are first-order concerns. We do not find evidence that students decrease their consumption of other forms of studying, nor do we find that students perform worse in other courses during the same quarter. Our point estimates, though not statistically significantly different from zero, are positive for most alternative studying methods, suggesting that a potential mechanism of the videos may be helping students realize what they \textit{don't} know, whereas students who selectively study spend too much time on material they already know.

A final concern is one of welfare. In a neoclassical model, instructors cannot make their students better off by forcing on them quantities of studying they would not otherwise have chosen for themselves. In a behavioral model, which we think is more appropriate in our university classroom setting, instructors \textit{can} improve student welfare through intervention when information barriers and myopia lead to suboptimal time allocation decisions. We observe two phenomena that supports the latter model. First, treated students tend not to bunch at the cutoff for the grade incentive. Second, video consumption remains much higher among treated students in the term following conclusion of the experiment.

While there are many educational interventions that instructors could offer their students, the research on causal effects of educational interventions remains limited. Our study serves as an example of a feasible research design that runs a lower risk of generating welfare losses for high performing students than does a class-wide experiment. It is our hope, as educators ourselves, that more research be conducted on the effectiveness of pedagogical technologies.

\printbibliography

%%% TABLES

\clearpage
\input{../tables/firststage.tex}

\clearpage
\input{../tables/secondstage.tex}

\clearpage
\input{../tables/spillover_grades.tex}

\clearpage
\input{../tables/spillover_studying.tex}

\clearpage
\input{../tables/spillover_100b.tex}

%%% PLOTS

% CDFs of videos watched by each treated vs control student
\clearpage
\begin{figure}[htb]
\begin{center}
\caption{Effect of grade incentive on videos watched}
\label{combo_cdf}
\includegraphics[width=1\textwidth, angle=0]{../plots/combo_cdf.pdf}
\footnotesize Top panels display the percent students in the \textit{Control}
 and \textit{Incentive} arms that watched at least $X$ unique videos (left) or hours of unique videos (right). Bottom panels display the differences between the two arms in the top panels with 95\% confidence intervals estimated by regressing an indicator for whether on the student watched at least $X\in\{0,...,X_{max}\}$ unique videos (or hours of unique videos) on the student's treatment status.
\end{center}
\end{figure}

% Time series: videos over time
\clearpage
\begin{figure}[htb]
\begin{center}
\caption{Weekly video watching by treatment arm}
\includegraphics[width=1\textwidth, angle=0]{../plots/tscombo.pdf}
\footnotesize Dashed lines represent Midterm 1, Midterm 2, and Final exams
\end{center}
\end{figure}

% video CDF

% *****************************************************************
% Appendix
% *****************************************************************

\clearpage

\section*{Appendix}

\renewcommand{\thesubsection}{\Alph{subsection}}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{table}}

\subsection{Additional experiment details}

In this section we outline additional experiment details that could prove useful for replication or understanding our analysis choices.

\subsubsection{Randomization} \label{a_randomization}
Students were assigned to treatment arms using a matched pairs design, a special case of blocked randomization in which each block contains exactly two units, one treated and one control. Several authors detail how matched pair designs can improve the \textit{ex ante} precision of treatment effect estimates (versus complete randomization) by matching treatment units whose potential outcomes are similar (e.g. \cite{ir2015}, \cite{ai2017}). The

Additionally, we were unable to observe most pretreatment covariates until after the experiment had concluded because of student privacy considerations, thereby making it impossible to block on these variables. We learned from the previous cohorts' data that between the first midterm score and math quiz score, both observable at the time of randomization, the midterm score predicted significantly more variation in the final exam score. Hence, we stratified on midterm score when assigning treatment. While we could have used an alternative method (e.g. matching methods) that take into consideration multiple covariates when assiging treatment, we opted for a simpler design given the high correlation between midterm and math quiz score and the comparatively high number of missing observations for the latter assessment.

We assigned treatment shortly after issuing midterm exam grades, which occured during the fourth week of the quarter. To assign treatment, we ordered the students by exam score, then paired students along this ordering for students below the median. Within pairs, we randomly assigned one student to \textit{Incentive}, the other to \textit{Control}. By construction, these two arms were \textit{ex ante} balanced on midterm exam score, and we verified at time of treatment that the arms were also balanced on math quiz score. Since this randomization was performed independently across year cohorts, by construction, the samples were also balanced on year.

Although our treatment assignment method provides a better chance of balance than does simple random sampling, by random chance and through non-random attrition, it is possible that the two treatment arms vary on \textit{ex post} observable and unobservable covariates that are correlated with the outcomes of interest, thereby confounding our treatment effect estimates. The primary cause of attrition was withdrawing from the course, which reduced our experiment sample by 35 students before the second midterm and an additional 21 students before the final exam. A 13\% withdraw rate is in line with the withdraw rates observed in previous quarters. Another cause of attrition, albeit not from the course, is age: four students under the age of 18 during the experiment were removed from the analysis dataset. Additionally, seven students opted out of having their data included in the experiment analysis.

Since neither the students' intent to withdraw, age, nor opt-out preferences were observable at the time of treatment assignment, we could not \textit{ex ante} balance this attrition across treatment arms. If students attrited non-randomly, that is, decided to attrite depending on their treatment status, then our treatment effect estimates would be biased. Fortunately, despite 8\% attrition before the second midterm and 13\% before the final exam, the two treatment arms below the median are balanced on nearly all observable pretreatment covariates, as shown in Tables \ref{balance_table_final} and \ref{balance_table_mid2}, which gives us confidence that the \textit{Control} arm is a good counterfactual for the \textit{Incentive} arm.

\subsubsection{Selection of control variables} \label{a_selection}

In this section we discuss how we select control variables included in linear models estimated in this paper.
% In our nonparametric estimation strategies that use generalized random forests, the algorithms implicitly select control variables that matter most for treatment heterogeneity and explaining variance in the outcome variables of interest (see Appendix \ref{a_grf}).

Equation \ref{itt_spec} includes a vector of control variables related linearly to the outcomes of interest. Although $d_i$, the treatment indicator is randomly assigned and in expectation $d_i$ is orthogonal to all observed and unobserved pretreatment covariates, in small samples stochastic imbalances can occur, which if controlled for can reduce bias of the treatment effect estimator \parencite{ai2017}. Even if perfect balance is achieved, controlling for orthogonal covariates can improve precision of the treatment effect estimator if the covariates can predict unexplained variance in the outcome.

By definition it is not possible to guarantee balance on unobserved covariates. As discussed in Appendix \ref{a_randomization}, we mechanically balanced the treatment arms on first midterm score, one of the few observables at the time of treatment assignment, with our knowledge from previous cohorts' data that the first midterm score explains a significant amount of variance in final exam score. Hence, in our estimation strategies including controls, we always include the first midterm score and year, following the recommendations of \textcite{bm2009} to control for all covariates used to seek balance when assigning treatment.

For variables unobservable at time of randomization but observable at time of analysis, we lack the luxury of guaranteed balance by construction, nor is it clear \textit{ex ante}, beyond our intuition, which will predict variation in the outcome variables of interest. On one hand, failing to control for valid predictors reduces statistical power. On the other hand, hand-picking control variables increases researcher degrees of freedom, risking increasing the prevalence of Type I errors \parencite{sns2011}. As such, in addition to a model without controls beyond the ones used for treatment assignment (year and midterm score), we fit a second model that includes a vector of linear controls chosen using the post-double-selection (PDS) procedure introduced by \textcite{bch2014a}.

PDS is a two step process in which first, model covariates are selected in an automated, principled fashion, and second, the model coefficients of interest are estimated while controlling for those selected covariates. The first step involves predicting, separately, both the outcome of interest (e.g., videos watched) and treatment status using lasso regression, which shrinks coefficient estimates towards zero. Note that since treatment is randomly assigned, the lasso should shrink most, if not all, of the coefficients towards zero when predicting treatment status. Next, the researcher takes the union of all covariates with non-zero coefficients and includes these covariates as controls in her model. With her control variables selected, she can now estimate treatment effects with reduced bias relative to including controls with less empirical rationale.

In Table \ref{controlvars_desc}, we describe all covariates observable in our study. In Table \ref{controlvars_selected_itt}, we describe the covariates selected as controls for estimating the effect of treatment on each outcome variable of interest. All models include either pair fixed effects or year and midterm score as controls. To ensure these controls are ``selected" by the PDS procedure, we partialed out these controls from the first step prediction models by residualizing both sides of the equation as described in \textcite{bch2014b}.

\subsection{LATE estimators using Neyman's repeated sampling approach} \label{a_neyman_late}

In this section we derive LATE estimators using the repeated sampling approach of \textcite{neyman1923}, which considers each pair as an independent, completely randomized experiment.

Similar to a Wald estimator, the point estimate of the LATE is the mean within-pair difference in outcome divided by the mean within-pair difference in videos:

\begin{equation} \label{neyman_late_spec}
	\wh{\gamma} = \frac{\wb{\Delta y}}{\wb{\Delta v}} = \frac{\frac{1}{J}\sum_{j=1}^{J} \Delta y_j }{\frac{1}{J}\sum_{j=1}^{J} \Delta v_j} = \frac{\wb{y_I} - \wb{y_C}}{\wb{v_I} - \wb{v_C}}
\end{equation}

where $y$ is the outcome of interest (grades) and $v$ is the number of videos, both indexed by pair $j\in J$ and treatment status $C$ or $I$ for \textit{Control} or \textit{Incentive}, respectively.

We use the delta method to calculate the approximate standard error of $\wh{\gamma}$. First, we define the following normally-distributed random variables:
\begin{equation}
\begin{split}
Y = \wb{y_I} - \wb{y_C} \sim \mathcal{N}(\mu_Y, \sigma^2_Y) \\
V = \wb{v_I} - \wb{v_C} \sim \mathcal{N}(\mu_V, \sigma^2_V)
\end{split}
\end{equation}

Using a first-order Taylor expansion and letting $g() = \frac{Y}{V}$, we have:
\begin{equation}
\begin{split}
	\text{Var}(g) & = \operatorname{E}[(g - \operatorname{E}(g))^2] \\
	& \approx \operatorname{E}[(g(\theta) + (Y-\theta_Y)g'_Y(\theta) + (V-\theta_V)g'_V(\theta) - g(\theta))^2] \\
	& = \operatorname{E}[(Y-\theta_Y)^2 (g'_Y(\theta))^2 + (V-\theta_V)^2 (g'_V(\theta))^2 + 2(Y-\theta_Y)(V-\theta_V)g'_Y(\theta)g'_V(\theta)] \\
	& = \text{Var}(Y)(g'_Y(\theta))^2 + \text{Var}(V)(g'_V(\theta))^2 + 2\text{Cov}(Y,V)g'_Y(\theta)g'_V(\theta)
\end{split}
\end{equation}

Expanding about $\theta=(\theta_Y,\theta_V)=(\mu_Y,\mu_V)$ and letting $g'_Y(\theta) = \mu_V^{-1}$ and $g'_V(\theta) = \frac{-\mu_Y}{\mu_V^{-2}}$:
\begin{equation} \label{var_expanded}
\begin{split}
	\text{Var}(g) & \approx \frac{1}{\mu_V^2}\text{Var}(Y) + \frac{\mu_Y^2}{\mu_V^4}\text{Var}(V) + 2\frac{-\mu_Y}{\mu_V^{-2}}\text{Cov}(Y,V) \\
	& = \frac{\mu_Y^2}{\mu_V^2}(\frac{\sigma_Y^2}{\mu_Y^2} + \frac{\sigma_V^2}{\mu_V^2} - 2\frac{\text{Cov}(Y,V)}{\mu_Y \mu_V})
\end{split}
\end{equation}

We use the following variance estimators of $Y$ and $V$ from Equation \ref{se_imai}:
\begin{equation}
\begin{split}
\wh{\text{Var}(Y)} & = \wh{\sigma_Y^2} = \frac{1}{J(J-1)}\sum_{j=1}^{J} (\Delta y_j - \wb{\Delta y})^2 \\
\wh{\text{Var}(V)} & = \wh{\sigma_V^2} = \frac{1}{J(J-1)}\sum_{j=1}^{J} (\Delta v_j - \wb{\Delta v})^2 \\
\wh{\text{Cov}(Y,V)} & = \wh{\sigma_{YV}} = \frac{1}{J(J-1)}\sum_{j=1}^{J} (\Delta y_j - \wb{\Delta y})(\Delta v_j - \wb{\Delta v})
\end{split}
\end{equation}

and the following estimators for the population means of $Y$ and $V$:
\begin{equation}
\begin{split}
\wh{\mu_Y} & = \operatorname{E}(\mu_Y) = \wb{\Delta y} \\
\wh{\mu_V} & = \operatorname{E}(\mu_V) = \wb{\Delta v}
\end{split}
\end{equation}

Substituting these variance and means estimators into the final step of \ref{var_expanded}, we arrive at the standard error estimator for $\wh{\gamma}$:

\begin{equation}
	\wh{\sigma_{\gamma}} = \frac{\wb{\Delta y}}{\wb{\Delta v}} \sqrt{ \frac{\wh{\sigma_Y^2}}{\wb{\Delta y}^2} + \frac{\wh{\sigma_V^2}}{\wb{\Delta v}^2} - 2 \frac{\wh{\sigma_{YV}}}{\wb{\Delta y} \wb{\Delta v}} }
\end{equation}

\clearpage
\input{../tables/balance_table_mid2.tex}

\clearpage
\input{../tables/balance_table_final.tex}

\clearpage
\input{../tables/controlvars_desc.tex}

\clearpage
\input{../tables/controlvars_selected_itt.tex}

\clearpage
\input{../tables/controlvars_selected_iv.tex}

%%% PLOTS

% Histogram of videos eligible for incentive
\clearpage
\begin{figure}[htb]
\begin{center}
\caption{Distribution of videos counted towards incentive}
\includegraphics[width=1\textwidth, angle=0]{../plots/hist_v_incent.pdf}
\footnotesize This plot includes only videos that would have counted towards the earning the grade incentive. Students were required to watch 40 unique of 48 eligible videos between the first midterm and final exam to earn the grade incentive. 91\% of \textit{Incentive} students met the requirements for the grade incentive versus 11\% of \textit{Control} students.
\end{center}
\end{figure}

% Time series: videos over time by exam topic
% This shows that students watch videos when most useful
\clearpage
\begin{figure}[htb]
\begin{center}
\caption{Weekly video watching by exam topic}
\includegraphics[width=1\textwidth, angle=0]{../plots/tscombo_exam.pdf}
\footnotesize Dashed lines represent Midterm 1, Midterm 2, and Final exams.
\end{center}
\end{figure}

% Treatment effects vs Midterm 1 score
\clearpage
\begin{figure}[htb]
\begin{center}
\caption{Effect of treatment on videos watched and exam scores by Midterm 1 score}
\includegraphics[width=1\textwidth, angle=0]{../plots/combo_binscatter.pdf}
\footnotesize Test scores in standard deviation units. Each point comprises 5-percentile bins along the domain. The control points displayed include both \textit{Control} and \textit{Above median} arms.
\end{center}
\end{figure}

% Histogram of 'binge watching' - distribution of max(weekly videos watched)
\clearpage
\begin{figure}[htb]
\begin{center}
\caption{Distribution of max videos watched in one week}
\includegraphics[width=1\textwidth, angle=0]{../plots/hist_maxweek.pdf}
\footnotesize These plots help illustrate potential ``binge watching" behavior. Compared to the \textit{Control} students, \textit{Incentive} students are more likely to watch 40 or more unique videos in a week, which occurs in the weeks preceding the final and not the second midterm.
\end{center}
\end{figure}





\end{document}

% \subsubsection{Generalized Random Forests} \label{a_grf}
%
% In this section we discuss our implementation of Generalized Random Forests proposed by \textcite{atw2019}
