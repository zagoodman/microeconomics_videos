\documentclass[12pt]{article}
\linespread{1.5}
\pdfminorversion=6

\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage[american]{babel}
\usepackage{booktabs}
\usepackage{bookmark}
\usepackage{tabularx}
\usepackage{float,color}
\usepackage{graphicx,scalerel}
\hypersetup{colorlinks,linkcolor=black,urlcolor=blue,citecolor=black}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{cmathbb}
\usepackage{placeins}
\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{setspace}
\newcommand{\Rho}{\mathrm{P}}
\usepackage{authblk}
\usepackage[flushleft]{threeparttable}
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{multirow}
\usepackage{array}
\usepackage{tikz}
\usepackage{csquotes}

\usepackage{epigraph}
\setlength \epigraphwidth {\linewidth}
\setlength \epigraphrule {0pt}
\AtBeginDocument{\renewcommand {\epigraphflush}{center}}
\renewcommand {\sourceflush} {center}

% Bibliography stuff
\usepackage[
hyperref=true,
giveninits=true, % render first and middle names as initials
uniquename=init,
maxcitenames=3,
maxbibnames=99,
dashed=false, % re-print recurring author names in bibliography
natbib=true,
useprefix=true, % for inclusion of `de' `da' in surname
%urldate=long,
backend=biber,
style=apa]{biblatex}
\addbibresource{bibliography.bib}

% Scale hats and bars (e.g. \wh{x})
\newcommand\wh[1]{\hstretch{2}{\hat{\hstretch{.5}{#1}}}}
\newcommand\wb[1]{\hstretch{2}{\bar{\hstretch{.5}{#1}}}}

% custom table functions
% indents second level entries
\newcommand{\indentrow}[1]{\quad #1}

% adds space above row
\newcommand{\customlinespace}{\addlinespace[.1cm]}

% command for all table/figure notes
\newcommand{\Fignote}[1]{
	\begin{tablenotes}[para,flushleft]\footnotesize
		\textit{Note}: #1
	\end{tablenotes}
}

% stars
\newcommand{\sym}[1]{\rlap{#1}}

% note addendem for regression tables
\newcommand{\Regnote}{Standard errors in parentheses are robust to heteroskedasticity. ***, **, and * indicate significance at the 1, 5, and 10 percent critical levels, respectively.}

% Flowchart helpers
\usetikzlibrary{shapes, arrows, positioning, trees}


% *****************************************************************
% Title page stuff
% *****************************************************************

\title{The effects of supplementary videos on learning in intermediate microeconomics: estimates from a field experiment.}
\author{Melissa Famulari}
\author{Zachary A. Goodman\thanks{mfamulari@ucsd.edu and zgoodman@ucsd.edu.
The authors thank the students who took intermediate microeconomics in the fall of 2018 and 2019 for participating in this study.
We thank UC San Diego's Teaching + Learning Commons for providing the anonymized data used for analysis.
We gratefully recognize Jessica Morales, Yingjia Zhang, and Shuli Zhu for outstanding research assistance.
We thank the applied microeconomics group at UC San Diego for their help with the experimental design.
We thank Julian Betts, Julie Cullen, Gordon Dahl, Simone Galperti, Craig McIntosh, Katherine Meckel and Jennifer Murdock for their suggestions which greatly improved the paper.
This research was approved under UC San Diego's Human Research Protections Program (IRB approval 170886 in fall 2018 and 2019).
The paper investigates the use of Intermediate Microeconomics Video Handbook (IMVH) video lectures by UC San Diego students, some of which were developed by one of the authors, in collaboration with UC San Diego and the UC Office of the President.
UC San Diego currently owns the rights to distribute the IMVH.
The videos were provided to the students at no charge and neither author has a direct financial interest in the distribution of the IMVH within the University of California.
As of fall 2020, one of the authors has a financial interest in the distribution of the IMVH outside of the University of California.}}
\affil{University of California, San Diego}
\date{This version: December 2022} % TODO: add link to most recent version


% *****************************************************************
% Begin Paper
% *****************************************************************

\begin{document}

% remove ``Abstract" from above abstract
% \renewcommand{\abstractname}{\vspace{-\baselineskip}}

\maketitle
\begin{abstract}
	We estimate the effectiveness of videos to improve learning outcomes in intermediate microeconomics.
	In a field experiment involving about 400 students, who revealed themselves as poor performers on an early assessment, we randomly assigned a grade-based incentive that induced treatment students to watch over 60\% more videos than did control students.
	We observe significant reduced form effects: being assigned treatment caused students to score 0.18 standard deviations higher on midterm and final exams.
	Using an instrumental variables approach, we estimate that the marginal hour of video content increased exam scores between 0.05 and 0.16 standard deviations.
	We rule out large negative spillover effects to other courses taken concurrently, and we observe persistent take-up of video watching in the subsequent quarter when exogenous incentives to watch videos were removed.
\end{abstract}

% *****************************************************************
% Introduction
% *****************************************************************

\section{Introduction}\label{sec:introduction}

\epigraph{\textit{``You expect me to read the textbook? Ha!''}}{-- Anonymous student}\bigskip

Every year, university students spend tens of thousands of dollars on tuition and course materials and hundreds of hours studying, in large part, to learn.
Instructors can help their students learn more efficiently by providing and recommending pedagogical tools that have high returns per unit time and financial cost.
Despite the value to students and instructors, little empirical work exists that estimates the effectiveness of different learning technologies~\parencite{aws2015}.

We measure the impacts of one such technology, instructional videos, on outcomes in intermediate microeconomics.
This research is timely as, after two years of remote instruction due to the Covid-19 pandemic, most educators have numerous videos they could make available to future students as a supplemental resource and the question is whether they should.
On the one hand, modern students, who have had unprecedented exposure to electronic media, may find videos more engaging and, perhaps, more effective at building human capital than conventional texts.
Further, videos have near-zero marginal cost and are accessible anywhere via the internet, helping reduce financial and geographic barriers to high-quality educational materials.
Finally, the perceived low cost of watching a video may be easier to overcome than perceived higher costs of other learning methods, potentially leading to more frequent studying, which decades of psychological research has demonstrated leads to more long-term learning than does cramming~\parencite{kornell2009, cpvw2006}.
On the other hand, students may substitute video watching for more effective learning methods and videos enable students to both procrastinate and cram.

Ultimately, the beneficial features of video-based learning tools are of value only if they can improve student learning outcomes, an empirical question we seek to answer in this paper.
The learning videos we study are short, were  professionally produced and were organized into a handbook, called the Intermediate Microeconomics Video Handbook (IMVH).
To estimate the effects the IMVH, we administered a field experiment involving nearly 400 undergraduates enrolled in the same one-quarter-long microeconomics course over two years.
Of note, these students all scored below the median on the first midterm exam, which suggests their study habits could be improved, and perhaps standing to gain the most from an intervention that targets studying.
We randomly assigned a grade-based incentive to half of these lower-performing students to encourage take-up of the videos, which were made available to all students in the class, allowing identification of intent to treat (ITT) effects and local average treatment effects (LATEs) while maintaining equitable access to learning resources.
We tracked video watching at the student level using the software platform that hosts the videos.
We observe grades, GPA, and video watching in both the term of the experiment and the subsequent term when students typically take the second intermediate microeconomics class in the sequence.

The first-stage impact of the exogenous encouragement on video watching is significant and substantial.
Students who receive the grade-based incentive watch over 28\% more unique videos by the second midterm and 63\% more unique videos by the final exam, or about 1.1 and 3.4 hours of content, respectively, than did their control peers.
We find large reduced-form effects of treatment on exam scores: for students in the bottom half of the class as of the first midterm, being assigned treatment (ITT) increases midterm and final exam scores by about 0.18 standard deviations.
Our estimates imply that the marginal hour of videos watched increases exam scores (LATE) by between 0.05 and 0.16 standard deviations.
Reinterpreting the LATE, we find that it takes between one and three hours of video watching to raise exam grades by one step, e.g., a C minus to a C\@.

To better understand the spillover effects of treatment, we examine other forms of studying including class attendance, visits to a tutoring center (specific to this course), and interacting with the class discussion board.
We do not find any statistically significant changes in any observed studying method, and we can rule out large changes.
In nearly all cases, treatment students used other studying methods at directionally \textit{greater} rates than did their control peers.
We also investigate spillovers to other courses taken during the term of the experiment and similarly find that treatment students perform directionally \textit{better} than their control peers.
Though not statistically significant, we can rule out large negative effects, suggesting that treatment did not cause students to substitute away from studying for other courses.

Although we observe that treated students performed better on course assessments, it is unclear whether treatment increases student utility.
An important piece of the welfare puzzle is whether treated students continue to use instructional videos at higher rates after exogenous incentives are removed.
Persistent take-up in the absence of external prodding provides some confidence that students, now with updated priors, value the technology.
Fortunately, we can observe video watching in the subsequent microeconomics course in the term following the experiment.
Despite there being no direct incentives to watch videos in the subsequent course, treatment students persistently watched more videos than did control students, about 8 - 10 more unique videos, or 1.2 - 1.5 more hours of unique content.
Our sample in the subsequent term is nearly half the original size, so we lack power to precisely estimate effects on exam scores;
however, our confidence intervals include effect sizes consistent with those observed in the experiment term.

Collectively, we interpret our findings as evidence that providing a small grade incentive to watch videos is a net positive on underperforming students' academic achievement.
Though formal welfare analysis is beyond the scope of this paper, we present suggestive evidence that incentivizing video-watching increases under-performing student utility, as our results are consistent with a poor-information model of student learning.
Our findings justify paternalistic incentive structures in settings where a large portion of the class is at risk of failing and the instructor has more information about the usefulness of a novel teaching technology than do her students.

The rest of the paper is organized as follows.
Section~\ref{sec:background} provides background on existing related literature.
Section~\ref{sec:studydesign} describes the study design and the videos used in the experiment.
Section~\ref{sec:results} presents the results of the experiment, and Section~\ref{sec:models} presents competing models of studying behavior that may explain the observed phenomena.
Section~\ref{sec:discussion} discusses the contributions and limitations of our study, and Section~\ref{sec:conclusion} concludes.

% *****************************************************************
% Background
% *****************************************************************

\section{Related Literature} \label{sec:background}

Students have many time-consuming activities to help them learn including attending class, watching recorded lectures, reading the textbook, doing homework, completing practice exams, and attending tutoring labs.
Several empirical challenges make it difficult to estimate the causal effects of such activities.
First, researchers must address nonrandom selection into using study methods as the decision is likely influenced by unobservable student characteristics, such as motivation or ability, that also affect student learning outcomes.
Second, empirical evidence suggests there is ``dynamic selection'' where students change their study strategies in response to a negative exam shock or being close to a letter grade threshold (\textcite{oettinger2002}, \textcite{ko2005}, \textcite{ss2008}, \textcite{bo2012} and \textcite{bo2015}.) Dynamic selection means that including student fixed effects in class performance regressions will not uncover causal effects.
Given these empirical challenges, we focus our review on research that uses experiments or quasi-experiments to identify the causal effects of learning methods\footnote{Even well-conducted experiments may not identify the causal effects of a learning method.  First, study methods may be substitutes or complements in student learning and experimental inducements to use one study strategy may affect takeup of another.  Second, experimental inducements to use a study method may change the total time students devote to a course. Experiments will then identify the causal effects of a study policy and all of the attendant changes in student behavior caused by that policy. While the causal effects of an educational policy are useful for educators considering how to design their classes, they are less useful for students wanting to know the most productive use of their study time.} that take students' time.\footnote{We do not include research that examines how to make studying methods more productive for a fixed quantity of time, though the intensive margin of pedagogical tools is certainly an important area of study with potentially clearer welfare implications.} We organize these studies into two broad groups: \textit{guided study}, where a content expert can immediately answer student questions and modify the presentation of course material, and \textit{self study}, where the student cannot get immediate feedback from an expert.\footnote{Discussion boards, where students can ask questions and get help from fellow classmates and the instructional staff, blurr the lines between guided and self study.}  Guided study includes attending lecture, office hours, tutoring labs, discussion sections and supplemental instruction while self-study includes reading the textbook, doing homework/problem sets, taking online quizzes, doing practice exams and watching recorded lectures.

There is experimental evidence on the effectiveness of several guided-study learning methods including attending lectures (\textcite{km2003}, \textcite{dgm2010}, \textcite{jcjao2015}, \textcite{tlad2020}), discussion sections (\textcite{ans2012}, \textcite{bs2013}, \textcite{kow2020}) and peer tutoring labs (\textcite{mgm2010}).
Collectively, these studies consistently find that student performance is improved across all guided-study methods, but only if students do not substitute guided-study time for self-study time.

For the effectiveness of self-study, there is experimental evidence on overall self-study time (\textcite{ss2008}, \textcite{oppp2019}, \textcite{cgpr2020}), doing homework (\textcite{ts2012}, \textcite{gr2013}) and watching supplemental videos (\textcite{noetel2021}).
The research on the causal effects of self-study overall and doing homework is mixed and ranges from no significant effect on exam scores to large positive effects.
In contrast, \textcite{noetel2021}'s meta analysis reports that student performance is significantly improved when given access to videos with no other change to instruction.\footnote{The average effect size across 34 RCTs where videos were supplementary, primarily conducted in medical education, was .88 standard deviations (Hedges g) with 88 percent of the studies finding a significant positive effect, 12 percent finding no statistically significant effects and no study finding a significantly negative effect.  In contrast, across the 83 RCTs cited in \textcite{noetel2021} where videos replaced some aspect of teaching, the effect size on average was .28 standard deviations (Hedges g) with 50 percent finding a significant positive effect, 19 percent finding a significant negative effect, and 31 percent finding no significant effects.  In economics, \textcite{exposito2020} find that University of Seville students taking a macroeconomics course in their second year did significantly better on a supplementary exam, which did not affect the student's GPA, when videos were substituted for lecture.}

% Effect of Setting Goals: \textcite{cgpr2020}, hereafter CGPR, explore the effect of having students set goals on class performance. They find that setting \textit{task-based} goals of completing a specific number of online practice exams improves student performance on exams. Those randomly assigned to set task-based goals completed 0.10 standard deviations more practice exams and increased total course points by 0.07 standard deviations. The authors found no significant effect of setting \textit{performance-based} goals of achieving specific grades in the course or on exams on total course points. In the present study, we set for the students a \textit{task-based} goal of watching a specific number of videos. Our intervention differs from that of CGPR in two important ways. First, the instructor, not the students, set the value of the goal. Second, we motivate students to watch videos through a quantifiable grade incentive instead of appealing to psychological forces such as internal commitment devices.

% *****************************************************************
% Study design
% *****************************************************************

\section{Study Design} \label{sec:studydesign}

\subsection{Description of the sample and institution}\label{subsec:description}

We conducted the field experiment in an undergraduate intermediate microeconomics course taught during fall 2018 and fall 2019 by one of the authors.
The university is a large, diverse and selective public research university in the United States.\footnote{The Carnegie Classification of Institutions of Higher Education classifies the university as an R1 (very high research activity) university. For the 2017-2018 academic year, the  undergraduate student body had the following demographics: 49.1\% female, 50.6\% male; 75.0\% in-state, 5.5\% out-of-state, and 19.5\% international; 59\% students of color; 28.6\% majoring in the social sciences, 26\% of which major in Economics. Among newly admitted students, about one-third were transfer students.  Average SAT scores were 652 and 605 for math and critical reading, respectively. About 34\% of students are the first in their family to attend a four-year university.} At this institution, intermediate microeconomics is a three-quarter sequence required for students majoring in Economics.
The experiment was conducted in the first course of the sequence, \textit{Micro A}.
We also observe grades and video watching in the second course of the sequence, \textit{Micro B}, during the winter 2019 and winter 2020 quarters.
The same instructor taught Micro B in both years (and was a different instructor than the one who taught Micro A).
Both Micro A and B instructors created half of the videos relevant to their course in the IMVH.

The class structure is similar across the Micro sequence.
Instructors teach two 80 minute lectures back-to-back on Tuesday and Thursday (e.g., one from 11--12:20pm and the other from 12:30--1:50pm).
Students have the option to attend either lecture and lectures are not recorded.
Two midterms and the final exam are held at a common time outside of lecture for the two classes.
In addition to lecture, students across both classes have access to a common class web site, weekly one-hour discussion sections run by graduate teaching assistants (TAs) who are all Economics PhD candidates, including, at the time, one of the authors.
In lieu of office hours, the graduate TAs and Undergraduate Instructional Assistants staff a tutoring lab open between three and four hours per day, six days per week.
Students may also attend weekly Supplemental Instruction (SI) sessions offered by undergraduates majoring in Economics and trained by the university in SI. Besides the IMVH, students have access to a variety of online learning resources including a discussion board moderated by the instructional team, four years of previous exam questions, weekly ungraded problem sets, and semi-weekly graded online quizzes.

% why might the results be externally valid, or at least more interesting beyond UCSD? Intermediate micro is a required course in all econ programs and tends to be a high-failure course at all institutions (this is why Office of President funded the IMVH and we got support for this project from faculty from every UC). This is also why UCSD offered to support our classes with supplemental instruction. Another argument we could make is that there are many hard, required courses like interm micro in STEM and people have been interested in determining how to help students succeed. Another tact we could take is to argue is the incentive aspect--a small grade incentive worked rmarkable well to encourage increased effort among at-risk students. Further, that increased effort improved performance, possibly because the IMVH was a particularly effective study tool but it may also be that any added time engaging with the course would have helped (joint test point). We may want to point out somewhere that course material that is relatively fixed may be best to create into a video book and so do not have to modify much over the years (which lowers the cost of providing the videos).

Students were told about the experiment during the first lecture, given a printed copy of the consent form in the second lecture, and provided a virtual copy of the consent form in the syllabus on the course webpage.
At any time during the quarter, students could opt out of having their data included in the analysis.\footnote{Students could opt out via an online form visible to a third party university organization so that neither the instructor nor research team could observe which students elected to opt out.} Students below the age of 18 at the start of the course as well as students enrolled via the university's extension program were removed from the analysis dataset.\footnote{Students under the age of 18 were excluded per IRB protocol. We exclude extension students because of their potentially very different preparation for the course and our inability to observe pretreatment covariates and outcomes outside of Micro A.} Ultimately, four students under 18, five extension students, and seven students who opted-out were removed from the analysis dataset, leaving a sample of 850 students.

There are two unique demographic features of the class worth noting.
First, many non-economics majors take the class to either satisfy general education requirements or to explore majoring in economics.
As there are many students in the experiment on the margin of majoring in economics, an important outcome is the likelihood the student takes Micro B. Second, about 37\% of the class is transfer students, most of whom are transferring from a community college.
For most transfer students, Micro A is their first experience with upper division coursework, their first class at a four-year research university, and their first time taking classes under the faster-paced quarter system.\footnote{Community colleges, the most common previous institution for transfer students, are on the semester system in the state of the university.} We examine treatment effect heterogeneity to understand whether transfer students might differentially benefit from the IMVH.

\subsection{Description of the Intermediate Microeconomics Video Handbook (IMVH)}\label{subsec:handbook_description}

The IMVH is a collection of 220 short videos that cover the material in a year-long intermediate microeconomics course sequence.\footnote{A preview of the IMVH can be found at \url{https://iti.ucsd.edu/IMVH_Misc/Promo/IMVHPromo.html}.} The videos were designed as a complement to both lectures and the course textbook.
Each topic typically has two videos, one with the graphical and verbal intuition and the other with the formal algebraic definitions and proofs, identified by having (Calculus) at the end of the video title.

The videos were created by six UC San Diego faculty members with professional videographer and production support.
Many videos utilize the ``learning glass,'' an innovative presentation technology where instructors write with neon markers on a large sheet of glass that has lights embedded along the glass edge to make the colors pop.
The remaining videos feature faculty superimposed in front of slides that are written on during the presentation.
Videos are closed captioned and were checked by graduate students for accuracy.
The IMVH was designed to help students find material quickly and so (a) videos can be accessed either via the table of contents or the index (b) for each video, the web interface includes time stamps for each concept covered, and (c) while watching the video, captions are searchable which allows students to jump to the searched-for word.

% alternative checkmark option:
% \def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\begin{table}
	\caption{Comparison of information transmission formats} \label{tab:tech_comparison}
	\centering
	\begin{tabular}{|c*{4}{|>{\centering\arraybackslash}m{0.1\linewidth}}|}
		\hline
		Feature & Lecture & eText-book & Lecture Capture & IMVH\\
		\hline
		Instructor's time used & \checkmark & & &\\
		Instructor-learner interaction & \checkmark & & &\\
		Learner-learner interaction & \checkmark & & & \\
		Readable & & \checkmark & ?
		& \checkmark \\
		Scalable & ?
		& \checkmark & \checkmark & \checkmark \\
		Searchable & & \checkmark & & \checkmark \\
		Skimmable & & \checkmark & & \checkmark \\
		Stoppable & ?
		& \checkmark & \checkmark & \checkmark \\
		Watchable & \checkmark & & \checkmark & \checkmark \\
		Consumed on Demand & & \checkmark & After Lecture &\checkmark \\
		\hline

	\end{tabular}
	\label{tab:infotransmission}
\end{table}

While we do not know of another textbook completely comprised of videos, the IMVH is similar to the Khan Academy website, lecture capture, and textbook websites that incorporate instructional videos.
Table~\ref{tab:tech_comparison} presents a classification of some options to present course material to students.\footnote{This table is a slight modification of the classification table Martin Osborne proposed to one of the authors in an email correspondence.} Besides the engaging viewable nature, the IMVH differs from a traditional textbook in that the instructors explain, graph, and derive mathematical results in much the same way one would in a conventional lecture.
Compared to lecture, students control the pace of the IMVH (they can rewatch, speed up, or slow down the videos), students can both listen to and read the IMVH due to closed captioning, and the IMVH has better visibility than lectures in large rooms.
Students have the option to watch the IMVH videos \textit{before} lecture to prepare, and no recurring demand is placed on the instructor's time.
The primary benefits of lecture over the IMVH is that students can receive immediate help when they have questions, student questions may have import externalities for the learning of other students and there is a social aspect as students can interact with each other before, during, and after lecture.
Finally, compared to recorded lectures, the IMVH videos are much shorter, averaging under ten minutes, are focused on a single topic and do not include lecture components that often do not work well when recorded, such as group work, student questions and class discussion.

\subsection{Experiment Design} \label{subsec:expdesign}

The experiment began four weeks into the ten week term following grading the first midterm exam.
All students who scored above the median on the first midterm, the \textit{Above median} arm, had a conventional grading scheme that placed weight only on exams and quizzes.
For students who scored below the median on the first midterm, we randomly assigned half to the  \textit{Control} arm who had a conventional grading scheme that placed weight only on exams and quizzes (the same grading scheme as the students scoring above the median) and half to the \textit{Incentive} arm, whose grading scheme allots four percentage points conditional on watching at least 40 of 48 eligible videos in the IMVH.\footnote{Watched in standard speed, 40 videos would require students to spend between 5.5 and 7.1 hours, depending on the length of videos chosen (on average 9.7 minutes in length each). Watching all 48 incentivized videos in standard speed would require just shy of eight hours.} These 48 videos cover new class content since the first midterm that would be assessed in the second midterm and final exam.
All students could still view the 26 videos relevant to the first midterm and, as they could help students on the cumulative final exam, we include them in our measures of video watching despite not counting towards the grade incentive.

The two different grading schemes are outlined in Table~\ref{tab:gradescheme}.
Notably, the four percentage points for video watching come at the expense of reduced weight placed on the first midterm score, which had already occurred at the time of treatment assignment.
Hence, at the time of treatment assignment, the video incentive is the sole forward-looking difference between treatment arms.

\begin{table}
	\caption{Grade scheme by treatment arm for students who scored below the median on the first midterm. Differences between the two grade schemes in bold.}
	\centering
	\begin{tabular}{ c|c|c }
		Assessment & Incentive & Control \\
		\hline
		$>$40 videos & \textbf{4\%} & \textbf{0\%} \\
		Midterm 1 & \textbf{18\%} & \textbf{22\%} \\
		Midterm 2 & 22\% & 22\% \\
		Final Exam & 50\% & 50\% \\
		Math Quiz & 1\% & 1\% \\
		Best 5 of 6 Quizzes & 5\% & 5\% \\
		\hline
		Total & 100\% & 100\% \\
	\end{tabular}
	\label{tab:gradescheme}
\end{table}

To improve balance between \textit{Incentive} and \textit{Control} arms and increase statistical power, we assigned students to treatment arms using paired randomization~\parencite{ai2017}, matching students by their first midterm scores before randomly assigning one member of each pair to \textit{Incentive} and the other to \textit{Control} (further details on treatment assignment can be found in Appendix~\ref{subsubsec:a_randomization}).
We emailed each student letting them know their assignment and grading scheme.
Students could also find their assignment listed in the online gradebook.
To confirm that students knew their assignment, we surveyed students using an in-class attendance quiz, and 94\% of students correctly identified their grading scheme.
We emailed the students who responded incorrectly to clarify their assignments.\footnote{11 of 164 \textit{Incentive}, 23 of 167 \textit{Control}, and 10 of 373 \textit{Above median} students did not identify their grading schemes correctly. 146 students did not answer the quiz, several of whom had dropped the course following the first midterm.}

We informed \textit{Incentive} students that they must watch the entire video and only one video at a time to get credit towards their 40 required videos.
It is not possible to observe ``watching'' as students could, for example, minimize their browser, walk away from their computer, or otherwise play a video without actively watching it.
As a proxy for watching, we use data recorded by the IMVH software that captures the video ID, student ID, and the date and time when a student opens a video link.
We define the following measures:
\begin{enumerate}
	\item \textit{Videos}: Number of links opened, including duplicates
	\item \textit{Unique videos}: Number of unique video links opened
	\item \textit{Hours of videos}: Total runtime of video links opened
	\item \textit{Hours of unique videos}: Total runtime of video links opened with duplicates removed
\end{enumerate}

For expositional ease, we use ``watching'' to refer to the link-opening behavior as defined above.
Although video watching in our data is a binary measure, watching behavior can vary greatly in intensity.
Some students take notes, pausing and rewatching portions of the video as needed.
Other students, we suspect, play videos in the background without absorbing much material.
Exploring the intensive margin of video watching remains an area for future research that will benefit from new technologies that can quantify video engagement including interactive content embedded in videos, eye-tracking devices, and more.

We helped students keep track of their progress towards 40 videos by periodically updating the online gradebook with counts determined from the IMVH data.
Although nearly all students followed our instructions to watch videos completely and sequentially,\footnote{The time between timestamps for video links opened in succession was almost always longer than the runtime of the video.} in a few cases, students opened 40 or more video links within a few minutes.
We manually adjusted their video counts in the gradebook and emailed them a reminder of the requirements for videos to count towards the grade incentive.\footnote{Although additional email communication as a result of treatment could violate the exclusion restriction, the small number of affected students is unlikely to have much (if any) influence on our results.}
Though it is doubtful these strategic students gained much from opening so many videos so quickly, to maintain interpretability of our results, we do \textit{not} remove these clicks from our video count measures.\footnote{Our causal effects are per ``links opened'' rather than per ``links opened subject to certain qualifying conditions''. The cost of this interpretability is likely downward bias on our causal effect estimates.}
Our \textit{unique} video measures, however, are less sensitive to this behavior.

To ensure fairness, we informed students that final letter grades would \textit{not} be affected by being in the experiment.
We accomplished parity between \textit{Control} and \textit{Incentive} arms through curving final letter grades.
First, we applied a curve to the \textit{Control} and \textit{Above median} arms as one group to achieve a final grade distribution in line with that of previous cohorts.
Second, we curved the \textit{Incentive} students' letter grades to match the grade distribution among the \textit{Control} students.\footnote{For example, assume the top 10 percent of the control group received an A final grade and the next 12 percent of the control group received an A minus final letter grade.  For the incentive group, the number of grade points they earned in the course was irrelevant outside of ranking the students:  the top 10 percent of the incentive group received an A final grade and the next 12 percent of the incentive group received an A minus final grade.  We did this for each letter grade.}

% *****************************************************************
% Empirical Strategy
% *****************************************************************

\subsection{Empirical strategies} \label{subsec:empiricalstrat}

We estimate the effect of being assigned to the \textit{Incentive} arm on our outcome variables of interest, Intent To Treat (ITT) effects, as well as the effect of watching videos for those induced by the incentive to watch more videos, a Local Average Treatment Effect (LATE)~\parencite{ir2015}.
We explore whether there is treatment effect heterogeneity across key demographic variables.
Below we outline the empirical strategies for estimating each of these effects.

\subsubsection{Intention To Treat (ITT)}

Our experiment has two-sided non-compliance: some students in the treatment arm do not watch videos and some students in the control arm watch videos.
Therefore, the causal effect of being assigned to the \textit{Incentive} arm on outcomes of interest, such as exam scores, are ITT estimates and not average treatment effect estimates.
Since the incentive itself in our setting is representative of how future instructors may induce their students to watch videos, the ITT estimates are policy-relevant for instructors considering a similar grade incentive to encourage video-delivered learning methods in their courses.

Our baseline ITT specification is the partially linear model:

\begin{equation} \label{eq:itt_spec}
	Y_i = \beta Z_i + f(X_i) + \epsilon_i
\end{equation}

where $Y_i$ is an outcome of interest (e.g.\ videos watched or test scores) for student $i$, $Z_i \in \{0,1\}$ is a treatment indicator with those in the \textit{Control} arm having $Z_i=0$ and those in the \textit{Incentive} arm having $Z_i=1$, $f()$ is a generic function through which $X_i$, a vector of controls, affects $Y_i$, and $\epsilon_i$ is an unobserved residual. $\beta$, our parameter of interest, is the causal effect of being assigned to the \textit{Incentive} arm on the outcome of interest $Y$, assumed to be constant across the population.\footnote{Our experiment takes place over two years, and we pool the sample across both years. Out of the 850 student-years, one student repeated the course and hence there are 849 unique students. For simplicity, we drop the subscript $t$ from our specifications, treating the one repeating student as independent across years. Dropping this student from the sample leaves the results virtually unchanged.} Under unconfoundedness, $\wh{\beta}$ is an unbiased estimate of the ITT effect~\parencite{ir2015}.\footnote{Though we cannot test whether $Z_i$ is confounded by unobservable covariates, we have confidence unconfoundedness holds given the random assignment of $Z_i$ and the balance across observable covariates as demonstrated in Table~\ref{balance_table_mid2} and~\ref{balance_table_final}.}

In our baseline estimation of Equation~\ref{eq:itt_spec}, we include in $X_i$ a year indicator and first midterm score, following the advice of \textcite{bm2009} to control for all covariates used in seeking balance.
In a second model, we include additional controls chosen using the Post-Double-Selection (PDS) procedure of \textcite{bch2014a}, explained in detail in Appendix~\ref{subsubsec:a_selection}.\footnote{We also estimated two additional models on the sample of students whose matched pair did not attrite:  we fit Equation~\ref{eq:itt_spec} including pair fixed effects and we estimate $\beta$ as the mean difference in outcomes across pairs. The drop in observations increases the width of our confidence intervals, albeit modestly since including only matched pairs reduces unexplained variance in the outcome variables of interest. Results using those whose match pair did not attrite are available on GitHub.}

\subsubsection{Local Average Treatment Effect (LATE)}

We estimate the LATE using two-stage least squares (2SLS) with assignment to the \textit{Incentive} grade scheme as the instrument, $Z_i$.

\begin{equation} \label{eq:firststage_spec}
	v_i = \alpha Z_i + f(X_i) + e_i
\end{equation}
\begin{equation} \label{eq:secondstage_spec}
	Y_i = \gamma \wh{v}_i + g(X_i) + u_i
\end{equation}

where $Y_i$ is an outcome of interest (e.g., exam scores) for a student $i$, $X_i$ is a vector of pretreatment covariates, $f()$ and $g()$ are generic functions through which $X_i$ affects $v_i$ and $Y_i$, respectively, $e_i$ and $u_i$ are unobserved model residuals, and $\wh{v}_i$ is instrumented videos estimated by Equation~\ref{eq:firststage_spec}.
We assume the influence of $Z_i$ on $v_i$ is monotonically increasing, that is, $v_I = \operatorname{E}(v_i|Z_i=1) \geq \operatorname{E}(v_i|Z_i=0) = v_C$.
Hence, $\gamma$ is the per-video average treatment effect, local to students induced by the incentive to watch on average $v_I - v_C$ additional videos.

Under the assumptions of unconfoundedness, excludability, monotonicity, and non-interference, $\wh{\gamma}$ is an unbiased estimate of the LATE~\parencite{ai1995}.
Unconfoundedness requires that $Z_i$ be independent of potential outcomes, a reasonable assumption given random assignment of students to the \textit{Incentive} arm.
Excludability assumes that outcomes (grades) are only affected by the instrument (incentive) through watching videos.
This assumption could be violated if, for example, telling a student she is treated were to give her more confidence on subsequent exams during the quarter.
Monotonicity, sometimes referred to as the ``no defiers'' assumption, is necessary because of two-sided noncompliance and requires that students assigned treatment watch weakly more videos than they would if they were assigned control.
A violation of this assumption could occur if students get utility from rebelling against their assigned grade scheme.
Non-interference, also known as the Stable Unit Treatment Value Assumption (SUTVA), assumes that each student's outcome depends only on their own treatment status and not the treatment status of their peers.
Violations of SUTVA may include control students benefiting from having treatment students in the same class and, perhaps, studying together.

Although we believe unconfoundedness,\footnote{Although we randomly assigned treatment, one concern is nonrandom attrition. In the results section, we show that the \textit{Incentive} and \textit{Control} arms remain balanced across observables by the end of the experiment.} excludability,\footnote{While this assumption is not testable, we took care in the experimental design to make the treatment and control arms as similar as possible except for the grading schemes. Of course, watching videos inherently requires time that takes away from some other activity. Hence, the results should be interpreted as the causal effects of more videos and less of whatever else students would have been doing.} and monotonicity\footnote{Though not testable directly, one testable implication of monotonicity is that the cumulative distribution function of videos watched for each treatment arm should not cross. Indeed, Figure~\ref{combo_cdf} shows that the two CDFs do not cross.} are reasonable assumptions, we have more concern about non-interference because of the potential for spillovers between students in the same class.
If we had unlimited resources, a robust experimental design would assign treatment at the class (or coarser) level, reducing the chance for interactions between treated and control students.
However, given our resource constraints, assigning treatment at coarser levels would have resulted in insufficient statistical power to detect reasonable effect sizes.
Hence, we proceed acknowledging the potential for spillovers between students.
We hypothesize that spillovers likely bias our estimates of the treatment effect \textit{downwards} as we believe control students are more likely to benefit from having well-studied peers than they are to lose from, for example, having peers too busy watching videos to join a study group.\footnote{Although spillovers are possible, we believe the magnitude of the spillovers are likely small given that students have for the most part not yet formed strong social networks. 47\% of students in the \textit{Incentive} or \textit{Control} arms are transfer students in their first term at the university. The remaining students are predominantly sophomores taking their first upper division course. Social dynamics at the university facilitate networks within ``colleges'' more than majors for the very reason of encouraging academic diversity among peer groups. One example of a possible positive spillover is the online discussion board where students could ask questions about content covered in the IMVH.}

Similar to our estimates of Equation~\ref{eq:itt_spec}, we estimate Equation~\ref{eq:secondstage_spec} with two sets of controls: only year and first midterm score and controls chosen using PDS\@.

\subsubsection{Treatment Effect Heterogeneity}

We also investigate the extent to which treatment effects vary along key demographic variables.
In particular, we add an interaction term to Equation~\ref{eq:itt_spec}:

\begin{equation} \label{eq:het_spec}
	Y_i = \beta_1 Z_i + \beta_2 Z_i \mathds{1}_{x_i=d} + f(X_i) + \epsilon_i
\end{equation}

where $Z_i \mathds{1}_{x_i=x}$ takes a value of 1 when treatment students have the value $d$ for demographic variable $x \in X$. $\beta_2$ represents the difference in treatment effects for those with demographic $d$ relative to those without.
In practice, we estimate Equation~\ref{eq:het_spec} including in $X_i$ dummies for year and the demographic characteristic of interest as well as first midterm score.
While we observe many demographic variables, we are underpowered to detect reasonable effect sizes after adjusting for multiplicity, given the small sample sizes of our subgroups.
As such, we focus on heterogeneity along our blocking variables and covariates we hypothesized \textit{ex ante} may have treatment heterogeneity:  levels of videos watched pretreatment and whether the student transferred from a community college, is an under-represented minority, or is of Asian ethnicity.\footnote{Levels of pretreatment video watching may matter either because those with greater experience watching videos may have greater treatment effects or, alternatively, they may have watched many videos during the experiment even if not treated, and hence the incentive may have little effect. As mentioned in Section~\ref{sec:studydesign}, nearly all transfer students in the experiment are taking their first term at a four-year university and may not yet have optimized their studying practices. The university achievement gap between underrepresented and majority groups is well documented~\parencite{bcm2009} and it is important to know how treatment may affect this gap.  Finally, a large fraction of the international students in this university are Asian and the IMVH may be particularly helpful to those for whom English is not a native language, such as captions and the ability to reply videos.  Unfortunately, we are unable to observe native language directly or better proxies such as country of home address or visa status.}



% *****************************************************************
% Results
% *****************************************************************

\section{Results} \label{sec:results}

In this section, we first examine attrition and establish that the \textit{Incentive} and \textit{Control} arms in our analysis sample are balanced on observable characteristics.
Second, we show that the grade encouragement worked: students in the \textit{Incentive} arm watched significantly more videos than did their \textit{Control} peers.
Third, we estimate the effects of being assigned to the \textit{Incentive} arm (ITT) and the effects of videos (LATE) on grade outcomes.
Fourth, to better understand mechanisms, we examine spillovers to other studying methods for Micro A as well as grades in other courses taken during the experiment term.
We then see whether behavior change persists after exogenous incentives are removed by estimating treatment effects on video watching and grades in the subsequent microeconomics course, Micro B.  Finally, we discuss several thresholds in our experiment to clarify the LATE we estimate and to consider the relationship to the \textit{population} ATE.


\subsection{Attrition and balance} \label{subsec:attrition}

At the university where the experiment took place, Micro A is the first upper-division economics course and has higher withdrawal rates than most other economics courses, a product of both challenging course material and updated priors on interest in the field.
In Micro A one year before the experiment, for students who scored below the median on the first midterm, 14.7\% did not take the second midterm and 24.2\% did not take the final exam.\footnote{The 2017 statistics are calculated from a sample that differs somewhat in inclusion criteria relative to the 2018 and 2019 samples. We provide these statistics to highlight the historically high rates of attrition and not to make comparisons with the experiment sample.} In the present study, high attrition is not problematic, other than reducing statistical power, as long as attrition is independent of potential outcomes.

Since all students below the median on the first midterm had equal probabilities of being assigned to the \textit{Control} and \textit{Incentive} arms, treatment arms are balanced on covariates in expectation.
In practice, due to chance and nonrandom attrition, treatment arms can be unbalanced on covariates, which can bias estimates if not addressed in the analysis, particularly in small samples~\parencite{ai2017}.
Appendix Table~\ref{subsubsec:a_attrition} details all sources of attrition and we check for balance on observable characteristics after attrition for both the second midterm and final exam samples.
As can be seen in Table~\ref{balance_table_takers}, we find no statistically significant difference between the \textit{Control} and \textit{Incentive} arms in observable covariates including first midterm score, year, previous term's cumulative GPA, videos watched before the first midterm, ethnicity, gender, and transfer status.
However, as discussed in Section~\ref{subsec:empiricalstrat}, to correct for potential imbalance and to improve precision, we estimate models that include controls chosen via the post-double-selection method of \textcite{bch2014a}.

\subsection{Relevancy of the encouragement instrument}\label{subsec:relevancy}

We use a Two-Stage Least Squares approach to estimate the LATE of watching videos on exam performance, as detailed in the Section~\ref{subsec:empiricalstrat}.
We must check that our instrument is both valid and relevant to ensure this method will produce an unbiased estimate of the LATE~\parencite{ir2015}.
The validity condition is met by assigning treatment at random conditional on midterm exam score and year of instruction.
Balance across pretreatment observables, as demonstrated in Table~\ref{balance_table_takers}, give us further confidence that treatment status is uncorrelated with demographics.

Next we check instrument relevancy, that is, whether treatment status generates significantly more video watching.
In Table~\ref{firststage_table} we present estimates from Equation~\ref{eq:itt_spec}.
We find that by the second midterm exam, being assigned to the \textit{Incentive} arm induces students to watch 9.1 - 10.5 videos and 6.0 - 6.8 unique videos more than being assigned to the \textit{Control} arm.
The gap between treatment and control grows by the final exam to 38.4 - 39.2 videos and 20.5 - 21.6 unique videos.
The larger gap by the final is unsurprising given that the deadline to earn the grade incentive was the day before the final exam.
Following the recommendations of \textcite{ass2019}, we assess the strength of our instrument using the effective F-statistic of \textcite{op2013} which, in our just-identified setting, coincides with the Wald statistic of \textcite{kp2006}.
The effective F-statistic for the second midterm and final exam first-stage specifications are 18.6 and 194.6, respectively, both of which are greater than the \textcite{sy2005} critical value of 16.4 and the rule-of-thumb cutoff of 10.

Graphically, we depict the distribution of videos watched as a function of treatment in Figure~\ref{combo_cdf}.
Notably, the gap between treatment and control distributions remains significantly positive at every level of video watching by the final exam.
The difference is most pronounced near the required number of videos to earn the grade incentive, after which the difference diminishes towards zero.
For the second midterm sample, the difference is smaller but significantly different from zero between zero and 62 videos watched.
We also show time series plots of video watching in Figure~\ref{timeseries}, which show no differences in video watching before the first midterm and marked increases before the second midterm and final exam.
Collectively, given the highly significant first-stage regression results, large first-stage F-statistics, and monotonic increase in video watching across the sample, we have high confidence that our instrument meets the relevancy criterion.

\subsection{Effects on exam scores}\label{subsec:effects}

In this section, we estimate the causal effects of being assigned to the \textit{Incentive} arm on exam scores (ITT).
This estimate is relevant for educators interested in predicting how requiring videos will change exam scores in their classes using the same grade-based incentive implemented in our experiment.
Additionally, we estimate the causal effect of watching videos on exam scores (LATE), which is of interest to educators deciding which teaching technologies to provide for their classes as well as to students choosing among different studying tools.

For both the ITTs and LATEs, we examine effects on the second midterm and final exams using both parametric methods (i.e.\ Equations~\ref{eq:itt_spec} and~\ref{eq:secondstage_spec}) and nonparametric methods a la the repeated sampling framework of \textcite{neyman1923}.
As results are similar across methods, we report parametric estimates here and the nonparametric results on GitHub.
We check that our parametric results are robust to model specification by estimating Equations~\ref{eq:itt_spec} and~\ref{eq:secondstage_spec} with and without $f(X_i)$ as a vector of linear control variables chosen via PDS~\parencite{bch2014a}.

Table~\ref{secondstage_table} presents estimates of the effects of treatment on second-midterm and final exam scores.
Across our four specifications, we estimate reduced-form (RF) impacts of being assigned to the \textit{Incentive} arm of 0.17 - 0.18 standard deviations on the second midterm.
These estimates, along with our first-stage estimates (Table~\ref{firststage_table}), imply LATEs of 0.26 - 0.30 standard deviations per 10 unique videos, or 0.16 - 0.18 standard deviations per hour of unique content.
For the final exam, we estimate similar ITT effects: being assigned to the \textit{Incentive} arm raises scores by 0.14 - 0.18 standard deviations.
However, given the larger first stage effects for the final exam, we estimate smaller LATEs: 0.08 - 0.09 standard deviations per 10 unique videos, or 0.04 - 0.05 standard deviations per hour of unique content.\footnote{Given the large F-statistics when estimating first-stage effects of our incentive instrument on videos watched, we are not particularly concerned about bias from weak instruments. However, following the advice of \textcite{ass2019}, we report Anderson-Rubin confidence sets, which are efficient regardless of the strength of our instrument. As can be seen in Appendix Table~\ref{tab:arci_table}, we find that the weak-instrument-robust confidence intervals are very similar to those presented in Table~\ref{secondstage_table}.}

\subsection{Spillovers to concurrent courses}\label{subsec:spillovers}

We next estimate spillover effects to other courses taken concurrently during the term of the experiment.
Although we find positive effects on exam scores in Micro A, it is important to examine spillover effects to other classes.
Although we do not observe student time use, an important proxy is whether grade outcomes declined in other courses, which would suggest that students reduced study time in other courses to watch videos in Micro A. On the other hand, if grades in other courses remain constant, it suggests that students substituted video watching for alternative studying methods within Micro A (a hypothesis we explore in the next section) or students increased total studying time in the quarter of the experiment.

Table~\ref{spillover_grades} presents our estimates of Equation~\ref{eq:itt_spec} where $Y_i$ is GPA, number of classes passed, or portion of classes taken for a letter grade.
Since video watching in Micro A may improve performance in other economics classes, we estimate the effects of treatment on term GPA calculated separately for all classes, all classes excluding Micro A, all classes outside of economics, and all economics classes excluding Micro A. In general, we find marginally significant or insignificant but directionally positive spillover effects on GPA.\footnote{At this university, term GPA is affected only by classes taken for a letter grade. Hence, students may not have attrited from the sample but may have taken all courses Pass/No Pass and thus have no term GPA. As such, we report sample sizes for each GPA specification.} We can rule out large negative spillover effects: in our worst-case specification for term GPA, our 95\% confidence interval rules out negative spillover effects larger than -0.02 (on a 4.0 scale), or less than 1\% of the mean term GPA among control students.
There is no statistically significant difference between our estimates of spillover effects on GPA when restricting to only economics or non-economics courses.

We additionally estimate the effects of treatment on the number of classes passed and find small, insignificant, but directionally positive effects.
We find that treatment caused students to pass 0.02 - 0.09 more classes, or about 1\% - 3\% more classes than the control mean.
We find no effect of treatment on number of classes not passed or withdrawn.
Interestingly, treatment students were somewhat less likely to take Micro A for a letter grade than were control students, but this difference is only marginally significant for one of the four specifications and insignificant for the rest.
We find no relationship between treatment and fraction of classes taken for a letter grade versus Pass/No Pass.
Across all grade spillovers examined, we find mostly small, directionally positive effects, which gives us confidence that treatment is likely not harmful to academic success outside of Micro A.

\subsection{Spillovers to other study methods in Micro A}\label{subsec:spillovers_studying}

We also examine spillovers to other studying methods within Micro A. Doing so helps us better understand mechanisms: do students substitute away from other studying when encouraged to watch more videos, or are they more likely to complement their video-watching with other unincentivized studying?
In Table~\ref{spillover_studying}, we display the results of estimating equation~\ref{eq:itt_spec} where $Y_i$ is an alternative form of studying.
We find that \textit{Incentive} students are directionally less likely to attend class, though point estimates are near zero and not statistically significant.
On the other hand, treatment students interacted with the online discussion board more than did control students, but again these estimates are not statistically distinguishable from zero.
We do not find any significant relationship between treatment and tutoring attendance.
Unfortunately, we do not observe the complete picture of student time use, but our evidence is consistent with students increasing self-study time while holding guided-study time nearly constant.

\subsection{Spillovers to subsequent term}\label{subsec:spillovers_micro_b}

While we offered treatment students a grade incentive to watch videos during Micro A, students were not offered a grade incentive during the subsequent course in the intermediate microeconomics sequence, Micro B. However, all students in Micro B maintained access to the IMVH, were given direction on which videos to watch each week in the syllabus, and were verbally encouraged to watch videos as a study method.
Fortunately, we are able to observe video watching and grade outcomes in Micro B.

We present our estimates of spillover effects in the subsequent term in Table~\ref{spillover_100b}.
In Panel A, we estimate the effect of treatment on videos watched during the subsequent term, for those who took Micro B. We find large and statistically significant effects: treatment caused students to watch 8.1 - 9.9 more unique videos and 1.2 - 1.5 hours of unique content compared to control students.
In Panel B, we estimate equation~\ref{eq:itt_spec} where $Y_i$ is the first midterm, second midterm, or final exam score.
Unfortunately given the small subsample of students who took Micro B, we are underpowered to detect effect sizes consistent with those observed during Micro A. Finally, in Panel C, we estimate the effect of treatment on taking Micro B and the number of classes passed and withdrawn.
We find no effects statistically distinguishable from zero, though, as mentioned in section~\ref{subsec:attrition}, treatment students were directionally less likely to take Micro B than were control students.

\subsection{Treatment Effect Heterogeneity}\label{subsec:heterogeneity}

Here we estimate Equation~\ref{eq:het_spec} to examine whether treatment effects vary along key demographic variables.
We present our results in Table~\ref{het_table}, which displays the coefficient estimates of $\beta_2$ from Equation~\ref{eq:het_spec}.
We find no evidence of treatment effect heterogeneity across our blocking variables, year and first midterm score.
We are hesitant to make strong conclusions given the width of our confidence intervals, but this finding is consistent with stable treatment effects across the distribution of student abilities and years of the experiment.
In Appendix Figures~\ref{locallin_mid2} and~\ref{locallin_final}, we fit local linear regressions of videos watched, midterm 2 scores, and final exam scores as functions of midterm 1 score.
We do not find any statistically significant differences along the midterm 1 score distribution;
however, for the final exam, the point estimates are largest for the bottom quarter of midterm 1 scorers.
Next, we examine heterogeneity by levels of videos watched pretreatment and find no significant differences in three of four specifications.
We find marginally significant positive effects on the second midterm for those with higher levels of pretreatment video watching.

Moving to student demographics, we do \textit{not} find statistically significant heterogeneous treatment effects for transfer students, and the point estimates for the two exams are similar in magnitude and opposite in direction.
Interestingly, we \textit{do} find marginally significant negative heterogeneous effects for female students on the final exam, but no significant effect on the second midterm, though the point estimate remains negative.
This observation that male students may have benefited more from the intervention is consistent with the findings of \textcite{cgpr2020} as well as the literature on self-control more broadly,\footnote{See, for example, \textcite{ds2006}, \textcite{dsmpzd2015}, and the works cited therein.} which suggests male students, who tend to have less self-control than female students, may benefit from interventions that address self-control problems.
We find significant negative effects for Asian students and positive effects for White students on the second midterm, but no effect on the final exam.
However, after adjusting for multiplicity, either using a Bonferroni correction or the less conservative methods proposed by \textcite{lsx2019}, none of our heterogeneity results remain significant.
Our results motivate future work investigating differences along gender and racial dimensions, which has implications for instructor recommendations and personalized education more generally.

\subsection{Experiment design choices and \textit{local} treatment effects} \label{subsec:lates_section}

Our experiment contains several cutoffs: a midterm score cutoff below which students are eligible for the experiment, a video cutoff above which treatment students earn the grade incentive, and a date after which videos no longer count towards the incentive.
All of these cutoffs influence the treatment effect estimates, which are \textit{local} to compliers.
Specifically, we estimate the effect of videos for those induced by the incentive to watch, so changing who is induced and the videos they are induced to watch will affect estimates of the LATE. Though the purpose of this study was not to identify optimal thresholds, we discuss some observations that may motivate future work and explain our intuition for the size of the \textit{population} average treatment effect (ATE).

We required treatment students to watch 40 of 48 eligible videos to receive the incentive, providing students some agency in the videos they chose to watch.
To better understand how the composition of watched videos differs between treatment arms, we plot video watch rates by video in Figure~\ref{video_cdf}.
This plot reveals that the videos students chose to watch are not random.
Watch rates in the control group vary across videos from nearly 0\% up to over 70\%, demonstrating that the perceived intrinsic value of videos varies considerably.
For treatment students, watch rates vary less across videos but are positively correlated with control watch rates.
Notably, treatment watch rates are 70\% or greater for videos whose control watch rates are between 5\% and 20\%.
This gap has implications for the estimated local average treatment effect: if treatment encourages students to watch low value videos, then the LATE estimate will be diluted relative to the population ATE.

We investigate this gap first by considering whether treatment encouraged students to watch shorter videos.
One may hypothesize that treatment students would choose to skip the longest videos since earning the incentive is not dependent on length of videos.
If students picked the shortest 40 videos, they would watch 1.6 fewer hours of content versus watching the longest 40 videos.
We fit a linear model that predicts a video's treatment watch rate using the duration of the video and its control watch rate.\footnote{It is important to control for the control group's watch rate since the educational value of a video may be endogenously related to its length (e.g., harder concepts take longer to explain).} Using this model, we find that each additional minute in duration is associated with a 0.3 percentage point decrease in watch rate by the treatment group.
For the longest incentivized video, this corresponds with a predicted 3.7 percentage point lower watch rate compared to the average length video.
We interpret this finding, given the strong correlation between control and treatment watch rates, as evidence that \textit{content} is a more important driver than is minimizing time cost when choosing which videos to watch.
However, it is plausible that some students prioritized shorter-length videos.

Next, we examine how watch rates varied over time.
In Figure~\ref{video_cdf_week} we plot watch rates by video organized by week each video's content was covered in class.
Interestingly, control group watch rates taper off towards the end of the term while treatment watch rates remain much higher.
This observation is consistent with the video incentive serving as a commitment mechanism, perhaps encouraging students to spend more time studying for the final exam.
Alternatively, perhaps control students watch fewer videos in weeks nine and ten as they shift their study time to other methods while preparing for the final.
It is theoretically ambiguous whether the gap in watch rates towards the end of the term suggests a larger or smaller LATE relative to the population ATE.

Finally, we consider the influence of the due date on our treatment effect estimates.
To earn the grade incentive, students had to watch 40 videos by the last day of instruction, which is the day before the final exam.
This is an intuitive deadline, but it allows procrastination-prone students to delay watching many hours of videos until the final week of the term.
It is plausible that these students could be harmed by treatment, or at the very least have very small treatment effects, which could explain part of the reason why our LATE estimates are smaller for the final exam than the second midterm.
To get a sense of whether treatment students were more likely to ``binge watch'' videos, we plot the max number of videos watched in one week per student in Figure~\ref{hist_binge}.
Though some control students watch 40 or more videos in one week, this behavior seems more prevalent in the treatment group.
While this may very well be a useful, rational studying strategy for some, research suggests that spreading learning over time may be more effective~\parencite{kornell2009}. \textcite{aw2002} find that offering deadlines, though costly from a rational agent perspective, results in better grades.
As an alternative to offering one deadline, offering multiple deadlines, perhaps weekly or shortly before both exams, may improve consistency between treatment effect estimates for the two exams.

Collectively, it is not obvious how the population ATE compares with the LATEs we estimate.
Neoclassical economic theory suggests that those who select into watching videos regardless of exogenous incentives likely do so because they benefit more than those who only select into watching videos only in the presence of exogenous incentives.
This view would suggest that the population ATE is greater than the LATE. However, this view is, perhaps, inconsistent with the alternative that students may not know how much studying is optimal, or what they should be studying.
While it is not possible to estimate the population ATE given our experimental design, our intuition is that the ATE is likely higher than the LATE estimated for the final exam and closer to that estimated for the second midterm, which is less likely to be diluted from deadline-induced binge watching and selecting the shortest videos.

\section{Models of Studying Behavior} \label{sec:models}

To understand our empirical results in the context of economic theory, we discuss three models of student studying behavior: a neoclassical model, an imperfect information model, and a behavioral/procrastination model.
For each model we consider the testable implications of a grade inducement to encourage adoption of a study method.\footnote{For all three models, we do not address the issue that the IMVH is a relatively unique study tool in that, to our knowledge, it is the first instructional book to be created entirely of videos. However, given the availability of close substitutes to the IMVH (lecture capture, for example) we do not explore the added issues of inducing students to use a study tool whose usefulness is not known to the instructor.} Neoclassical models of studying behavior assume that rational agents know their returns to studying using the methods available to them and allocate the optimal study time to each method given their utility function, which is increasing in leisure and grades and decreasing in time spent studying.
College instructors have little knowledge about student utility functions and do not know student preferences over performance in other classes or other uses of the student's time which may also have large payoffs in the labor or marriage markets.
In this model, there is no room for an instructor to increase student well-being by intervening in their study decisions.
Both \textcite{oettinger2002} and \textcite{kow2020} find empirical support for the neoclassical model. \textcite{oettinger2002} finds that student effort responds rationally to nonlinear grade incentives: across 1200 students in a principles of economics class with absolute grading standards, he finds evidence of bunching just above letter grade cutoffs and student performance on the final exam is higher if the student is just below a grade threshold. \textcite{kow2020} explore the effects of a university policy that required students who performed poorly in their first year to attend a large fraction of the tutorials for each class in their second year.
At the poor performance threshold, both tutorial and lecture attendance increased by over 50 percent with a concomitant decline in self-study hours.
Grades for students at the policy threshold were \textit{lower} by 0.16-0.26 standard deviations.
At least for students at the margin of poor performance, the requirement to attend tutorials appears to have hurt students by not allowing them to pick their optimal study strategy across self study, tutorials, and lectures.

A key assumption of the neoclassical model is that students possess complete information about the returns across studying methods.
However, there is evidence from psychology that college students do not know the return to various study options\footnote{See, for example, \textcite{mccabe2011,prcc2007,drmnw2013}.} and many universities fund ``Teaching and Learning Centers'' or ``Academic Skills Centers,'' part of whose mission is to help undergraduates learn to study more productively.\footnote{All nine University of California campuses have such a center. Examples outside the UC include Dartmouth's Academic Skills Center, Michigan's Center for Research on Teaching and Learning, UNC's Learning Center, and Yale's Teaching and Learning Center.} Further, the ``raison d'etre'' of higher education is not only to teach students specific skills but to teach students how to learn.
As an alternative to the neoclassical model, we hypothesize that students supply a quantity of study time that is optimal given their information constraints.
In this `imperfect information' model, students choose study methods and quantities that are suboptimal relative to those they would have picked in a full information setting.
Hence, an intervention by an entity that has more information about returns to studying across various methods (i.e.\ an instructor) can enhance student utility.

A third model is a behavioral one in which students plan to study more than they end up studying when the time comes.
This phenomenon is consistent with two-self models in which a person's ``planner'' self, the one who desires high grades at the expense of leisure, is at odds with her ``doer'' self who must choose between immediately gratifying leisure and delayed gratification from higher grades.
Indeed, survey and experimental data suggest that many students study less than they report they ``should'' and finish the term with grades lower than what they had anticipated they would earn at the start of the term.\footnote{See, for example, \textcite{ferrari1992,ccog2017,lo2016}.}
\textcite{cgpr2020} provide empirical support for this model by finding that setting tasked-based goals helps improve college student performance.
As descriptive evidence in support of this model, \textcite{blmo2019} find that students that do much worse than expected in college are those who say they have poor time management or procrastination issues, including a tendency to cram and spending very little time studying.

We consider the testable implications of the three models applied to a setting where students are incentivized to use a time-consuming educational input, say, a set of instructional videos (or attending class, reading the textbook, working on homework, etc.).
The incentive is structured such that students who consume the educational input receive a higher grade in the course by consuming a set level of the input.
In this simple setting, students gain utility only from leisure and grades.
We assume grades, a function of time spent studying, and utility are both continuous, smooth, and increasing and concave in their inputs.
Students can choose to study using the incentivized educational input, $v$, or some outside option that is not directly incentivized, $o$, or a combination thereof.

Across all three models, before the first educational input is incentivized, students allocate time to the two studying methods until the marginal benefit of each (through higher grades) is equal to the marginal cost of forgone leisure.
Consider the population of students initially consuming below the requisite level to earn the grade incentive.
These students must decide if earning the grade incentive is worth forgone leisure and less time allocated to their outside studying option.
Next we explore the differences in predictions across the three models for \textit{compliers}, those for whom the incentive induces greater take-up of the incentivized input.

% Depending on the relative returns to studying and the students' preferences, it is theoretically ambiguous how adding the incentive will affect grades. Next we highlight the primary differences between the models.


\begin{figure}
	\begin{center}
		\begin{tikzpicture}[scale=0.6]
			% axes
			\draw [thick,<->] (0,8) node[left]{$v^*(L)$}--(0,0)--(11,0) node[right]{$L$};
			\node [below left] at (0,0) {$0$};

			% v*
			\draw (0,5) -- (2,4);
			\draw (2,4) -- (6,4);
			\draw (6,4) -- (6,2);
			\draw (6,2) -- (10,0);
			\draw [dashed] (2,4) -- (6,2);

			% video incentive threshold
			\node [left] at (0,4) {$T$};
			\draw (-0.1,4) -- (0.1, 4);
			\node [below] at (6,0) {$1-T$};
			\draw (6,-0.1) -- (6,0.1);
			%\draw [dashed] (0,4) -- (10,4);
			%\draw [dashed] (6,0) -- (6,10);

			% points for U1 and U2
			\draw [fill] (4.4,2.8) circle [radius=.05];
			\node [below] at (4.4,2.8) {$v_1$};
			\draw [fill] (3.95,4) circle [radius=.05];
			\node [above] at (3.95, 4) {$v_2$};

		\end{tikzpicture}

		\vspace{1\baselineskip}

		% Utility curves
		\begin{tikzpicture}[scale=0.6]
			% axis
			\draw[thick,<->] (0,8) node[left]{$G(v^*,o^*)$}--(0,0)--(11,0) node[right]{$L$};
			\node [below left] at (0,0) {$0$};
			% video incentive threshold
			\node [below] at (6,0) {$1-T$};
			\draw (6,-0.1) -- (6,0.1);

			% production frontier 1 - not incentivized
			\draw (10,0)--(10,2);
			\draw plot [smooth] coordinates {(10,2) (8,4) (6,5) (4,5.6) (2,5.85) (0,6)};

			% production frontier 2 - incentivized
			\draw plot [smooth] coordinates {(10,0) (8,2) (6,3)};
			\draw (6,3)--(6,3.75);
			\draw plot [smooth] coordinates {(6,3.75) (5,4.75) (4,5.3) (2,5.85)};

			% indifference curves
			% original
			\draw[blue] (4,5.65) arc[start angle=252, delta angle=15, radius=9cm];
			\draw[blue] (4,5.65) arc[start angle=252, delta angle=-19, radius=9cm] coordinate (u1);
			\node[left] at (u1) {\scriptsize $U_1$};
			% tangency
			\draw [fill] (4.4,5.52) circle [radius=.05];
			\node [above] at (4.4,5.52) {$v_1$};

			% new
			\draw[blue] (4,5.32) arc[start angle=252, delta angle=9, radius=14cm];
			\draw[blue] (4,5.32) arc[start angle=252, delta angle=-12, radius=14cm] coordinate (u2);
			\node[left] at (u2) {\scriptsize $U_2$};
			% tangency
			\draw [fill] (3.95,5.33) circle [radius=.05];
			\node [below] at (3.95, 5.33) {$v_2$};

			% grade incentive
			\node [left] at (0,3.75) {$G(T,0)$};
			\draw (-0.1,3.75) -- (0.1,3.75);
			\node [left] at (0,2) {$I(v \geq T)$};
			\draw (-0.1,2) -- (0.1,2);


		\end{tikzpicture}

		\caption{A student maximizes her utility $U$, a function of leisure hours, $L$, and grades, $G$. Grades are a function of video watching hours, $v$, and hours spent on the next best studying option, $o$. \textit{Above}: Let $v^*(L)$ be the demand curve for video watching as a function of leisure $L \in [0,1]$. Assume the student maximizes her utility by watching $v_1$ videos and spending $o_1$ hours on her other study method. Suppose an instructor offers a grade incentive worth $I$ units, which a student can earn by watching $v \geq T$ hours of videos. Note that at $L=1-T$ hours of leisure, the student maximizes grades by spending all study time watching videos, that is, $v^*=T$.
		\textit{Below}: Student's utility maximization problem for the neoclassical model. The grade incentive, $I$, is given to the student conditional on watching $T$ hours of videos (inner budget constraint) or, in the unincentivized case, given regardless of video watching (outer budget constraint). Time bundles along the inner budget constraint are weakly less preferred since the incentive draws the student away from otherwise more efficient $(v,o)$ combinations.}
	\end{center}\label{fig:utility_max}
\end{figure}

In the neoclassical model, as long as $o$ and $v$ are not perfect substitutes in the grades production function, the marginal return to grades of the incentivized input is less than that of the outside option for compliers.
This model predicts bunching at the incentivized level cutoff since compliers would prefer to spend their marginal hours on leisure or studying with their other method.
This model predicts a weak increase in video watching and weak decrease in other studying and leisure consumption.
It is ambiguous whether cumulative study time increases or decreases as this depends on relative utility benefits of leisure and grades and the returns to studying by each method.
However, if cumulative study time remains constant or decreases, then exam performance should strictly decrease since students are now suboptimally allocating study time versus their first-best allocation when considering only marginal returns to studying.
On the other hand, if cumulative study time increases, students may earn greater exam grades but achieve lower utility compared to baseline.
Importantly, this model predicts that in subsequent quarters students return to their pre-incentive levels of studying.

In the imperfect information model, students' \textit{ex ante} allocations to each studying method are not necessarily first-best.
The effect of the incentive on video-watching depends on whether students update their priors about the returns to watching videos as they work towards hitting the minimum required level for the grade incentive.
At this cutoff, they make a decision whether to continue watching videos depending on their updated perceptions of the marginal benefit.
We do not expect bunching in this model unless students believe the cutoff for the grade incentive is the optimal level or the updated marginal benefit at the cutoff is lower than the marginal benefit of the next best studying option or the marginal utility of leisure.
A sharp prediction is that video watching will continue at the incentivized level in the absence of the grade incentive as students have learned an effective study tool.
We also expect the treatment effect to be greater for students with more information problems, perhaps students in their first semester/quarter at university.

Finally, in the behavioral/procrastination model, the instructor's inducement helps students stick to study plans up to the minimum required level for the grade incentive.
As long as total study time does not fall, the inducement will increase exam performance (see \textcite{aw2002} where students with externally set deadlines had higher grades relative to students who choose their own deadlines).
This model also predicts bunching at the incentivized level cutoff as long as using the incentivized input does not change the student's ``planner'' and ``doer'' selves.
In the absence of the inducement, i.e., in future classes, a sharp prediction is that video watching will revert to pre-inducement levels.

We summarize the predictions across models in Table~\ref{tab:modelpredictions}.
One key empirical difference is whether or not students return to their pre-incentive level of video watching in the absence of the incentive.
We find that treatment students continued watching videos in the absence of any grade incentive in Micro B, which is inconsistent with the predictions from both the neoclassical and behavioral models of learning in our setting.
Another key empirical difference across the models is whether students watch exactly 40 videos, the number of videos required to earn the grade incentive.
In Figure~\ref{hist_incent} we plot the distribution of incentive-eligible videos watched, and one can see that treatment students typically watched more 40 videos.
Since students did not receive immediate feedback about the number of videos they had watched, the lack of bunching may be influenced by student uncertainty about whether they had met the 40-video threshold.\footnote{Anecdotally, many students reported keeping track of which videos they had watched by checking-off the list of incentive-eligible videos and/or taking notes on each video watched.} Nevertheless, we view our results as most consistent with the imperfect information model of learning.

\begin{table}
	\caption{Predictions Across Models for Compliers, those induced by incentive to consume more videos.}
	\centering
	\begin{tabular}{ c|c|c|c }
		Outcome & Neoclassical & Imperfect& Behavioral/ \\
		& & Information & Procrastination\\
		\hline
		Number of Videos, $v$ & up & up & up \\
		Other study method, $o$ & ?
		& ? & ?\\
		Total studying, $v+o$ & ?
		& ? & ? \\
		Bunching at 40 videos, $T$ & Yes & No & Yes \\
		Exam Performance & Down* & Up & Up\\
		Video use, future classes** & return to baseline & remain at incentivized level & return to baseline \\
		\hline
	\end{tabular}
	Notes: *As long as total studying stays the same or falls.
	**We assume future classes do not incentivize video watching.
	\label{tab:modelpredictions}

\end{table}


% *****************************************************************
% Discussion
% *****************************************************************

\section{Discussion} \label{sec:discussion}

\subsection{Contributions}\label{subsec:contributions}

Our results add to the literature on what motivates students to increase their performance in college.
Previous research finds financial incentives have little effect (see papers cited in \textcite{gmr2011}) but tend to work better if educational inputs as opposed to outputs are incentivized (\textcite{fryer2011}, \textcite{gmr2011}).
There is mixed evidence on the effects of having students set goals on the use of educational inputs with no penalty for missing the goal (see \textcite{cgpr2020} who find positive grade effects of setting goals on number of practice quizzes to complete while \textcite{oppp2019} find that setting a weekly schedule ahead of time and weekly reminders via a text message had only a small effect on study time and no effect on output as measured by grades, retention or credit accumulation.).
Interestingly, \textcite{cgpr2020} find no effect of having students set goals on class outcomes, such as course grade or exam scores.
We find that a small grade incentive is effective in motivating poorly performing college students to significantly take-up video watching, an educational input.\footnote{It is possible that the grade incentive is not an important motivator as \textcite{dgm2010}) required students to attend class if they performed poorly on an early assessment and, though TAs carefully recorded attendance, there was no penalty for not attending class. Nevertheless, poorly performing students significantly increased class attendance.} We also reduced the weight on an early assessment and allowed students to earn back the lost points fully by meeting the video watching requirement, which may also be an important motivator.
Grade incentives have the unappealing feature that grades are directly a function of input use.
The grade incentive used in this study was small, at most four percent of the student's grade, which may help mitigate this concern.

Second, we find that inducing students who performed poorly on an early assessment to increase the amount of time they spend watching instructional videos increased their exam performance.
Since there was no drop in either grades for other courses taken in the same quarter or a drop in the use of many other educational for the class, this suggests that total study time for the course increased for treatment students.
Unfortunately, we were not able to determine where the added study time came from, which is important for student welfare calculations.
Our results are consistent with other experimental and quasi-experimental studies that find positive effects of educational inputs on college student performance.
%Zack--we need to contrast our results with other studies to see which input appears to most effective at increasing performance. See the ClarkGillProwseRush paper, section on benchmarking.

Universities often have policies that emphasize improving the performance poorly performing students, such as academic probation policies and the policy studied by \textcite{kow2020}.
We focus on students who perform poorly on the first midterm, a population similar to that of \textcite{dgm2010}.
Some may wonder why we did not include the entire class in the experiment.
While including the entire class would have increased statistical power, we were concerned that the additional precision could come at the expense of welfare losses by high performing students.
The first midterm provides a signal of which students likely know for themselves how to study, both methods and duration.
Coercing these high-type students to spend time with a potentially different studying method runs a greater risk of harming utility.
Students who, through a low midterm score, both indicate a need for alternative studying practices and stand to benefit the most from instructor-provided guidance.

Finally, we find support for an imperfect information model of learning because treatment students frequently watched more than the 40 videos they were required to watch to earn the grade incentive and because treatment students continued watching videos at a significantly higher rate in the following class when watching videos was not exogenously incentivized. \textcite{alo2009} also find continued higher use of academic support services after the incentivized year for women.
An imperfect information model of learning can also account for why incentivizing educational inputs has been found to be more effective than incentivizing grades or exam performance directly (see studies cited in \textcite{gmr2011})

\subsection{Limitations}\label{subsec:limitations}

The present study has several limitations that should be considered before, for example, creating one's own videos and incentivizing students to use them.
First, the population studied is students who score below the median on the first midterm of an intermediate microeconomics course at a large, highly-selective public research university.
The extent to which treatment effects vary by course, instructor, university, or along the top half of the midterm score distribution is important but beyond the scope of this paper.\footnote{As an example, \textcite{ck2020} found that an intervention for poorly performing students (personalized e-mails from the professor with useful information about where to get help) that was effective in raising exams scores in introductory microeconomics classes at a large public research university did not raise exam scores across several different types of classes in a broader-access institution.} Additionally, the causal effects of watching videos that we estimate are \textit{local} to compliers, i.e.\ students induced by the grade incentive to watch additional videos.
As discussed in Section~\ref{subsec:lates_section}, we cannot recover the \textit{population} average treatment effect.

The positive effects we estimate are attributable to watching IMVH videos and spending more time studying for the course.
Our experimental design does not allow us to separately identify the effects of these two mechanisms for improving exam performance.
Would a similar incentive structure that induces greater takeup of an alternative study method have similar effects?
We view this as an important question for future research.

Another question is whether \textit{any} instructional videos would have the same impact on exam scores that we estimate for the IMVH or are their aspects of the IMVH that specifically contribute to student learning?
 Research in psychology identifies several aspects of videos that appear to improve learning transmission (see \textcite{mayer2021} and papers cited therein) and so it is possible that inducing students to watch other videos may have larger or smaller effects than what we estimate for the IMVH.

The next consideration is the time frame during which the experiment took place, 2018 to 2019.
About three months after the conclusion of our experiment, most students in the United States and all students at the studied university began remote learning as the COVID-19 pandemic prompted stay-at-home orders.
With increased experience learning via electronic media, it is possible that treatment effects will be higher in the future than we estimate in our paper.
On the other hand, if students find online learning materials increasingly \textit{less} engaging, we may find the opposite.

In addition to estimating the effects of video handbooks in other educational settings, future research should examine treatment effects in the presence of weekly deadlines instead of one final deadline at the end of the term.
Given our observation of greater ``binge watching'' by treatment students and the smaller effect sizes before the final exam compared to the second midterm exam, we suspect weekly deadlines may reduce the deleterious effects of procrastination.
Despite the rich literature on the advantages of spread-out studying~\parencite{kornell2009, cpvw2006}, we note that ``binge watching'' was not unique among treatment students.
Indeed, most students within each treatment arm watched more videos the last week of the term than any other week.


% *****************************************************************
% Conclusion
% *****************************************************************

\section{Conclusion} \label{sec:conclusion}

We examine the effectiveness of an innovative educational technology, a video handbook composed of 220 brief instructional videos on intermediate microeconomic theory.
We used random assignment of a grade-based incentive to experimentally vary takeup of the video handbook, and we found that greater takeup caused students to score significantly higher on exams.
Specifically, we estimate that treatment causes students to score about 0.18 standard deviations higher on midterm and final exams.
For students on the margin of watching videos, watching an additional hour of unique content causes students to score between 0.05 to 0.15 standard deviations higher on exams.

Instructors may have concerns about making a resource such as the IMVH available if they believe students may substitute away from lectures or other more productive studying methods \textcite{kay2012}.
Another concern is that forcing students to spend more time studying in one's class may worsen performance in other classes.
Our analysis provides some confidence that neither of these fears are first-order concerns.
We do not find evidence that students decrease their consumption of other forms of studying, nor do we find that students perform worse in other courses during the same quarter.
Our point estimates of the effect of treatment on takeup of other studying methods, though not statistically significantly different from zero, are \textit{positive} for most alternatives, suggesting that if any, students consider the videos complements to other forms of studying.
A potential mechanism might be that the videos help students realize what they \textit{don't} know, increasing the marginal benefit of subsequent studying.

A final concern is one of welfare.
In a neoclassical model, instructors cannot make their students better off by exogenously incentivizing quantities of studying they would not otherwise have chosen for themselves.
In an imperfect information model, which we think is more appropriate in our university classroom setting, instructors \textit{can} improve student welfare through intervention when information barriers lead to suboptimal time allocation decisions.
We observe two phenomena that support this model.
First, treatment students do not bunch at the cutoff for the grade incentive.
Second, video consumption remains much higher among treatment students in the term following conclusion of the experiment.

While there are many educational interventions that instructors could offer their students, the research on causal effects of educational interventions remains limited.
Our study serves as an example of a feasible research design that runs a lower risk of generating welfare losses for high performing students than does a class-wide experiment.
It is our hope, as educators ourselves, that more research will be conducted on the effectiveness of pedagogical technologies.

\printbibliography

%%% TABLES

\clearpage
\input{../tables/balance_table_takers.tex}

\clearpage
\input{../tables/firststage.tex}

\clearpage
\input{../tables/secondstage.tex}

\clearpage
\input{../tables/spillover_grades.tex}

\clearpage
\input{../tables/spillover_studying.tex}

\clearpage
\input{../tables/spillover_100b.tex}

\clearpage
\input{../tables/het_table.tex}

%%% PLOTS

% CDFs of videos watched by each treated vs control student
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Effect of grade incentive on videos watched}
\label{combo_cdf}
\includegraphics[width=1\textwidth, angle=0]{../plots/combo_cdf}
\footnotesize Top panels display the percent students in the \textit{Control}
 and \textit{Incentive} arms that watched at least $X$ unique videos (left) or hours of unique videos (right).
 Bottom panels display the differences between the two arms in the top panels with 95\% confidence intervals estimated by regressing an indicator for whether on the student watched at least $X\in\{0,\dots,X_{\max}\}$ unique videos (or hours of unique videos) on the student's treatment status.
\end{center}
\end{figure}

% Time series: videos over time
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Weekly video watching by treatment arm}
\label{timeseries}
\includegraphics[width=1\textwidth, angle=0]{../plots/tscombo}
\footnotesize Dashed lines represent Midterm 1, Midterm 2, and Final exams
\end{center}
\end{figure}

% video CDF
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Video watch rates by video and treatment arm, grouped by incentive}
\label{video_cdf}
\includegraphics[width=1\textwidth, angle=0]{../plots/bar_uviews}
\footnotesize Each bar represents the fraction of the treatment arm that watched a particular video.
Bars are in order of control group watch rates separately for incentivized and non-incentivized videos.
\end{center}
\end{figure}

\clearpage
\begin{figure}[t]
\begin{center}
\caption{Video watch rates by video and treatment arm, grouped by week}
\label{video_cdf_week}
\includegraphics[width=1\textwidth, angle=0]{../plots/bar_uviews_week}
\footnotesize Each bar represents the fraction of the treatment arm that watched a particular video.
Bars are in order of control group watch rates separately for each week the corresponding content was covered in lecture, as listed in the syllabus.
\end{center}
\end{figure}


% *****************************************************************
% Appendix
% *****************************************************************

\clearpage

\section*{Appendix}

\renewcommand{\thesubsection}{\Alph{subsection}}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\subsection{Additional experiment details}\label{subsec:additional_details}

In this section we outline additional experiment details that could prove useful for replication or understanding our analysis choices.

\subsubsection{Randomization} \label{subsubsec:a_randomization}
Students were assigned to treatment arms using a matched pairs design, a special case of blocked randomization in which each block contains exactly two units, one treated and one control.
Several authors detail how matched pair designs can improve the \textit{ex ante} precision of treatment effect estimates (versus complete randomization) by matching treatment units whose potential outcomes are similar (e.g.~\cite{ir2015,ai2017}).

We were unable to observe most pretreatment covariates until after the experiment had concluded because of student privacy considerations, thereby making it impossible to block on these variables.
We learned from the previous cohorts' data that between the first midterm score and math quiz score, both observable at the time of randomization, the midterm score predicted significantly more variation in the final exam score.
Hence, we stratified on midterm score when assigning treatment.
While we could have used an alternative method (e.g.\ matching methods) that take into consideration multiple covariates when assigning treatment, we opted for a simpler design given the high correlation between midterm and math quiz score and the comparatively high number of missing observations for the latter assessment (the math quiz was given on the second class day and so before some students enrolled in the class).

We assigned treatment shortly after issuing the first midterm exam grades, which occurred during the fourth week of the quarter.
To assign treatment, we ordered the students by exam score, then paired students along this ordering for students below the median.
Within pairs, we randomly assigned one student to \textit{Incentive}, the other to \textit{Control}.
By construction, these two arms were \textit{ex ante} balanced on midterm exam score, and we verified at time of treatment that the arms were also balanced on math quiz score.
Since this randomization was performed independently across year cohorts, by construction, the samples were also balanced on year.

Although our treatment assignment method provides a better chance of balance than does simple random sampling, by random chance and through non-random attrition, it is possible that the two treatment arms vary on \textit{ex post} observable and unobservable covariates that are correlated with the outcomes of interest, thereby confounding our treatment effect estimates.
The primary cause of attrition was withdrawing from the course, which reduced our experiment sample by 35 students before the second midterm and an additional 21 students before the final exam.
A 13\% withdrawal rate is in line with those observed in previous quarters.
Another cause of attrition, albeit not from the course, is age: four students under the age of 18 during the experiment were removed from the analysis dataset.
Additionally, seven students opted out of having their data included in the experiment analysis.

Since neither the students' intent to withdraw, age, nor opt-out preferences were observable at the time of treatment assignment, we could not \textit{ex ante} balance this attrition across treatment arms.
If students attrited non-randomly, that is, decided to attrite depending on their treatment status, then our treatment effect estimates would be biased.
Fortunately, despite 8\% attrition before the second midterm and 13\% before the final exam, the two treatment arms below the median are balanced on all observable pretreatment covariates, as shown in Table~\ref{balance_table_takers}, which gives us confidence that the \textit{Control} arm is a good counterfactual for the \textit{Incentive} arm.

\subsubsection{Attrition Details} \label{subsubsec:a_attrition}

Students could have attrited in three ways.
The first, and largest source of attrition was withdrawing from the course.
Most course withdrawals occurred \textit{before} students learned about their treatment status (19 Controls and 9 Treated) as the university's penalty-free drop deadline was the Friday before the Monday when treatment status was revealed.
Second, students under the age of 18 at the start of the experiment were excluded due to the IRB protocol.
Finally, students who opted out of having their data included in the experiment were excluded.
The analysis sample was prepared and anonymized by a campus-based independent education research organization, per IRB requirement, which removed four minors and seven opt-outs from the sample and merged demographic variables before returning the anonymized data to the research team.\footnote{In total, five \textit{Above median}, three \textit{Control}, and three \textit{Incentive} students were removed for age or opting-out.}

%Zack:  Please remove the 100B line from the Table (every time I try to do this, I destroy the Table!)  For "Below Median=436, should be <=Median (since there are 425 above median and 436 below median as currentluy labelled)?

\begin{figure}[t]
	\begin{center}
		\caption{Attrition by treatment arm}
		\label{attritchart}
		\footnotesize
		\begin{tikzpicture}[
			node distance=0mm,
			every node/.style = {shape=rectangle, draw=none, text width = 30mm, minimum height = 1em, align=center},
			pval/.style = {shape=rectangle, draw=none, inner sep=0mm, outer sep=0mm, minimum height=1em, align=left},
			cmnt/.style = {shape=rectangle, draw=none, inner sep=0mm, outer sep=0mm, minimum height=1em, align=right},
			level 1/.style = {sibling distance=50mm},
			level 2/.style = {sibling distance=35mm}, edge from parent fork down, edge from parent/.style = {draw, semithick, -latex},
			]
			\begin{scope}
				\node [text width = 50mm] (L1) {Midterm 1 takers \\ eligible for experiment: 861}
				child {node (L2) {Above median: 425}
					child {node (L3) {\textit{Above median}}
						child {node (L4) {420}
							child {node (L45) {418 (100\%)}
								child {node (L5) {417 (99.8\%)}
									child {node (L6) {415 (99.3\%)}
										child {node (L7) {284 (67.9\%)}
				}}}}}}}
				child {node {Below or equal to median: 436}
					child {node {\textit{Control}: 218}
						child {node {215}
							child {node {196 (100\%)}
								child {node {193 (98.5\%)}
									child {node {184 (93.9\%)}
										child {node {114 (58.2\%)}
					}}}}}}
					child {node {\textit{Incentive}: 218}
						child {node (R4) {215}
							child {node (R45) {206 (100\%)}
								child {node (R5) {202 (98.1\%)}
									child {node (R6) {190 (92.2\%)}
										child {node (R7) {100 (48.5\%)}
				}}}}}}};
			\end{scope}
			% row labels, from bottom to top
			\node[cmnt] (l7) [left=3mm of L7] {Took Micro B};
			\node[cmnt] (l6) [left=3mm of L6] {Took Final Exam};
			\node[cmnt] (l5) [left=3mm of L5] {Took Midterm 2};
			\node[cmnt] (l45) [left=3mm of L45] {Observed \\ treatment status};
			\node[cmnt] (l4) [left=3mm of L4] {Opt-in and \\ 18 or older};
			\node[cmnt] (l3) [left=3mm of L3] {Treatment arm};
			\node[cmnt] (l2) [left=of l3.east |- L2.west] {Midterm score};
			%pvals, from top to bottom
			\node[pval] (r5) [right=3mm of R5] {$p=0.75$};
			\node[pval] (r6) [right=3mm of R6] {$p=0.52$};
			\node[pval] (r7) [right=3mm of R7] {$p=0.05$};
		\end{tikzpicture}
		Percentages in parentheses are the portion of students who observe their treatment status and took the second midterm, final exam, and enrolled in Micro B, calculated separately for each treatment arm.
		The p-values are from a two-sample t-test of the equality of attrition rates between the \textit{Control} and \textit{Incentive} arms at each stage.
	\end{center}
\end{figure}


\subsubsection{Selection of control variables} \label{subsubsec:a_selection}

In this section we discuss how we select control variables included in our linear models.

Equation~\ref{eq:itt_spec} includes a vector of control variables related linearly to the outcomes of interest.
Although $d_i$, the treatment indicator is randomly assigned and in expectation $d_i$ is orthogonal to all observed and unobserved pretreatment covariates, in small samples stochastic imbalances can occur, which if controlled for can reduce bias of the treatment effect estimator~\parencite{ai2017}.
Even if perfect balance is achieved, controlling for orthogonal covariates can improve precision of the treatment effect estimator if the covariates can predict unexplained variance in the outcome.

By definition it is not possible to guarantee balance on unobserved covariates.
As discussed in Appendix~\ref{subsubsec:a_randomization}, we mechanically balanced the treatment arms on first midterm score, one of the few observables at the time of treatment assignment, with our knowledge from previous cohorts' data that the first midterm score explains a significant amount of variance in final exam score.
Hence, in our estimation strategies including controls, we always include the first midterm score and year, following the recommendations of \textcite{bm2009} to control for all covariates used to seek balance when assigning treatment.

For variables unobservable at time of randomization but observable at time of analysis, we lack the luxury of guaranteed balance by construction, nor is it clear \textit{ex ante}, beyond our intuition, which will predict variation in the outcome variables of interest.
On one hand, failing to control for valid predictors reduces statistical power.
On the other hand, hand-picking control variables increases researcher degrees of freedom, risking increasing the prevalence of Type I errors~\parencite{sns2011}.
As such, in addition to a model without controls beyond the ones used for treatment assignment (year and midterm score), we fit a second model that includes a vector of linear controls chosen using the post-double-selection (PDS) procedure introduced by \textcite{bch2014a}.

PDS is a two step process in which first, model covariates are selected in an automated, principled fashion, and second, the model coefficients of interest are estimated while controlling for those selected covariates.
The first step involves predicting, separately, both the outcome of interest (e.g., videos watched) and treatment status using lasso regression, which shrinks coefficient estimates towards zero.
Note that since treatment is randomly assigned, the lasso should shrink most, if not all, of the coefficients towards zero when predicting treatment status.
Next, the researcher takes the union of all covariates with non-zero coefficients and includes these covariates as controls in her model.
With her control variables selected, she can now estimate treatment effects with reduced bias relative to including controls with less empirical rationale.

In Table~\ref{controlvars_desc}, we describe all covariates observable in our study.
In Table~\ref{controlvars_selected_itt}, we describe the covariates selected as controls for estimating the effect of treatment on each outcome variable of interest.
All models include year and first midterm score as controls.
To ensure these controls are ``selected'' by the PDS procedure, we partialed out these controls from the first step prediction models by residualizing both sides of the equation as described in \textcite{bch2014b}.


\subsection{LATE estimators using Neyman's repeated sampling approach} \label{subsec:a_neyman_late}
%Zack--I think we should remove the matched-pairs analysis from the paper and put it all GitHub.  Can we create a section of GitHub just for the matched-pairs analysis?  Can you make a new document with all of this material?

In this section we derive LATE estimators using the repeated sampling approach of \textcite{neyman1923}, which considers each pair as an independent, completely randomized experiment.

Similar to a Wald estimator, the point estimate of the LATE is the mean within-pair difference in outcome divided by the mean within-pair difference in videos:

\begin{equation} \label{eq:neyman_late_spec}
	\wh{\gamma} = \frac{\wb{\Delta y}}{\wb{\Delta v}} = \frac{\frac{1}{J}\sum_{j=1}^{J} \Delta y_j }{\frac{1}{J}\sum_{j=1}^{J} \Delta v_j} = \frac{\wb{y_I} - \wb{y_C}}{\wb{v_I} - \wb{v_C}}
\end{equation}

where $y$ is the outcome of interest (grades) and $v$ is the number of videos, both indexed by pair $j\in J$ and treatment status $C$ or $I$ for \textit{Control} or \textit{Incentive}, respectively.

We use the delta method to calculate the approximate standard error of $\wh{\gamma}$.
First, we define the following normally-distributed random variables:
\begin{equation}
\begin{split}
Y = \wb{y_I} - \wb{y_C} \sim \mathcal{N}(\mu_Y, \sigma^2_Y) \\
V = \wb{v_I} - \wb{v_C} \sim \mathcal{N}(\mu_V, \sigma^2_V)
\end{split}\label{eq:equation1}
\end{equation}

Using a first-order Taylor expansion and letting $g() = \frac{Y}{V}$, we have:
\begin{equation}
\begin{split}
	\text{Var}(g) & = \operatorname{E}[(g - \operatorname{E}(g))^2] \\
	& \approx \operatorname{E}[(g(\theta) + (Y-\theta_Y)g'_Y(\theta) + (V-\theta_V)g'_V(\theta) - g(\theta))^2] \\
	& = \operatorname{E}[(Y-\theta_Y)^2 (g'_Y(\theta))^2 + (V-\theta_V)^2 (g'_V(\theta))^2 + 2(Y-\theta_Y)(V-\theta_V)g'_Y(\theta)g'_V(\theta)] \\
	& = \text{Var}(Y)(g'_Y(\theta))^2 + \text{Var}(V)(g'_V(\theta))^2 + 2\text{Cov}(Y,V)g'_Y(\theta)g'_V(\theta)
\end{split}\label{eq:equation2}
\end{equation}

Expanding about $\theta=(\theta_Y,\theta_V)=(\mu_Y,\mu_V)$ and letting $g'_Y(\theta) = \mu_V^{-1}$ and $g'_V(\theta) = \frac{-\mu_Y}{\mu_V^{-2}}$:
\begin{equation} \label{eq:var_expanded}
\begin{split}
	\text{Var}(g) & \approx \frac{1}{\mu_V^2}\text{Var}(Y) + \frac{\mu_Y^2}{\mu_V^4}\text{Var}(V) + 2\frac{-\mu_Y}{\mu_V^{-2}}\text{Cov}(Y,V) \\
	& = \frac{\mu_Y^2}{\mu_V^2}(\frac{\sigma_Y^2}{\mu_Y^2} + \frac{\sigma_V^2}{\mu_V^2} - 2\frac{\text{Cov}(Y,V)}{\mu_Y \mu_V})
\end{split}
\end{equation}

%We use the following variance estimators of $Y$ and $V$ from Equation~\ref{se_imai}:
\begin{equation}
\begin{split}
\wh{\text{Var}(Y)} & = \wh{\sigma_Y^2} = \frac{1}{J(J-1)}\sum_{j=1}^{J} (\Delta y_j - \wb{\Delta y})^2 \\
\wh{\text{Var}(V)} & = \wh{\sigma_V^2} = \frac{1}{J(J-1)}\sum_{j=1}^{J} (\Delta v_j - \wb{\Delta v})^2 \\
\wh{\text{Cov}(Y,V)} & = \wh{\sigma_{YV}} = \frac{1}{J(J-1)}\sum_{j=1}^{J} (\Delta y_j - \wb{\Delta y})(\Delta v_j - \wb{\Delta v})
\end{split}\label{eq:equation3}
\end{equation}

and the following estimators for the population means of $Y$ and $V$:
\begin{equation}
\begin{split}
\wh{\mu_Y} & = \operatorname{E}(\mu_Y) = \wb{\Delta y} \\
\wh{\mu_V} & = \operatorname{E}(\mu_V) = \wb{\Delta v}
\end{split}\label{eq:equation4}
\end{equation}

Substituting these variance and means estimators into the final step of~\ref{eq:var_expanded}, we arrive at the standard error estimator for $\wh{\gamma}$:

\begin{equation}
	\wh{\sigma_{\gamma}} = \frac{\wb{\Delta y}}{\wb{\Delta v}} \sqrt{ \frac{\wh{\sigma_Y^2}}{\wb{\Delta y}^2} + \frac{\wh{\sigma_V^2}}{\wb{\Delta v}^2} - 2 \frac{\wh{\sigma_{YV}}}{\wb{\Delta y} \wb{\Delta v}} }\label{eq:equation5}
\end{equation}

\clearpage
\input{../tables/balance_table_all.tex}

\clearpage
\input{../tables/balance_table_matched.tex}

\clearpage
\begin{spacing}{1.0}
\begin{table} \centering \caption{Anderson-Rubin confidence sets}
\label{tab:arci_table}
\begin{threeparttable}
\begin{tabular}{ ccc }
\toprule
Outcome variable & Endogenous variable & Anderson-Rubin CI \\
\midrule
Midterm 2 score & 10 unique videos & [-0.001, 0.653] \\
Midterm 2 score & 1 hour videos & [-0.001, 0.392] \\
Final exam score & 10 unique videos & [0.000, 0.163] \\
Final exam score & 1 hour videos & [0.000, 0.102] \\
\bottomrule
\end{tabular}
\Fignote{This table displays Anderson-Rubin confidence sets at the 95\% confidence level for the 2SLS estimator $\hat{\gamma}$ from Equation~\ref{eq:secondstage_spec} including year dummies and first midterm score as controls. Outcomes are measured in standard deviations. Instrumented endogenous variables are measured in 10s of unique videos or hours of unique content.}
\end{threeparttable}
\end{table}
\end{spacing}

% \clearpage
% \begin{spacing}{1.0}
% \begin{table} \centering \caption{Regression Discontinuity estimates}
% \label{rdtable}
% \begin{threeparttable}
% \begin{tabular}{m{0.35\linewidth} *{3}{>{\centering\arraybackslash}m{0.1\linewidth}}}
% \toprule
% & Control Mean & (1) & (2) \\
% \midrule
% \multicolumn{4}{l}{\textbf{Panel A}: Midterm 2} \\
% \customlinespace \indentrow{Unique videos} & 22.7 & 9.12\sym{***} & 9.34\sym{***} \\
% & & (3.35) & (3.42) \\
% \customlinespace \indentrow{Midterm 2 score} & 0.73 & 0.06 & 0.07 \\
% & & (0.20) & (0.19) \\
% \midrule
% Observations & & 812 & 779 \\
% \midrule
% \multicolumn{4}{l}{\textbf{Panel B}: Final exam} \\
% \customlinespace \indentrow{Unique videos} & 32.0 & 24.16\sym{***} & 24.95\sym{***} \\
% & & (2.39) & (2.48) \\
% \customlinespace \indentrow{Final exam score} & 0.78 & 0.11 & 0.14 \\
% & & (0.18) & (0.19) \\
% \midrule
% Observations & & 789 & 747 \\
% \bottomrule
% \end{tabular}
% \Fignote{This table reports coefficients on $\tau(c)$ from Equations \ref{rd_spec}. Panel A includes Midterm 2-takers while Panel B includes Final Exam-takers. Model (1) includes all students while (2) contains only students whose pair did not attrite. Standard errors are in parentheses. ***, **, and * indicate significance at the 1, 5, and 10 percent critical levels, respectively.}
% \end{threeparttable}
% \end{table}
% \end{spacing}

\clearpage
\input{../tables/controlvars_desc.tex}

\clearpage
\input{../tables/controlvars_selected_itt.tex}

\clearpage
\input{../tables/controlvars_selected_iv.tex}

%%% PLOTS

% Histogram of videos eligible for incentive
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Distribution of videos counted towards incentive}
\label{hist_incent}
\includegraphics[width=1\textwidth, angle=0]{../plots/hist_v_incent}
\footnotesize This plot includes only videos that would have counted towards the earning the grade incentive.
Students were required to watch 40 unique of 48 eligible videos between the first midterm and final exam to earn the grade incentive.
91\% of \textit{Incentive} students met the requirements for the grade incentive versus 11\% of \textit{Control} students.
\end{center}
\end{figure}

% Time series: videos over time by exam topic
% This shows that students watch videos when most useful
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Weekly video watching by exam topic}
\label{timeseries_by_exam} % TODO add reference in paper
\includegraphics[width=1\textwidth, angle=0]{../plots/tscombo_exam}
\footnotesize Dashed lines represent Midterm 1, Midterm 2, and Final exams.
\end{center}
\end{figure}

% Local smoothing: midterm 2 exam scores

%Zack --can we remove the matched pairs graph and put it in the matched pairs paper/analysis?  In these graphs I dont like that we also call the above median group, the "control" group.  Since we standardize exam scores, I assume it is the actual controls standard deviation that we use. I lean towards just showing the treatment and controls in these graphs.  It may be useful to see there are no weird jumps in the exam scores at the median between the "controls" and the "above median" and it may also help people to understand our experiment better.  If you think this is important, then can you change the color of the "above median" line and add it to the legend?
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Effects of treatment along first midterm score, by midterm 2}
\label{locallin_mid2}
\includegraphics[width=1\textwidth, angle=0]{../plots/lpolymid2}
\footnotesize Videos (top) includes unique videos watched before the second midterm exam.
Exam scores (bottom) are measured in control standard deviations.
Confidence bands represent 95\% confidence intervals of the conditional mean outcome.
The left plots includes all students who took the second midterm while the right plots exclude any students whose matched pair attrited.
\end{center}
\end{figure}

% Local smoothing: final exam scores
%same comment as above
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Effects of treatment along first midterm score, by final}
\label{locallin_final}
\includegraphics[width=1\textwidth, angle=0]{../plots/lpolyfinal}
\footnotesize Videos (top) includes unique videos watched before the final exam.
Exam scores (bottom) are measured in control standard deviations.
Confidence bands represent 95\% confidence intervals of the conditional mean outcome.
The left plots includes all students who took the final exam while the right plots exclude any students whose matched pair attrited.
\end{center}
\end{figure}

% Treatment effects vs Midterm 1 score
% \clearpage
% \begin{figure}[t]
% \begin{center}
% \caption{Effect of treatment on videos watched and exam scores by Midterm 1 score}
% \includegraphics[width=1\textwidth, angle=0]{../plots/combo_binscatter.pdf}
% \footnotesize Test scores in standard deviation units. Each point comprises 5-percentile bins along the domain. The control points displayed include both \textit{Control} and \textit{Above median} arms.
% \end{center}
% \end{figure}

% Histogram of `binge watching' - distribution of max(weekly videos watched)
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Distribution of max videos watched in one week}
\label{hist_binge}
\includegraphics[width=1\textwidth, angle=0]{../plots/hist_maxweek}
\footnotesize These plots help illustrate potential ``binge watching'' behavior.
Compared to the \textit{Control} students, \textit{Incentive} students are more likely to watch 40 or more unique videos in a week, which occurs in the weeks preceding the final and not the second midterm.
\end{center}
\end{figure}

% Regression discontinuity: no manipulation
% \clearpage
% \begin{figure}[t]
% \begin{center}
% \caption{Distribution of Midterm 1 scores}
% \label{mid1dist}
% \includegraphics[width=1\textwidth, angle=0]{../plots/mid1dist.pdf}
% \footnotesize Midterm scores are measured in control standard deviations. This histogram includes all students who took the final exam. As such, this plot would allow one to observe bunching if treatment caused differential attrition or manipulation of one's midterm score.
% \end{center}
% \end{figure}

% video CDF by duration
\clearpage
\begin{figure}[t]
\begin{center}
\caption{Video watch rates by video and treatment arm, grouped by incentive, ordered by video duration}
\label{video_cdf_duration}
\includegraphics[width=1\textwidth, angle=0]{../plots/bar_uviews_duration}
\footnotesize Each bar represents the fraction of the treatment arm that watched a particular video.
Bars are in order of video duration separately for incentivized and non-incentivized videos.
\end{center}
\end{figure}


\end{document}
