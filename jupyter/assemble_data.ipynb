{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View this notebook here:\n",
    "https://nbviewer.jupyter.org/github/zagoodman/microeconomics_videos/blob/master/jupyter/assemble_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prep\" data-toc-modified-id=\"Prep-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prep</a></span></li><li><span><a href=\"#Merge-anonymized-data\" data-toc-modified-id=\"Merge-anonymized-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Merge anonymized data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Demographic-data\" data-toc-modified-id=\"Demographic-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Demographic data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preceding-term-vars\" data-toc-modified-id=\"Preceding-term-vars-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Preceding term vars</a></span></li><li><span><a href=\"#Student-level\" data-toc-modified-id=\"Student-level-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Student-level</a></span></li><li><span><a href=\"#GPA-and-course-units\" data-toc-modified-id=\"GPA-and-course-units-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>GPA and course units</a></span><ul class=\"toc-item\"><li><span><a href=\"#Concurrent-term\" data-toc-modified-id=\"Concurrent-term-2.1.3.1\"><span class=\"toc-item-num\">2.1.3.1&nbsp;&nbsp;</span>Concurrent term</a></span></li><li><span><a href=\"#Following-term\" data-toc-modified-id=\"Following-term-2.1.3.2\"><span class=\"toc-item-num\">2.1.3.2&nbsp;&nbsp;</span>Following term</a></span></li></ul></li></ul></li><li><span><a href=\"#Treatment-and-outcome-data\" data-toc-modified-id=\"Treatment-and-outcome-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Treatment and outcome data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Student-level\" data-toc-modified-id=\"Student-level-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Student level</a></span></li></ul></li></ul></li><li><span><a href=\"#Videos\" data-toc-modified-id=\"Videos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Videos</a></span><ul class=\"toc-item\"><li><span><a href=\"#100A\" data-toc-modified-id=\"100A-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>100A</a></span></li><li><span><a href=\"#Week-level\" data-toc-modified-id=\"Week-level-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Week-level</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Export</a></span></li><li><span><a href=\"#100B\" data-toc-modified-id=\"100B-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>100B</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Export</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file takes anonymized data from the T+LC, cleans them, and returns analysis-ready data frames:\n",
    "1. `id-year_level_data.csv`: treatment and exam score data\n",
    "2. `dem_concurrent.csv`: demographic data and GPA/courseload during the quarter of the experiment\n",
    "3. `dem_nextquarter.csv`: demographic data and GPA/courseload during the quarter following treatment\n",
    "\n",
    "All code in this file is in Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_flavor as pf\n",
    "\n",
    "@pf.register_dataframe_accessor('z')\n",
    "class MyFlavor(object):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self._df = df\n",
    "\n",
    "    def get_unique_students(self, var1 = \"id\", var2 = \"year\"):\n",
    "        df = self._df\n",
    "        x = len(df[[var1, var2]].drop_duplicates())\n",
    "        print(\"N unique students: {}\".format(x))\n",
    "        return x\n",
    "    \n",
    "    def get_vars(self):\n",
    "        df = self._df\n",
    "        cols = [x for x in df if x in tlcvarnames]\n",
    "        return df[cols].rename(columns = tlcvarnames)\n",
    "    \n",
    "tlcvarnames = {'id': 'id',\n",
    "               'year': 'year',\n",
    "               'de id': 'id', \n",
    "               'deid': 'id',\n",
    "               'previous_cum_gpa': 'prev_cumgpa',\n",
    "               'term_code_econ': 'term',\n",
    "               'term-econ100a': 'term',\n",
    "               'term code courses': 'term_courses',\n",
    "               'term_precoursegpa': 'term_pregpa',\n",
    "               'term code econ 100a': 'termecon',\n",
    "               'apct_type_desc': 'apptype',\n",
    "               'ethnicity_grouped': 'ethnicity',\n",
    "               'gender': 'gender',\n",
    "               'term_code_econ': 'term',\n",
    "               'year_econ': 'year',\n",
    "               'year - econ 100a': 'year',\n",
    "               'year - econ100a': 'year',\n",
    "               'year-econ100a': 'year',\n",
    "               'year-zack': 'year',\n",
    "               'measure names': 'measure',\n",
    "               'measure values': 'values',\n",
    "               'Class Units - Letter Grade': 'units_letter',\n",
    "               'Class Units - P/NP': 'units_pnp',\n",
    "               'Class Units - Withdrawn': 'units_w',\n",
    "               'GPA - Classes Letter Grade': 'gpa_letter',\n",
    "               'GPA - Classes Letter Grade - No Econ': 'gpa_letter_sansecon',\n",
    "               'GPA - Classes Letter Grade - No Econ 100A': 'gpa_letter_sans100a',\n",
    "               'GPA - Classes Letter Grade - Only Econ - No Econ 100A': 'gpa_econ_sans100a',\n",
    "               'N Classes - Letter Grade': 'nclass_letter',\n",
    "               'N Classes - Not Passed': 'nclass_np',\n",
    "               'N Classes - P/NP': 'nclass_pnp',\n",
    "               'N Classes - Passed': 'nclass_p',\n",
    "               'N Classes - Withdrawn': 'nclass_w'\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge anonymized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preceding term vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre 100A (term before) cumulative gpa. Missing for all entering freshmen or first time UCSD students\n",
    "\n",
    "dfd = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Pre Course GPA-FA18.xlsx\")\n",
    "dfd = pd.concat([dfd, pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Pre Course GPA-FA19.xlsx\")], 0)\n",
    "dfd.columns = [x.lower() for x in dfd.columns]\n",
    "dfd = dfd.z.get_vars()\n",
    "dfd.loc[dfd.term == 'FA18', 'year'] = 2018\n",
    "dfd.loc[dfd.term == 'FA19', 'year'] = 2019\n",
    "# reorder and drop term\n",
    "dfd = dfd[['id', 'year', 'term_pregpa', 'prev_cumgpa']]\n",
    "\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students(var2='id')\n",
    "display(dfd.isnull().sum())\n",
    "#dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ethnicity, gender, transfer status\n",
    "\n",
    "df = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Student-level.xlsx\")\n",
    "df.rename(str.lower, axis=1, inplace=True)\n",
    "df = df.z.get_vars()\n",
    "df['transfer'] = np.array(df.apptype == 'Transfer Student', dtype='int')\n",
    "df.loc[df.gender == 'Men (Cis & Trans)', 'gender'] = 'm'\n",
    "df.loc[df.gender == 'Women (Cis & Trans)', 'gender'] = 'f'\n",
    "df.loc[df.gender == 'Data Unavailable', 'gender'] = 'u'\n",
    "# drop term and reorder\n",
    "df = df[['id', 'year', 'ethnicity', 'gender', 'transfer', 'apptype']]\n",
    "\n",
    "# merge with dfd\n",
    "dfd = df.merge(dfd, how='outer', on=['id', 'year'])\n",
    "\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students()\n",
    "print(dfd.isnull().sum())\n",
    "#dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPA and course units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Student-quarter-level_FA18-WI19.xlsx\")\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "df = df.z.get_vars()\n",
    "df2 = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Student-quarter-level_FA19-WI20.xlsx\")\n",
    "df2.columns = [x.lower() for x in df2.columns]\n",
    "df2 = df2.z.get_vars()\n",
    "\n",
    "df = pd.concat([df, df2], 0)\n",
    "print(len(df))\n",
    "display(pd.crosstab(df.year, df.term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concurrent term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concurrent data\n",
    "\n",
    "dfc = df.loc[df.term_courses.isin([\"FA18\", \"FA19\"])]\n",
    "dfc = dfc[['id', 'year', 'measure', 'values']]\n",
    "\n",
    "# reshape wide\n",
    "dfc = dfc.pivot_table(index=['id', 'year'], columns='measure', values='values').reset_index()\n",
    "dfc = dfc.z.get_vars()\n",
    "\n",
    "# merge with rest of demographic data\n",
    "dfd = dfd.merge(dfc, how='outer', on=['id', 'year'])\n",
    "\n",
    "# add zeros where needed\n",
    "for x in ['units_pnp', 'units_w']:\n",
    "    dfd.loc[dfd[x].isnull(), x] = 0\n",
    "\n",
    "# check and explore data\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students()\n",
    "print(dfd.dtypes)\n",
    "print(dfd.isnull().sum())\n",
    "\n",
    "# export\n",
    "dfd.to_csv(\"../data/generated/dem_concurrent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## following quarter's data\n",
    "\n",
    "dff = df.loc[df.term_courses.isin([\"WI19\", \"WI20\"])]\n",
    "dff = dff[['id', 'year', 'measure', 'values']]\n",
    "\n",
    "# reshape wide\n",
    "dff = dff.pivot_table(index=['id', 'year'], columns='measure', values='values').reset_index()\n",
    "dff = dff.z.get_vars()\n",
    "\n",
    "# merge with rest of demographic data\n",
    "dfd = dfd.iloc[:, 0:8]\n",
    "dfd = dfd.merge(dff, how='outer', on=['id', 'year'])\n",
    "\n",
    "# add zeros where needed\n",
    "for x in ['units_letter', 'units_pnp', 'units_w', \\\n",
    "          'nclass_letter', 'nclass_np', 'nclass_pnp', \\\n",
    "          'nclass_p', 'nclass_w']:\n",
    "    dfd.loc[dfd[x].isnull(), x] = 0\n",
    "\n",
    "# check and explore data\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students()\n",
    "print(dfd.dtypes)\n",
    "print(dfd.isnull().sum())\n",
    "\n",
    "# export\n",
    "dfd.to_csv(\"../data/generated/dem_nextquarter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment and outcome data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/raw/DeID_all_pid-year-level_data with description.xlsx\")\n",
    "df.rename(columns={'DeID': 'id'}, inplace=True)\n",
    "print(len(df))\n",
    "df.z.get_unique_students()\n",
    "df = df[['id'] + [x for x in df.columns if x != 'id']]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think that's all I need for now...may add more later\n",
    "\n",
    "# drop the two duplicate entries\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "df.z.get_unique_students()\n",
    "\n",
    "# export\n",
    "df.to_csv(\"../data/generated/id-year_level_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = pd.read_csv('../data/raw/DeID_video-level_data.csv')\n",
    "dfv = dfv.loc[dfv.videoid.notnull()]\n",
    "dfv.loc[dfv.syllabus_week.isnull(), 'syllabus_week'] = -1\n",
    "\n",
    "# int columns\n",
    "for v in dfv.columns[7:-1]:\n",
    "    dfv[v] = dfv[v].apply(lambda x: int(x))\n",
    "    \n",
    "# date columns\n",
    "dfv['lastview'] = pd.to_datetime(dfv['last.view'])\n",
    "dfv['firstview'] = pd.to_datetime(dfv['first.view'])\n",
    "\n",
    "# unnecessary cols\n",
    "dfv.drop(['first.view', 'last.view', 'videocode', 'length_mins'], 1, inplace=True)\n",
    "\n",
    "# get treatment status\n",
    "df = pd.read_csv(\"../data/generated/id-year_level_data.csv\")\n",
    "df['arm'] = df.toberandomized + df.treated\n",
    "df = df[['id', 'year', 'arm', 'finalscore']]\n",
    "# display(df.head())\n",
    "\n",
    "# rename 'DeID' to 'id' in dfv, then merge treatment status\n",
    "dfv.rename(columns={'DeID': 'id'}, inplace=True)\n",
    "dfv = dfv.merge(df, on=['id', 'year'], how='left')\n",
    "# dfv.head()\n",
    "\n",
    "print(dfv.isnull().sum())\n",
    "dfv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore\n",
    "\n",
    "dftmp = dfv[['syllabus_week', 'incentivized', 'videoid']].drop_duplicates()\n",
    "print('This many relevant videos: {}'.format(dftmp.syllabus_week.notnull().sum()))\n",
    "print('This many incentivized videos: {}'.format((dftmp.incentivized > 0).sum()))\n",
    "print('\\nThis many relevant videos by syllabus week:')\n",
    "print(dftmp.syllabus_week.value_counts().sort_index())\n",
    "print('\\nThis many students per year:')\n",
    "print(dfv[['id', 'year']].drop_duplicates().year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration per video (seconds)\n",
    "\n",
    "# We can only observe video opens, so calculate \"duration\" as if every video watched\n",
    "#  in its entirety. \n",
    "\n",
    "# Calc duration in seconds\n",
    "dfv['duration'] = dfv.minutes * 60 + dfv.seconds\n",
    "\n",
    "# Distribution of durations?\n",
    "plt.hist(dfv.duration, 50)\n",
    "plt.title('Distribution of video durations (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen helpful flags\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# first flag\n",
    "dfv['firstflag'] = dfv.groupby(['id', 'year', 'videoid']).firstview.transform('min') == dfv.firstview\n",
    "\n",
    "# before midterm 1\n",
    "dfv['b4_mid1'] = np.array(\\\n",
    "                 ((dfv.year == 2018) & (dfv.firstview <= dt(2018, 10, 19, 19, 20))) | \\\n",
    "                 ((dfv.year == 2019) & (dfv.firstview <= dt(2019, 10, 23, 21, 20))), dtype='int')\n",
    "\n",
    "# before midterm 2\n",
    "dfv['b4_mid2'] = np.array(\\\n",
    "                 ((dfv.year == 2018) & (dfv.firstview <= dt(2018, 11, 19, 19, 20))) | \\\n",
    "                 ((dfv.year == 2019) & (dfv.firstview <= dt(2019, 11, 13, 21, 20))), dtype='int')\n",
    "\n",
    "# before final\n",
    "dfv['b4_final'] = np.array(\\\n",
    "                 ((dfv.year == 2018) & (dfv.firstview <= dt(2018, 12, 8, 18, 0))) | \\\n",
    "                 ((dfv.year == 2019) & (dfv.firstview <= dt(2019, 12, 7, 14, 30))), dtype='int')\n",
    "\n",
    "# counts towards grade incentive\n",
    "dfv['incent_counts'] = np.array(\\\n",
    "                       ~(dfv.b4_mid1) & (dfv.b4_final) & (dfv.incentivized >= 1), dtype='int')\n",
    "\n",
    "# adjust incentive to int y/n\n",
    "dfv['incentivized'] = np.array(dfv.incentivized >= 1, dtype='int')\n",
    "\n",
    "for v in ['b4_mid1', 'b4_mid2', 'b4_final', 'incentivized', 'incent_counts']:\n",
    "    print(dfv[v].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum duration for each student: total duration, and total unique duration. \n",
    "# only keep course relevant videos.\n",
    "# do this for: pre mid1, pre mid2, pre final\n",
    "\n",
    "# init df at DeID-level\n",
    "mergevars = ['id', 'year']\n",
    "dfd = dfv.loc[:, mergevars].drop_duplicates()\n",
    "\n",
    "# relevant only\n",
    "print('This many videos were irrelevant: {}'.format(sum(dfv.relevant_100a == 0)))\n",
    "print('This percent of videos were irrelevant: {}'.format(sum(dfv.relevant_100a == 0) / len(dfv)))\n",
    "dfr = dfv.loc[dfv.relevant_100a > 0].copy()\n",
    "\n",
    "# relevant duration, all (not unique) - before mid1, mid2, final\n",
    "dfagg = dfr.copy()\n",
    "dfagg['duration_mid1'] = dfagg.duration * dfagg.b4_mid1\n",
    "dfagg['duration_mid2'] = dfagg.duration * dfagg.b4_mid2\n",
    "dfagg['duration_final'] = dfagg.duration * dfagg.b4_final\n",
    "dfaggd = dfagg.groupby(mergevars)[['duration_mid1', 'duration_mid2', 'duration_final']].agg('sum').reset_index()\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# relevant duration, unique - before mid1, mid2, final\n",
    "# keep only max duration per videoid\n",
    "dfaggd = dfagg.groupby(['id', 'year', 'videoid'])[['duration_mid1', 'duration_mid2', 'duration_final']].agg('max').reset_index()\n",
    "dfaggd = dfaggd.groupby(mergevars)[['duration_mid1', 'duration_mid2', 'duration_final']].agg('sum').reset_index()\n",
    "dfaggd.columns = ['id', 'year', 'duration_mid1_u', 'duration_mid2_u', 'duration_final_u']\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# units of hours\n",
    "for v in dfd.columns[2:]:\n",
    "    dfd[v] = dfd[v] / 3600\n",
    "\n",
    "# sanity check\n",
    "print('Sanity checks: should be 0 first.')\n",
    "print(sum(dfd.duration_final < dfd.duration_final_u - .01))\n",
    "print(sum(dfd.duration_final <= dfd.duration_final_u + .01))\n",
    "\n",
    "# do the same thing for counts\n",
    "\n",
    "# count all\n",
    "countvars = ['b4_mid1', 'b4_mid2', 'b4_final', 'incentivized', 'incent_counts']\n",
    "renamevars = ['mid1', 'mid2', 'final', 'incentivized', 'incent_counts']\n",
    "dfaggd = dfr.groupby(mergevars)[countvars].agg('sum').reset_index()\n",
    "display(dfaggd.head())\n",
    "dfaggd.columns = mergevars + ['videos_' + x for x in renamevars]\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# count unique\n",
    "dfaggd = dfr.copy()\n",
    "for v in countvars:\n",
    "    dfaggd[v] = dfaggd.loc[dfaggd[v] == 1, 'videoid']\n",
    "dfaggd = dfaggd.groupby(mergevars)[countvars].agg('nunique').reset_index()\n",
    "display(dfaggd.head())\n",
    "dfaggd.columns = mergevars + ['videos_' + x + '_u' for x in renamevars]\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# sanity check\n",
    "print('Sanity checks: should be 0 first.')\n",
    "print(sum(dfd.videos_final < dfd.videos_final_u))\n",
    "print(sum(dfd.videos_final <= dfd.videos_final_u))\n",
    "\n",
    "display(dfd.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week-level\n",
    "\n",
    "Collapse at the student-week level for creating time series plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back students with no video watching\n",
    "df = pd.read_csv(\"../data/generated/id-year_level_data.csv\")\n",
    "df['arm'] = df.toberandomized + df.treated\n",
    "keepvars = ['id', 'year', 'arm', 'finalscore']\n",
    "df = df[keepvars]\n",
    "# display(df.head())\n",
    "\n",
    "# Outer merge treatment status\n",
    "dfv = dfv.merge(df, on=keepvars, how='outer')\n",
    "dfv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data aggregated at id-year-week-arm level\n",
    "\n",
    "\n",
    "# get week of year for each video watched\n",
    "# week = 0 is week experiment began (43)\n",
    "from datetime import date\n",
    "dfv['weekoy'] = dfv.loc[dfv.firstview.notnull()].\\\n",
    "    firstview.apply(lambda x: x.isocalendar()[1]) - 43\n",
    "# print(dfv.weekoy.value_counts())\n",
    "\n",
    "# init df at id-year-weekoy-arm-level\n",
    "mergevars = ['id', 'year', 'arm', 'weekoy']\n",
    "muidx = pd.MultiIndex.from_product([dfv['id'].drop_duplicates(), dfv['year'].drop_duplicates(), dfv.loc[dfv.weekoy.notnull(), 'weekoy'].unique()])\n",
    "dfdw = pd.DataFrame(index=muidx).reset_index()\n",
    "dfdw.columns = ['id', 'year', 'weekoy']\n",
    "dfdw = dfdw.merge(dfv[['id', 'year', 'arm', 'finalscore']].drop_duplicates(), on=['id', 'year'], how='inner')\n",
    "dfdw.sort_values(mergevars, inplace=True)\n",
    "print(len(dfdw[['id', 'year']].drop_duplicates()))\n",
    "# display(dfdw.head())\n",
    "\n",
    "\n",
    "# relevant videos\n",
    "dfr = dfv.loc[dfv.relevant_100a == 1].copy()\n",
    "\n",
    "# count all videos\n",
    "dfaggw = dfr.groupby(mergevars)[['relevant_100a', 'duration', 'pre_mid1', 'mid1_to_mid2', 'post_mid2']].\\\n",
    "    agg('sum').reset_index()\n",
    "dfaggw.columns = mergevars + ['videos', 'duration', 'videos_pre_mid1', 'videos_mid1_to_mid2', 'videos_post_mid2']\n",
    "dfdw = dfdw.merge(dfaggw, on=mergevars, how='left')\n",
    "\n",
    "# count unique, keeping first time watched video\n",
    "dfaggwd = dfr.sort_values(['id', 'year', 'firstview']).drop_duplicates(['id', 'year', 'arm', 'videoid'])\n",
    "dfaggwd = dfaggwd.groupby(mergevars)[['relevant_100a', 'duration', 'pre_mid1', 'mid1_to_mid2', 'post_mid2']].agg('sum').reset_index()\n",
    "dfaggwd.columns = mergevars + ['videosu', 'durationu', 'videos_pre_mid1u', 'videos_mid1_to_mid2u', 'videos_post_mid2u']\n",
    "dfdw = dfdw.merge(dfaggwd, on=mergevars, how='left')\n",
    "\n",
    "# int cols\n",
    "for c in dfdw.columns[5:]:\n",
    "    # fill in zeros where appropriate\n",
    "    dfdw[c] = dfdw[c].apply(lambda x: int(x) if x == x else 0)\n",
    "\n",
    "dfdw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dftmp = dfdw.groupby(['arm', 'weekoy']).agg('mean').reset_index()\n",
    "for c in ['videos_pre_mid1', 'videos_mid1_to_mid2', 'videos_post_mid2']:\n",
    "    plt.plot(dftmp.loc[dftmp.arm == 1].weekoy, dftmp.loc[dftmp.arm == 1, c], label='control')\n",
    "    plt.plot(dftmp.loc[dftmp.arm == 2].weekoy, dftmp.loc[dftmp.arm == 2, c], label='treated')\n",
    "    plt.title(c  + ', All')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "for c in ['videos_pre_mid1u', 'videos_mid1_to_mid2u', 'videos_post_mid2u']:\n",
    "    plt.plot(dftmp.loc[dftmp.arm == 1].weekoy, dftmp.loc[dftmp.arm == 1, c], label='control')\n",
    "    plt.plot(dftmp.loc[dftmp.arm == 2].weekoy, dftmp.loc[dftmp.arm == 2, c], label='treated')\n",
    "    plt.title(c  + ', Unique')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks\n",
    "print(dfdw.isnull().sum())\n",
    "print(len(dfdw.loc[dfdw.finalscore.isnull()].id.value_counts()))\n",
    "\n",
    "# export\n",
    "dfdw.to_csv('../data/generated/id-year-weekoy-arm_level.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export video data by updating the pid-year-level data\n",
    "\n",
    "df = pd.read_csv(\"../data/generated/id-year_level_data.csv\")\n",
    "dropvars = ['videos', 'relevant', 'videos_u', 'relevant_u',\n",
    "            'duration_all', 'duration_rel', 'duration_u',\n",
    "            'videos_b4_mid1_rel', 'videos_b4_mid1_relu',\n",
    "            'videos_b4_mid2_rel', 'videos_b4_mid2_relu', \n",
    "            'duration_mid1', \n",
    "            'duration_mid2',\n",
    "            'duration_final',\n",
    "            'duration_mid1_u',\n",
    "            'duration_mid2_u',\n",
    "            'duration_final_u',\n",
    "            'videos_mid1',\n",
    "            'videos_mid2',\n",
    "            'videos_final',\n",
    "            'videos_incentivized',\n",
    "            'videos_incent_counts',\n",
    "            'videos_mid1_u',\n",
    "            'videos_mid2_u',\n",
    "            'videos_final_u',\n",
    "            'videos_incentivized_u',\n",
    "            'videos_incent_counts_u']\n",
    "\n",
    "df = df.loc[:, [c for c in df.columns if c not in dropvars]]\n",
    "df.head()\n",
    "\n",
    "# merge dfd on df and add zeros when no videos observed\n",
    "df = df.merge(dfd, on=['id', 'year'], how='outer')\n",
    "for c in df.columns[-23:]:\n",
    "    df.loc[df[c].isnull(), c] = 0\n",
    "for c in df.columns[-10:]:\n",
    "    df[c] = df[c].apply(lambda x: int(x))\n",
    "\n",
    "print(df.isnull().sum())\n",
    "display(df.head())\n",
    "\n",
    "# save csv\n",
    "df.to_csv(\"../data/generated/id-year_level_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.videos_incent_counts_u.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 100B video history\n",
    "dfv = pd.read_csv('../data/raw/DeID_video-level_data_100b.csv')\n",
    "dfv = dfv.loc[dfv.videoid.notnull()]\n",
    "\n",
    "# keep only necessary vars\n",
    "dfv = dfv[['DeID', 'year', 'visit.id', 'first.view', 'last.view', 'videoid']]\n",
    "\n",
    "# merge duration\n",
    "lengths = pd.read_excel('../data/raw/video_lengths.xlsx')\n",
    "lengths = lengths[['videoid', 'minutes', 'seconds']]\n",
    "lengths['duration'] = lengths.minutes * 60 + lengths.seconds\n",
    "dfv = dfv.merge(lengths[['videoid', 'duration']], on='videoid', how='left')\n",
    "    \n",
    "# date columns\n",
    "dfv['lastview'] = pd.to_datetime(dfv['last.view'])\n",
    "dfv['firstview'] = pd.to_datetime(dfv['first.view'])\n",
    "\n",
    "# unnecessary cols\n",
    "dfv.drop(['first.view', 'last.view'], 1, inplace=True)\n",
    "\n",
    "# rename id col\n",
    "dfv.rename(columns={'DeID': 'id'}, inplace=True)\n",
    "\n",
    "print(dfv.isnull().sum())\n",
    "dfv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration per video (seconds)\n",
    "\n",
    "plt.hist(dfv.duration, 50)\n",
    "plt.title('Distribution of video durations (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum duration for each student: total duration, and total unique duration. \n",
    "\n",
    "# init df at DeID-level\n",
    "mergevars = ['id', 'year']\n",
    "dfd = dfv.loc[:, mergevars].drop_duplicates()\n",
    "\n",
    "# relevant duration, all (not unique)\n",
    "dfagg = dfv.copy()\n",
    "dfaggd = dfagg.groupby(mergevars)[['duration']].agg('sum').reset_index()\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# relevant duration, unique\n",
    "# keep only max duration per videoid\n",
    "dfaggd = dfagg.groupby(['id', 'year', 'videoid'])[['duration']].agg('max').reset_index()\n",
    "dfaggd = dfaggd.groupby(mergevars)[['duration']].agg('sum').reset_index()\n",
    "dfaggd.columns = ['id', 'year', 'duration_u']\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# units of hours\n",
    "for v in dfd.columns[2:]:\n",
    "    dfd[v] = dfd[v] / 3600\n",
    "\n",
    "# colnames\n",
    "dfd.columns = ['id', 'year', 'duration_b', 'duration_b_u']\n",
    "\n",
    "# sanity check\n",
    "print('Sanity checks: should be 0 first.')\n",
    "print(sum(dfd.duration_b < dfd.duration_b_u - .01))\n",
    "print(sum(dfd.duration_b <= dfd.duration_b_u + .01))\n",
    "\n",
    "\n",
    "# do the same thing for counts\n",
    "\n",
    "# count all\n",
    "dfagg['cnt'] = 1\n",
    "dfaggd = dfagg.groupby(mergevars)[['cnt']].agg('sum').reset_index()\n",
    "dfaggd.columns = ['id', 'year', 'videos_b']\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# count unique\n",
    "dfaggd = dfv.copy()\n",
    "dfaggd['videos_b_u'] = dfaggd.videoid\n",
    "dfaggd = dfaggd.groupby(mergevars)[['videos_b_u']].agg('nunique').reset_index()\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# sanity check\n",
    "print('Sanity checks: should be 0 first.')\n",
    "print(sum(dfd.videos_b < dfd.videos_b_u))\n",
    "print(sum(dfd.videos_b <= dfd.videos_b_u))\n",
    "\n",
    "display(dfd.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export video data by updating the pid-year-level data\n",
    "\n",
    "df = pd.read_csv(\"../data/generated/id-year_level_data.csv\")\n",
    "dropvars = ['videos_b', 'relevant_b', 'videos_u_b', \n",
    "            'relevant_u_b', 'duration_all_b', \n",
    "            'duration_rel_b', 'duration_u_b']\n",
    "\n",
    "df = df.loc[:, [c for c in df.columns if c not in dropvars]]\n",
    "display(df.head(2))\n",
    "\n",
    "# merge dfd on df and add zeros when no videos observed\n",
    "df = df.merge(dfd, on=['id', 'year'], how='outer')\n",
    "for c in df.columns[-4:]:\n",
    "    df.loc[df[c].isnull(), c] = 0\n",
    "for c in df.columns[-2:]:\n",
    "    df[c] = df[c].apply(lambda x: int(x))\n",
    "\n",
    "print(df.isnull().sum())\n",
    "display(df.head())\n",
    "\n",
    "# save csv\n",
    "df.to_csv(\"../data/generated/id-year_level_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "540px",
    "left": "26px",
    "top": "110.8px",
    "width": "222.7px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
