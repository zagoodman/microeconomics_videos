{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View this notebook here:\n",
    "https://nbviewer.jupyter.org/github/zagoodman/microeconomics_videos/blob/master/jupyter/assemble_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prep\" data-toc-modified-id=\"Prep-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prep</a></span></li><li><span><a href=\"#Merge-anonymized-data\" data-toc-modified-id=\"Merge-anonymized-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Merge anonymized data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Demographic-data\" data-toc-modified-id=\"Demographic-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Demographic data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preceding-term-vars\" data-toc-modified-id=\"Preceding-term-vars-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Preceding term vars</a></span></li><li><span><a href=\"#Student-level\" data-toc-modified-id=\"Student-level-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Student-level</a></span></li><li><span><a href=\"#GPA-and-course-units\" data-toc-modified-id=\"GPA-and-course-units-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>GPA and course units</a></span><ul class=\"toc-item\"><li><span><a href=\"#Concurrent-term\" data-toc-modified-id=\"Concurrent-term-2.1.3.1\"><span class=\"toc-item-num\">2.1.3.1&nbsp;&nbsp;</span>Concurrent term</a></span></li><li><span><a href=\"#Following-term\" data-toc-modified-id=\"Following-term-2.1.3.2\"><span class=\"toc-item-num\">2.1.3.2&nbsp;&nbsp;</span>Following term</a></span></li></ul></li></ul></li><li><span><a href=\"#Treatment-and-outcome-data\" data-toc-modified-id=\"Treatment-and-outcome-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Treatment and outcome data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Student-level\" data-toc-modified-id=\"Student-level-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Student level</a></span></li></ul></li></ul></li><li><span><a href=\"#Videos\" data-toc-modified-id=\"Videos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Videos</a></span><ul class=\"toc-item\"><li><span><a href=\"#100A\" data-toc-modified-id=\"100A-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>100A</a></span></li><li><span><a href=\"#Export\" data-toc-modified-id=\"Export-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Export</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file takes anonymized data from the T+LC, cleans them, and returns analysis-ready data frames:\n",
    "1. `id-year_level_data.csv`: treatment and exam score data\n",
    "2. `dem_concurrent.csv`: demographic data and GPA/courseload during the quarter of the experiment\n",
    "3. `dem_nextquarter.csv`: demographic data and GPA/courseload during the quarter following treatment\n",
    "\n",
    "All code in this file is in Python 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_flavor as pf\n",
    "\n",
    "@pf.register_dataframe_accessor('z')\n",
    "class MyFlavor(object):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self._df = df\n",
    "\n",
    "    def get_unique_students(self, var1 = \"id\", var2 = \"year\"):\n",
    "        df = self._df\n",
    "        x = len(df[[var1, var2]].drop_duplicates())\n",
    "        print(\"N unique students: {}\".format(x))\n",
    "        return x\n",
    "    \n",
    "    def get_vars(self):\n",
    "        df = self._df\n",
    "        cols = [x for x in df if x in tlcvarnames]\n",
    "        return df[cols].rename(columns = tlcvarnames)\n",
    "    \n",
    "tlcvarnames = {'id': 'id',\n",
    "               'year': 'year',\n",
    "               'de id': 'id', \n",
    "               'deid': 'id',\n",
    "               'previous_cum_gpa': 'prev_cumgpa',\n",
    "               'term_code_econ': 'term',\n",
    "               'term-econ100a': 'term',\n",
    "               'term code courses': 'term_courses',\n",
    "               'term_precoursegpa': 'term_pregpa',\n",
    "               'term code econ 100a': 'termecon',\n",
    "               'apct_type_desc': 'apptype',\n",
    "               'ethnicity_grouped': 'ethnicity',\n",
    "               'gender': 'gender',\n",
    "               'term_code_econ': 'term',\n",
    "               'year_econ': 'year',\n",
    "               'year - econ 100a': 'year',\n",
    "               'year - econ100a': 'year',\n",
    "               'year-econ100a': 'year',\n",
    "               'year-zack': 'year',\n",
    "               'measure names': 'measure',\n",
    "               'measure values': 'values',\n",
    "               'Class Units - Letter Grade': 'units_letter',\n",
    "               'Class Units - P/NP': 'units_pnp',\n",
    "               'Class Units - Withdrawn': 'units_w',\n",
    "               'GPA - Classes Letter Grade': 'gpa_letter',\n",
    "               'GPA - Classes Letter Grade - No Econ': 'gpa_letter_sansecon',\n",
    "               'GPA - Classes Letter Grade - No Econ 100A': 'gpa_letter_sans100a',\n",
    "               'GPA - Classes Letter Grade - Only Econ - No Econ 100A': 'gpa_econ_sans100a',\n",
    "               'N Classes - Letter Grade': 'nclass_letter',\n",
    "               'N Classes - Not Passed': 'nclass_np',\n",
    "               'N Classes - P/NP': 'nclass_pnp',\n",
    "               'N Classes - Passed': 'nclass_p',\n",
    "               'N Classes - Withdrawn': 'nclass_w'\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge anonymized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preceding term vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre 100A (term before) cumulative gpa. Missing for all entering freshmen or first time UCSD students\n",
    "\n",
    "dfd = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Pre Course GPA-FA18.xlsx\")\n",
    "dfd = pd.concat([dfd, pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Pre Course GPA-FA19.xlsx\")], 0)\n",
    "dfd.columns = [x.lower() for x in dfd.columns]\n",
    "dfd = dfd.z.get_vars()\n",
    "dfd.loc[dfd.term == 'FA18', 'year'] = 2018\n",
    "dfd.loc[dfd.term == 'FA19', 'year'] = 2019\n",
    "# reorder and drop term\n",
    "dfd = dfd[['id', 'year', 'term_pregpa', 'prev_cumgpa']]\n",
    "\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students(var2='id')\n",
    "display(dfd.isnull().sum())\n",
    "#dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ethnicity, gender, transfer status\n",
    "\n",
    "df = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Student-level.xlsx\")\n",
    "df.rename(str.lower, axis=1, inplace=True)\n",
    "df = df.z.get_vars()\n",
    "df['transfer'] = np.array(df.apptype == 'Transfer Student', dtype='int')\n",
    "df.loc[df.gender == 'Men (Cis & Trans)', 'gender'] = 'm'\n",
    "df.loc[df.gender == 'Women (Cis & Trans)', 'gender'] = 'f'\n",
    "df.loc[df.gender == 'Data Unavailable', 'gender'] = 'u'\n",
    "# drop term and reorder\n",
    "df = df[['id', 'year', 'ethnicity', 'gender', 'transfer', 'apptype']]\n",
    "\n",
    "# merge with dfd\n",
    "dfd = df.merge(dfd, how='outer', on=['id', 'year'])\n",
    "\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students()\n",
    "print(dfd.isnull().sum())\n",
    "#dfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPA and course units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Student-quarter-level_FA18-WI19.xlsx\")\n",
    "df.columns = [x.lower() for x in df.columns]\n",
    "df = df.z.get_vars()\n",
    "df2 = pd.read_excel(\"../data/raw/Econ-Goodman-Su20-Student-quarter-level_FA19-WI20.xlsx\")\n",
    "df2.columns = [x.lower() for x in df2.columns]\n",
    "df2 = df2.z.get_vars()\n",
    "\n",
    "df = pd.concat([df, df2], 0)\n",
    "print(len(df))\n",
    "display(pd.crosstab(df.year, df.term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concurrent term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concurrent data\n",
    "\n",
    "dfc = df.loc[df.term_courses.isin([\"FA18\", \"FA19\"])]\n",
    "dfc = dfc[['id', 'year', 'measure', 'values']]\n",
    "\n",
    "# reshape wide\n",
    "dfc = dfc.pivot_table(index=['id', 'year'], columns='measure', values='values').reset_index()\n",
    "dfc = dfc.z.get_vars()\n",
    "\n",
    "# merge with rest of demographic data\n",
    "dfd = dfd.merge(dfc, how='outer', on=['id', 'year'])\n",
    "\n",
    "# add zeros where needed\n",
    "for x in ['units_pnp', 'units_w']:\n",
    "    dfd.loc[dfd[x].isnull(), x] = 0\n",
    "\n",
    "# check and explore data\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students()\n",
    "print(dfd.dtypes)\n",
    "print(dfd.isnull().sum())\n",
    "\n",
    "# export\n",
    "dfd.to_csv(\"../data/generated/dem_concurrent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## following quarter's data\n",
    "\n",
    "dff = df.loc[df.term_courses.isin([\"WI19\", \"WI20\"])]\n",
    "dff = dff[['id', 'year', 'measure', 'values']]\n",
    "\n",
    "# reshape wide\n",
    "dff = dff.pivot_table(index=['id', 'year'], columns='measure', values='values').reset_index()\n",
    "dff = dff.z.get_vars()\n",
    "\n",
    "# merge with rest of demographic data\n",
    "dfd = dfd.iloc[:, 0:8]\n",
    "dfd = dfd.merge(dff, how='outer', on=['id', 'year'])\n",
    "\n",
    "# add zeros where needed\n",
    "for x in ['units_letter', 'units_pnp', 'units_w', \\\n",
    "          'nclass_letter', 'nclass_np', 'nclass_pnp', \\\n",
    "          'nclass_p', 'nclass_w']:\n",
    "    dfd.loc[dfd[x].isnull(), x] = 0\n",
    "\n",
    "# check and explore data\n",
    "print(len(dfd))\n",
    "dfd.z.get_unique_students()\n",
    "print(dfd.dtypes)\n",
    "print(dfd.isnull().sum())\n",
    "\n",
    "# export\n",
    "dfd.to_csv(\"../data/generated/dem_nextquarter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment and outcome data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/raw/DeID_all_pid-year-level_data with description.xlsx\")\n",
    "df.rename(columns={'DeID': 'id'}, inplace=True)\n",
    "print(len(df))\n",
    "df.z.get_unique_students()\n",
    "df = df[['id'] + [x for x in df.columns if x != 'id']]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think that's all I need for now...may add more later\n",
    "\n",
    "# drop the two duplicate entries\n",
    "print(len(df))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "df.z.get_unique_students()\n",
    "\n",
    "# export\n",
    "df.to_csv(\"../data/generated/id-year_level_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = pd.read_csv('../data/raw/DeID_video-level_data.csv')\n",
    "dfv = dfv.loc[dfv.videoid.notnull()]\n",
    "dfv.loc[dfv.syllabus_week.isnull(), 'syllabus_week'] = -1\n",
    "\n",
    "# int columns\n",
    "for v in dfv.columns[7:-1]:\n",
    "    dfv[v] = dfv[v].apply(lambda x: int(x))\n",
    "    \n",
    "# date columns\n",
    "dfv['lastview'] = pd.to_datetime(dfv['last.view'])\n",
    "dfv['firstview'] = pd.to_datetime(dfv['first.view'])\n",
    "\n",
    "# unnecessary cols\n",
    "dfv.drop(['first.view', 'last.view', 'videocode', 'length_mins'], 1, inplace=True)\n",
    "\n",
    "print(dfv.isnull().sum())\n",
    "dfv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore\n",
    "\n",
    "dftmp = dfv[['syllabus_week', 'incentivized', 'videoid']].drop_duplicates()\n",
    "print('This many relevant videos: {}'.format(sum(dftmp.syllabus_week.notnull())))\n",
    "print('This many incentivized videos: {}'.format(sum(dftmp.incentivized > 0)))\n",
    "print('\\nThis many relevant videos by syllabus week:')\n",
    "print(dftmp.syllabus_week.value_counts().sort_index())\n",
    "print('\\nThis many students per year:')\n",
    "print(dfv[['DeID', 'year']].drop_duplicates().year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration per video (seconds)\n",
    "\n",
    "# Steps:\n",
    "# 1. Get duration from lastview and firstview\n",
    "# 2. Topcode duration by the length of the video\n",
    "# 3. Impute duration when = 0 by multiplying max duration by within-student average % watched\n",
    "\n",
    "# Step 1\n",
    "dfv['duration'] = dfv.apply(lambda x: (x['lastview'] - x['firstview']).seconds, 1)\n",
    "dfv['duration_max'] = dfv.minutes * 60 + dfv.seconds\n",
    "\n",
    "# Step 2\n",
    "dfv['duration'] = dfv.apply(lambda x: min(x.duration, x.duration_max), 1)\n",
    "plt.hist(dfv.duration, 50)\n",
    "plt.title('Distribution of video durations (seconds)')\n",
    "plt.show()\n",
    "plt.hist(dfv.duration / dfv.duration_max, 50)\n",
    "plt.title('Distribution of video durations (percent)')\n",
    "plt.show()\n",
    "\n",
    "# Step 3\n",
    "dfv['duration_impute'] = np.array(dfv.duration == 0, dtype='int')\n",
    "dfv['percent'] = dfv.duration / dfv.duration_max\n",
    "dfv['percent'] = dfv.groupby(['DeID', 'duration_impute'])['percent'].transform('mean')\n",
    "\n",
    "dfsub = dfv.loc[dfv.duration_impute == 0, ['DeID', 'percent']].drop_duplicates()\n",
    "dfsub.columns = ['DeID', 'p_duration']\n",
    "plt.hist(dfsub.p_duration, 50)\n",
    "plt.title('Dist. of mean % watched, 0\\'s excluded')\n",
    "plt.show()\n",
    "\n",
    "# merge dfsub on dfv and impute\n",
    "dfv = dfv.merge(dfsub, on=['DeID'], how='left')\n",
    "dfv.loc[dfv.p_duration.isnull(), 'p_duration'] = 0.521 # see next cell\n",
    "dfv.loc[dfv.duration_impute == 1, 'duration'] = dfv.loc[dfv.duration_impute == 1].\\\n",
    "    apply(lambda x: x.p_duration * x.duration_max, 1)\n",
    "plt.hist(dfv.duration, 50)\n",
    "plt.title('Distribution of imputed and observed vid duration (seconds)')\n",
    "plt.show()\n",
    "plt.hist(dfv.duration / dfv.duration_max, 50)\n",
    "plt.title('Distribution of imputed and observed duration (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aside - look at relationship between having lots of videos at 0s and % needing imputation, N videos\n",
    "\n",
    "# def plot function\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "def plot_lowess(x, y, title=''):\n",
    "    \"\"\"Plots scatter of y vs x with lowess overlaid. Give y and x pre sorted.\n",
    "    \"\"\"\n",
    "    lowess_y = lowess(y, x, frac=0.25, is_sorted=True, return_sorted=False, missing='raise')\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, lowess_y, 'red')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# prep data\n",
    "dftmp = dfv.groupby('DeID')['duration_impute'].agg(['mean']).reset_index()\n",
    "dftmp = dftmp.merge(dfv.loc[dfv.duration_impute == 0, ['DeID', 'p_duration']].drop_duplicates(), \n",
    "                   on='DeID', how='left')\n",
    "dftmp.columns = ['DeID', 'p_impute', 'p_duration']\n",
    "\n",
    "# is there a relationship between % watched and % needing imputation?\n",
    "dfsub = dftmp.loc[dftmp.p_duration.notnull()].copy().sort_values('p_impute')\n",
    "plot_lowess(dfsub.p_impute, dfsub.p_duration, '% needing imputation vs mean % duration')\n",
    "                  \n",
    "# Do those with high imputation watch different number of videos?\n",
    "dftmp2 = dfv.groupby('DeID').videoid.agg(['count']).reset_index()\n",
    "dftmp2.columns = ['DeID', 'vcount']\n",
    "dftmp = dftmp.merge(dftmp2, on='DeID', how='left')\n",
    "dfsub = dftmp.loc[dftmp.vcount.notnull() & (dftmp.vcount < 300)].copy().sort_values('p_impute')\n",
    "plot_lowess(dfsub.p_impute, dfsub.vcount, '% needing imputation vs N videos watched')\n",
    "\n",
    "# mean duration vs N videos watched\n",
    "dfsub = dftmp.loc[dftmp.p_duration.notnull() & (dftmp.vcount < 300)].copy().sort_values('vcount')\n",
    "plot_lowess(dfsub.vcount, dfsub.p_duration,  'N videos watched vs mean % duration')\n",
    "\n",
    "\n",
    "# Look at those with no imputation info\n",
    "dfsub = dftmp.loc[dftmp.p_impute == 1]\n",
    "print('This many have no duration info: {}'.format(len(dfsub.DeID.unique())))\n",
    "print('Watched this many vids on average: {}'.format(dfsub.vcount.mean()))\n",
    "scalar = dftmp.loc[(dftmp.vcount <= 3) | (dftmp.p_impute > 0.9)].p_duration.mean()\n",
    "print('Avg. duration for those who watched <3 vids or need >90% imputed: {:.3f}%'.format(scalar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen helpful flags\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# first flag\n",
    "dfv['firstflag'] = dfv.groupby(['DeID', 'year', 'videoid']).firstview.transform('min') == dfv.firstview\n",
    "\n",
    "# before midterm 1\n",
    "dfv['b4_mid1'] = ((dfv.year == 2018) & (dfv.firstview <= dt(2018, 10, 19, 19, 20))) | \\\n",
    "                 ((dfv.year == 2019) & (dfv.firstview <= dt(2019, 10, 23, 21, 20))) \n",
    "\n",
    "# before midterm 2\n",
    "dfv['b4_mid2'] = ((dfv.year == 2018) & (dfv.firstview <= dt(2018, 11, 19, 19, 20))) | \\\n",
    "                 ((dfv.year == 2019) & (dfv.firstview <= dt(2019, 11, 13, 21, 20))) \n",
    "\n",
    "# before final\n",
    "dfv['b4_final'] = ((dfv.year == 2018) & (dfv.firstview <= dt(2018, 12, 8, 18, 0))) | \\\n",
    "                 ((dfv.year == 2019) & (dfv.firstview <= dt(2019, 12, 7, 14, 30))) \n",
    "\n",
    "for v in ['b4_mid1', 'b4_mid2', 'b4_final']:\n",
    "    print(dfv[v].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum duration for each student: total duration, and total unique duration. \n",
    "# only keep course relevant videos.\n",
    "# do this for: pre mid1, pre mid2, pre final\n",
    "\n",
    "# init df at DeID-level\n",
    "mergevars = ['DeID', 'year']\n",
    "dfd = dfv.loc[:, mergevars].drop_duplicates()\n",
    "\n",
    "# relevant only\n",
    "print('This many videos were irrelevant: {}'.format(sum(dfv.relevant_100a == 0)))\n",
    "print('This percent of videos were irrelevant: {}'.format(sum(dfv.relevant_100a == 0) / len(dfv)))\n",
    "dfr = dfv.loc[dfv.relevant_100a > 0]\n",
    "\n",
    "# Avg unique videos watched - imputed vs not\n",
    "u_imputed = dfr.groupby('DeID').videoid.agg('nunique').mean()\n",
    "u_n_imputed = dfr.loc[dfr.duration_impute == 0].groupby('DeID').videoid.agg('nunique').mean()\n",
    "print('This percent of unique videos imputed: {}'.format((u_imputed - u_n_imputed) / u_imputed))\n",
    "print('This percent of video counts imputed: {}'.format(sum(dfr.duration_impute)/len(dfr)))\n",
    "\n",
    "# relevant duration, all (not unique) - before mid1, mid2, final\n",
    "dfagg = dfr.copy()\n",
    "dfagg['duration_mid1'] = dfagg.duration * dfagg.b4_mid1\n",
    "dfagg['duration_mid2'] = dfagg.duration * dfagg.b4_mid2\n",
    "dfagg['duration_final'] = dfagg.duration * dfagg.b4_final\n",
    "dfaggd = dfagg.groupby(mergevars)[['duration_mid1', 'duration_mid2', 'duration_final']].agg('sum').reset_index()\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# relevant duration, unique - before mid1, mid2, final\n",
    "# keep only max duration per videoid\n",
    "dfaggd = dfagg.groupby(['DeID', 'year', 'videoid'])[['duration_mid1', 'duration_mid2', 'duration_final']].agg('max').reset_index()\n",
    "dfaggd = dfaggd.groupby(mergevars)[['duration_mid1', 'duration_mid2', 'duration_final']].agg('sum').reset_index()\n",
    "dfaggd.columns = ['DeID', 'year', 'duration_mid1_u', 'duration_mid2_u', 'duration_final_u']\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# units of hours\n",
    "for v in dfd.columns[2:]:\n",
    "    dfd[v] = dfd[v] / 3600\n",
    "\n",
    "# sanity check\n",
    "print('Sanity checks: should be 0 first.')\n",
    "print(sum(dfd.duration_final < dfd.duration_final_u - .01))\n",
    "print(sum(dfd.duration_final <= dfd.duration_final_u + .01))\n",
    "\n",
    "# do the same thing for counts\n",
    "\n",
    "# count all\n",
    "dfaggd = dfr.groupby(mergevars)[['b4_mid1', 'b4_mid2', 'b4_final']].agg('sum').reset_index()\n",
    "dfaggd.columns = ['DeID', 'year', 'videos_mid1', 'videos_mid2', 'videos_final']\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# count unique\n",
    "dfaggd = dfr.copy()\n",
    "for v in ['mid1', 'mid2', 'final']:\n",
    "    dfaggd['videos_'+v+'_u'] = dfaggd.loc[dfaggd['b4_'+v] == 1, 'videoid']\n",
    "dfaggd = dfaggd.groupby(mergevars)[['videos_mid1_u', 'videos_mid2_u', 'videos_final_u']].agg('nunique').reset_index()\n",
    "dfd = dfd.merge(dfaggd, on=mergevars, how='left')\n",
    "\n",
    "# sanity check\n",
    "print('Sanity checks: should be 0 first.')\n",
    "print(sum(dfd.videos_final < dfd.videos_final_u))\n",
    "print(sum(dfd.videos_final <= dfd.videos_final_u))\n",
    "\n",
    "# rename deid to id\n",
    "dfd.rename(columns={'DeID': 'id'}, inplace=True)\n",
    "\n",
    "display(dfd.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export video data by updating the pid-year-level data\n",
    "\n",
    "# TODO: add 100B stuff\n",
    "\n",
    "df = pd.read_csv(\"../data/generated/id-year_level_data.csv\")\n",
    "dropvars = ['videos', 'relevant', 'videos_u', 'relevant_u', \\\n",
    "            'duration_all', 'duration_rel', 'duration_u', \\\n",
    "            'videos_b4_mid1_rel', 'videos_b4_mid1_relu', \\\n",
    "            'videos_b4_mid2_rel', 'videos_b4_mid2_relu']\n",
    "\n",
    "df = df.loc[:, [c for c in df.columns if c not in dropvars]]\n",
    "df.head()\n",
    "\n",
    "# merge dfd on df and add zeros when no videos observed\n",
    "df = df.merge(dfd, on=['id', 'year'], how='outer')\n",
    "for c in df.columns[-12:]:\n",
    "    df.loc[df[c].isnull(), c] = 0\n",
    "for c in df.columns[-6:]:\n",
    "    df[c] = df[c].apply(lambda x: int(x))\n",
    "\n",
    "# print(df.isnull().sum())\n",
    "# display(df.head())\n",
    "\n",
    "# save csv\n",
    "df.to_csv(\"../data/generated/id-year_level_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When do students watch videos?\n",
    "plt.hist(dfv.firstview.apply(lambda x: x.hour), 24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
